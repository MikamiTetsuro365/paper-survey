<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2018-02-17T12:08:17+00:00</updated><id>https://shunk031.github.io/paper-survey/</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Radical-level-Ideograph-Encoder-for-RNN-based-Sentiment-Analysis-of-Chinese-and-Japanese" rel="alternate" type="text/html" title="Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese" /><published>2017-12-04T00:00:00+00:00</published><updated>2017-12-04T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Radical-level-Ideograph-Encoder-for-RNN-based-Sentiment-Analysis-of-Chinese-and-Japanese</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Radical-level-Ideograph-Encoder-for-RNN-based-Sentiment-Analysis-of-Chinese-and-Japanese">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字を構成する部首レベル(Radical-level)の特徴から、CNNおよびBi-directional RNNを用いて少ないパラメータ数で文書分類を行うモデルを提案している。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;単語の埋め込み表現はNLPのタスクにおいて幅広く利用されているが、規模の大きなボキャブラリはモデルの学習の際にコストとなる。先行研究では文字レベルの特徴を用いることでモデリングを行っているものがあるが、日本語や中国語における文字のボキャブラリは、英語のアルファベットと比べて非常に大きい。&lt;/p&gt;

&lt;p&gt;本研究では日本語や中国語で用いられている漢字に注目し、それらを構成する部首等を分解して利用するRadical-levelの特徴を利用したアーキテクチャとなっている。Radical-levelの特徴を用いることでWord-levelより90%程度小さいボキャブラリで、なおかつ先行研究と比べてとても少ないパラメータ数で同程度の精度を達成している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Radical-level-Ideograph-Encoder-for-RNN-based-Sentiment-Analysis-of-Chinese-and-Japanese/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;先行研究(Kim+ 2016)のモデルと似ているアーキテクチャを採用
    &lt;ul&gt;
      &lt;li&gt;異なる点はCharacter-levelの特徴の代わりにRadical-levelの特徴を利用している&lt;/li&gt;
      &lt;li&gt;Highway layerは利用していない
        &lt;ul&gt;
          &lt;li&gt;[Kim+ 2016]ではHighway layerを利用し、精度を上げている&lt;/li&gt;
          &lt;li&gt;しかしながら本アーキテクチャでは精度向上にあまり寄与しなかったため、今回は採用していない&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;N Radical-level Embeddings
    &lt;ul&gt;
      &lt;li&gt;各文字に対して、”部首”や”つくり”に分解した文字表現であるRadical-level Embeddingsを用いる&lt;/li&gt;
      &lt;li&gt;Radical-level Embeddingsが固定長になるようにゼロパディングを行っている
&lt;img src=&quot;/paper-survey/assets/img/nlp/Radical-level-Ideograph-Encoder-for-RNN-based-Sentiment-Analysis-of-Chinese-and-Japanese/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNNエンコーダを用いた文字Radicalから単語表現の獲得
    &lt;ul&gt;
      &lt;li&gt;複数サイズのカーネルで複数種類の特徴を抽出&lt;/li&gt;
      &lt;li&gt;stride=1でRadical-levelの特徴を抽出する&lt;/li&gt;
      &lt;li&gt;stride=nでCharacter-levelの特徴を抽出する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bi-directional LSTMエンコーダを用いた単語表現から文書表現の獲得&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;中国語のレビューデータセットであるCtripと日本語のレビューデータセットであるRakutenレビューデータセットを用いてベースラインと提案モデルの精度を比較している。各レビューデータセットに付与されている6ポイントの評価を3ポイント以上をpositive、それ以下をnegativeとして感情分析タスクとして評価を行っている。&lt;/p&gt;

&lt;p&gt;ベースラインのモデルと比較して、Character-level embeddingsモデルより13%程度少ないパラメータ数で、Word-level embeddingsモデルより90%程度少ないパラメータ数で先行研究のモデルと同程度の精度を出していることが分かる。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;小さいボキャブラリ数、少ないパラメータ数で効率の良い学習が可能&lt;/li&gt;
  &lt;li&gt;CNNエンコーダは効果的である&lt;/li&gt;
  &lt;li&gt;本アーキテクチャではHighway layerはあまり効果がなかった
    &lt;ul&gt;
      &lt;li&gt;本実験で行った2クラスの感情分析タスクの場合、CNNエンコーダとBi-directional RNNエンコーダ間の全結合層を必要としないため、Highway networkは常に入力を出力に渡すよう学習していると考えられる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;本研究のベースとなるアーキテクチャについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The character-aware neural language model
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017&quot;&gt;Kim, Yoon, et al. “Character-Aware Neural Language Models.” AAAI. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bi-directional RNN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://scholar.google.co.jp/scholar?output=instlink&amp;amp;q=info:oX-eyNpk1wAJ:scholar.google.com/&amp;amp;hl=ja&amp;amp;lr=&amp;amp;as_sdt=0,5&amp;amp;scillfp=4816690074695771990&amp;amp;oi=lle&quot;&gt;Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673–2681, 1997.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hierarchical attention networks
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/N16-1174&quot;&gt;Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. Hierarchical attention networks for document classification. In the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016), pages 1480–1489, 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FastText
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.03312&quot;&gt;Ke, Yuanzhi, and Masafumi Hagiwara. “Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese.” arXiv preprint arXiv:1708.03312 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Dynamic Routing Between Capsules</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Dynamic-Routing-Between-Capsules" rel="alternate" type="text/html" title="Dynamic Routing Between Capsules" /><published>2017-11-20T00:00:00+00:00</published><updated>2017-11-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Dynamic-Routing-Between-Capsules</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Dynamic-Routing-Between-Capsules">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;Capsule構造とRouting-by-Agreementでアフィン変換に対してよりロバストになったCapsule Network(CapsNet)を提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Convolutional Neural Network(CNN)は画像認識において最先端のアプローチとなっているが，ある物体を他の視点からみたサンプルに対する予測精度は低い．これは予めアフィン変換を施した学習データを用いることで対処されるが，特徴量の学習は次元数の増加に応じて指数関数的に増加してしまい，効率が悪い．&lt;/p&gt;

&lt;p&gt;先行研究で提案されているCapsuleは，ニューラルネットワークのある層がサブ構造を含むようなアーキテクチャを利用することで指数関数的な効率の悪さを抑えたものとなっている．&lt;/p&gt;

&lt;p&gt;また先行研究で用いられているCNNは以下のような欠点が存在している．&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNNはある一方向のみの物体で学習を行っている場合，回転を扱うことができないため，向きが変わると予測できない場合がある．&lt;/li&gt;
  &lt;li&gt;プーリング操作を行うととても深いレイヤーでは位置に対する不変性が得られるが，とても荒い表現になってしまう．
    &lt;ul&gt;
      &lt;li&gt;例えば鼻や口などの正確な空間関係を損なってしまう．顔認識の場合は鼻や口といった物体の正しい空間関係の認識が必要である．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本研究では計算効率の良いCapsule構造を取り入れ，非線形変換である「squash」，およびプーリング操作に変わりアフィン変換にロバストな「routing」からなるCapsule Networkを提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Squash
    &lt;ul&gt;
      &lt;li&gt;通常の典型的なニューラルネットワークでは、ユニットの出力のみがReLUといった非線形活性化関数によって潰される．&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Capsule Networkでは，Capsuleから出力されるベクトル全体が潰される．&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  {\bf v}_{j} = \frac{ \| {\bf s}_{j} \|^{2} }{ 1 + \| {\bf s}_{j} \|^{2} } \frac{ {\bf s}_{j} }{ \|{\bf s}_{j} \| }
\end{align*}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Routing (Routing-by-Agreement)
    &lt;ul&gt;
      &lt;li&gt;特徴の関連性に基づいて次のレイヤーのCapsuleにルーティングする．&lt;/li&gt;
      &lt;li&gt;Max-poolingでは最大の値のみ保持するが，これがCNNの欠点とも言える．&lt;/li&gt;
      &lt;li&gt;RoutingによってCapsuleは前の層からの特徴の加重和を得る．これは物体が重なっている場合の特徴検出に適している．
&lt;img src=&quot;/paper-survey/assets/img/cv/Dynamic-Routing-Between-Capsules/procedure1.png&quot; alt=&quot;Procedure 1&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CapsNet
    &lt;ul&gt;
      &lt;li&gt;全体のアーキテクチャ
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Convolution+ReLU &amp;gt; PrimaryCapsules &amp;gt; DigitCaps&lt;/code&gt;
&lt;img src=&quot;/paper-survey/assets/img/cv/Dynamic-Routing-Between-Capsules/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;PrimaryCapsules
        &lt;ul&gt;
          &lt;li&gt;32チャンネルのConvolutional 8D Capsules
            &lt;ul&gt;
              &lt;li&gt;各primary capsuleは9x9のカーネルでstrideが2のconvolutionユニットが8つから構成されている．&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;PrimaryCapsulesは複数のConvolutionの結果をsquashしていると見ることができる．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;DigitCaps
        &lt;ul&gt;
          &lt;li&gt;1クラスあたり16のDigitCapsの構造であり，各クラスについてよりロバストな表現を学習する．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Margin loss
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;複数のクラスを許容するために，各クラスに対応するCapsuleに対してmargin lossを定義している．&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  L_{k} = T_{k} \max{(0, m^{+} - \| {\bf v}_k \|)}^{2} + \lambda (1 - T_k) \max{(0, \| {\bf v}_k - m^{-} \|)}^{2}
\end{align*}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;各クラスに対するlossをすべて足し合わせて全lossとする．&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再構成による正規化効果
    &lt;ul&gt;
      &lt;li&gt;margin lossの他に入力画像と再構成画像とのMSEをreconstruction lossとして追加している．&lt;/li&gt;
      &lt;li&gt;DigitCapsの後段に3層の全結合層を持つDecoderを導入し，正則化の効果を追加している．&lt;/li&gt;
      &lt;li&gt;学習時には再構成対象の特徴表現のみを利用し，それ以外はマスクしている．
&lt;img src=&quot;/paper-survey/assets/img/cv/Dynamic-Routing-Between-Capsules/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNISTデータセットを用いた精度比較&lt;br /&gt;
ベースラインとなる先行研究の3層のCNNとCapsNetの精度を比較している．ベースラインモデルのパラメータ数が35.4Mに対して，CapsNetは8.2Mであり，再構成に使われているパラメータ数を除くと6.8Mであることから，ベースラインのモデルより少ないパラメータ数でよりよい精度を出していることがわかる．&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;アフィン変換に対するロバスト性&lt;br /&gt;
予めMNISTデータセットで学習したベースライン/CapsNetの両モデルに対して，MNISTデータセットにアフィン変換を施したaffNISTデータセットをテストデータとして用いて精度を比較している．ベースライン/CapsNetの両モデルともにトレーニングデータに対しては99%以上の正解率を出しているが，テストデータにおけるベースラインの正解率は66%であった．これに対しCapsNetは正解率79%と先行研究と比べてとても高い精度を出している．&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重なり合っている物体の認識について&lt;br /&gt;
異なるクラスに属する数字が重なり合っているようなデータセットであるMultiMNISTデータセットを作成し，Routingが一種のAttentionのような働きをしていることを確認する実験を行っている．これは複数のオブジェクトが重なり合っていても認識できる効果があると考えられている．&lt;br /&gt;
以下の再構成画像ではCapsNetが重なり合う2つの数字をそれぞれ正確に認識していることがわかる．&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/cv/Dynamic-Routing-Between-Capsules/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Capsuleに関する研究は今世紀初めの音声認識のためのRecurrent Neural Networkの研究と同様の段階にある．&lt;/li&gt;
  &lt;li&gt;Capsuleがより良いアプローチだと信じる根本的な理由はあるが、CNNのような近年爆発的に研究されてきた技術を凌駕するには，もっと小さな洞察を必要だろう．&lt;/li&gt;
  &lt;li&gt;シンプルなCapsule構造がすでに重なり合う数字を分けて認識し，とてもよい性能を発揮しているという事実は，Capsule構造が今後研究対象として価値のあるものと考えているので，早期に結果を示してみた．&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Capsuleについて
&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-21735-7_6&quot;&gt;Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. “Transforming auto-encoders.” International Conference on Artificial Neural Networks. Springer Berlin Heidelberg, 2011.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST/affNISTデータセットの精度比較におけるベースラインのモデルについて
&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf&quot;&gt;Wan, Li, et al. “Regularization of neural networks using dropconnect.” Proceedings of the 30th international conference on machine learning (ICML-13). 2013.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MultiMNISTデータセットの精度比較におけるベースラインのモデルについて
&lt;a href=&quot;https://arxiv.org/abs/1412.7755&quot;&gt;Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755, 2014.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.09829&quot;&gt;Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. “Dynamic Routing Between Capsules.” arXiv preprint arXiv:1710.09829 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Unsupervised Deep Embedding for Clustering Analysis</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Unsupervised-Deep-Embedding-for-Clustering-Analysis" rel="alternate" type="text/html" title="Unsupervised Deep Embedding for Clustering Analysis" /><published>2017-11-13T00:00:00+00:00</published><updated>2017-11-13T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Unsupervised-Deep-Embedding-for-Clustering-Analysis</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Unsupervised-Deep-Embedding-for-Clustering-Analysis">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;特徴表現の学習とクラスタリングを同時に行う，Deep Embedding Clustering(DEC)を提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;一般的なクラスタリング手法としてk-meansやGaussian Mixture Model(GMM)などがあげられる．これらの手法は幅広いアプリケーションに適用されている．
しかしながら入力の次元数が大きい場合には計算量が増大してしまい，効率が悪くなってしまう．&lt;/p&gt;

&lt;p&gt;k-meansを改良して高次元の入力に対しても効率よく扱う手法も提案されているが，線形の埋め込み表現に限られてしまっている．&lt;/p&gt;

&lt;p&gt;柔軟な距離計算が導入できるスペクトラルクラスタリングでは，k-meansよりよいパフォーマンスを発揮することが知られているが，
メモリの消費量が大きかったり，データ数が大きくなると計算が難しくなるといった問題がある．&lt;/p&gt;

&lt;p&gt;本研究では，高次元で規模の大きいデータセットに対しても効率的にクラスタリングできるよう特徴量を学習する Deep Embedding Clustering(DEC)を提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Deep-Embedding-for-Clustering-Analysis/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep Embedding Clustering
    &lt;ul&gt;
      &lt;li&gt;KLダイバージェンスを用いたクラスタリング
        &lt;ul&gt;
          &lt;li&gt;埋め込み先データ点とセントロイドとの距離はt分布で計算する&lt;/li&gt;
          &lt;li&gt;Auxiliary target distribution(今回はデルタ分布)とのKLダイバージェンスが小さくなるように計算を行う&lt;/li&gt;
          &lt;li&gt;提案モデルの学習過程はself-trainingと捉えることができる&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deep Neural Network(DNN)のパラメータ初期化
        &lt;ul&gt;
          &lt;li&gt;提案モデルはStacked autoencoder(SAE)を用いて初期化&lt;/li&gt;
          &lt;li&gt;各autoencoderはdenoising autoencoderとして，ノイズの乗った入力に対してノイズを除去するような出力を得るよう学習させる&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;MNIST，STL-10，REUTERSの3つのデータセットを用いてる，教師なし学習の評価指標であるunsupervised clusering accuracy(ACC)を評価指標とし，先行研究のk-means，LDMGI，SEC，DECとbackpropなしのDECを比較している．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;非線形写像であるDNNをfreezeした場合(DEC w/o backprop)は通常のDECよりもパフォーマンス面で悪くなっている．&lt;/li&gt;
  &lt;li&gt;信頼度が高いものについては大方正しいクラスタに属している．信頼度が低いデータは曖昧なもので，最終的には違うクラスタと判定してしまう場合もある．&lt;/li&gt;
  &lt;li&gt;SGDのような反復的な最適化を行うことで，学習が進むに連れ綺麗にクラスタに別れていることがわかる．&lt;/li&gt;
  &lt;li&gt;DNNのパラメータをautoencoderで初期化しなかった場合のパフォーマンスは低かった．&lt;/li&gt;
  &lt;li&gt;不均衡なデータに対しても提案手法であるDECはよいパフォーマンスを見せている．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Deep-Embedding-for-Clustering-Analysis/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;self-trainingについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=354805&quot;&gt;Nigam, Kamal, and Rayid Ghani. “Analyzing the effectiveness and applicability of co-training.” Proceedings of the ninth international conference on Information and knowledge management. ACM, 2000.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LDMGIについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/5454426/&quot;&gt;Yang, Yi, et al. “Image clustering using local discriminant models and global integration.” IEEE Transactions on Image Processing 19.10 (2010): 2761-2773.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SECについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/6030950/&quot;&gt;Nie, Feiping, et al. “Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering.” IEEE Transactions on Neural Networks 22.11 (2011): 1796-1808.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/xieb16.pdf&quot;&gt;Xie, Junyuan, Ross Girshick, and Ali Farhadi. “Unsupervised deep embedding for clustering analysis.” International Conference on Machine Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Character-level Intra Attention Network for Natural Language Inference</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference" rel="alternate" type="text/html" title="Character-level Intra Attention Network for Natural Language Inference" /><published>2017-11-08T00:00:00+00:00</published><updated>2017-11-08T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;自然言語推論に対して文字レベルの特徴とIntra Attentionを用いたCharacter-level Intra Attention Network(CIAN)を提案している．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語推論(Natural Language Inference(NLI))は，自然言語処理タスクの一つである．「前提(premise)」と「仮説(hypothesis)」からなる２つのテキストを用いて学習を行い，前提と仮設が正しいか・矛盾しているか・中立かを予測する．
先行研究ではNLIタスクに対してRNNベースのエンコーダを利用したものが多く，特にLSTMやGRUなどが幅広く使われている．
これらのエンコーダーは単語レベルのembeddingを用いており，Word2VecやGloveなどの学習済み単語ベクトルで初期化することが多い．&lt;/p&gt;

&lt;p&gt;しかしながら近年のコーパスにおける語彙数の増加により，予め用意した単語ベクトルには含まれない，語彙外の単語が多く存在してしまう．&lt;/p&gt;

&lt;p&gt;本研究ではベースラインモデルのEmbedding層の代わりに，文字レベルの特徴を用いるConvolutional Neural Network(CharCNN)を適用している．またIntra attentionを適用することで，よりリッチな文構造を学習することが可能となっている．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Character-level Intra Attention Network
    &lt;ul&gt;
      &lt;li&gt;Character-level Convolutional Neural Networks
        &lt;ul&gt;
          &lt;li&gt;文字レベルの特徴を用いることで単語レベルの特徴を用いるときに発生してしまう語彙外の単語の出現を抑えることができる&lt;/li&gt;
          &lt;li&gt;畳み込み層は「GermanやGermany」といったつづりの違いについても特徴を抽出することができる&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Intra Attention
        &lt;ul&gt;
          &lt;li&gt;先行研究で文書分類に対して導入されたIntra AttentionをNLIタスクに適用&lt;/li&gt;
          &lt;li&gt;BiLSTMの隠れ状態と重み・バイアスを活性化関数tanhへ通すことで隠れ表現を得たのち，softmaxを掛けることで重要度を表す重み行列を得ることができる&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;NLIタスクを評価する際に使用されてきたStanford Natural Language Inference(SNLI)コーパスと，新たに導入された大規模なコーパスであるMulti-Genre NLI(MNLI)コーパスを使用している．
提案モデルの学習の際にはMNLIのトレーニングデータすべてを利用したのと，SNLIトレーニングデータセットから20%程度ランダムに選んだデータを使っている．&lt;/p&gt;

&lt;p&gt;ベースラインのBiLSTMを用いたモデルと比較すると，提案モデルがよりよい精度を出していることがわかる．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;Attentionの重みを可視化した結果は以下のようになっている．左が前提の文で右が仮説の文を表している．より色が濃い単語が重要で予測に寄与しているものとなっている．
可視化の結果から，モデルはより似ている意味を持つ単語(ここではloveやenjoy)に注意をしていることがわかる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference/figure3-1.png&quot; alt=&quot;Figure 3-1&quot; /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Character-level-Intra-Attention-Network-for-Natural-Language-Inference/figure3-2.png&quot; alt=&quot;Figure 3-2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;CharCNNについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3016285&quot;&gt;Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2016. Character-aware neural language models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. pages 2741–2749.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intra attentionについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/N16-1174&quot;&gt;Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy. 2016. Hierarchical attention networks for document classifi- cation. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 1480–1489.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.07469&quot;&gt;Yang, Han, Marta R. Costa-jussà, and José AR Fonollosa. “Character-level Intra Attention Network for Natural Language Inference.” arXiv preprint arXiv:1707.07469 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Random Erasing Data Augmentation</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Random-Erasing-Data-Augmentation" rel="alternate" type="text/html" title="Random Erasing Data Augmentation" /><published>2017-10-29T00:00:00+00:00</published><updated>2017-10-29T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Random-Erasing-Data-Augmentation</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Random-Erasing-Data-Augmentation">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;入力画像に対してランダムに矩形領域をノイズでマスクするRandom Erasingを提案し，モデルの識別精度を向上させる．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;画像認識分野で素晴らしい結果を残しているConvolutional Neural Network(CNN)は多数の複雑なパラメータが存在しているため，
トレーニングデータに対しては良い精度を表し，テストデータに対しては精度が悪くなってしまう「過学習」が問題になっている．&lt;/p&gt;

&lt;p&gt;こうした問題に対して先行研究では以下のようなData augmentationや正則化を用いて過学習を抑制するよう提案を行っている．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Translation&lt;/li&gt;
  &lt;li&gt;Rotation&lt;/li&gt;
  &lt;li&gt;Random flipping&lt;/li&gt;
  &lt;li&gt;Random cropping&lt;/li&gt;
  &lt;li&gt;Adding noises&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;DropConnect&lt;/li&gt;
  &lt;li&gt;Adaptive dropout&lt;/li&gt;
  &lt;li&gt;Stochastic Pooling&lt;/li&gt;
  &lt;li&gt;DisturbLabel&lt;/li&gt;
  &lt;li&gt;PatchShuffle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本研究では以下のような利点があるRandom Erasingを提案している．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;パラメータの学習や省メモリで動く軽量な手法である．既存の学習の枠組みを変えることなく，さまざまなCNNモデルに適用することができる．&lt;/li&gt;
  &lt;li&gt;既存のData augmentationや正則化手法とRandom Erasingを組み合わせることにより，認識性能が更に向上する．&lt;/li&gt;
  &lt;li&gt;画像の一部分が隠されているようなサンプルに対するCNNのロバスト性を向上させる．ランダムに選んだテストデータに対して一部分を隠して推論させた場合においても高い性能を発揮する．&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Random-Erasing-Data-Augmentation/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random Erasing
    &lt;ul&gt;
      &lt;li&gt;入力画像に対してランダムに矩形領域をノイズでマスクする．
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;ハイパーパラメータ&lt;/p&gt;

            &lt;table&gt;
              &lt;thead&gt;
                &lt;tr&gt;
                  &lt;th&gt;パラメータ&lt;/th&gt;
                  &lt;th&gt;説明&lt;/th&gt;
                &lt;/tr&gt;
              &lt;/thead&gt;
              &lt;tbody&gt;
                &lt;tr&gt;
                  &lt;td&gt;Erasing probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;&lt;/td&gt;
                  &lt;td&gt;Random Erasingを使用する確率&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                  &lt;td&gt;Area ratio range &lt;script type=&quot;math/tex&quot;&gt;s_l&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;s_h&lt;/script&gt;&lt;/td&gt;
                  &lt;td&gt;マスクする領域の最小/最大比率 (画像全体に対する面積比)&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                  &lt;td&gt;Aspect ratio range &lt;script type=&quot;math/tex&quot;&gt;r_1&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;r_2&lt;/script&gt;&lt;/td&gt;
                  &lt;td&gt;マスク領域のアスペクト比の最小/最大値&lt;/td&gt;
                &lt;/tr&gt;
              &lt;/tbody&gt;
            &lt;/table&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;アルゴリズムは以下のようになっている．&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/cv/Random-Erasing-Data-Augmentation/algorithm1.png&quot; alt=&quot;Algorithm 1&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;画像分類タスクに対してはCIFAR-10/100，物体識別タスクに対してはPASCAL VOC 2007，人物の認識タスクに対してはMarket-1501，DukeMTMC-reID，CHHK03をそれぞれ利用している．&lt;/p&gt;

&lt;p&gt;AlexNetやVGG19，ResNet，Wide ResNet，ResNeXt，FRCN，ASDNなど，さまざまなSoTAモデルに対してRandom Erasingを適用した結果を比較している．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Random ErasingはDropoutを画像レベルで適用しているもの似ている．
    &lt;ul&gt;
      &lt;li&gt;Dropoutと異なる点
        &lt;ul&gt;
          &lt;li&gt;連続した矩形領域に対して適用している．&lt;/li&gt;
          &lt;li&gt;ノイズや障害物に対してモデルをよりロバストにすることを重点に置いている．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;Dropoutについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DropConnectについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.08378&quot;&gt;Varior, Rahul Rama, Mrinal Haloi, and Gang Wang. “Gated siamese convolutional neural network architecture for human re-identification.” European Conference on Computer Vision. Springer International Publishing, 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adaptive dropoutについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf&quot;&gt;Ba, Jimmy, and Brendan Frey. “Adaptive dropout for training deep neural networks.” Advances in Neural Information Processing Systems. 2013.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stochastic Poolingについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3557&quot;&gt;Zeiler, Matthew D., and Rob Fergus. “Stochastic pooling for regularization of deep convolutional neural networks.” arXiv preprint arXiv:1301.3557 (2013).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DisturbLabelについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper.pdf&quot;&gt;Xie, Lingxi, et al. “Disturblabel: Regularizing cnn on the loss layer.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PatchShuffleについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.07103&quot;&gt;Kang, Guoliang, et al. “PatchShuffle Regularization.” arXiv preprint arXiv:1707.07103 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.04896&quot;&gt;Zhong, Zhun, et al. “Random Erasing Data Augmentation.” arXiv preprint arXiv:1708.04896 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Deconvolutional Paragraph Representation Learning</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Deconvolutional-Paragraph-Representation-Learning" rel="alternate" type="text/html" title="Deconvolutional Paragraph Representation Learning" /><published>2017-10-28T00:00:00+00:00</published><updated>2017-10-28T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Deconvolutional-Paragraph-Representation-Learning</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Deconvolutional-Paragraph-Representation-Learning">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ConvolutionおよびDeconvolutionを用いて，長い文章の潜在的表現を計算効率よく学習する枠組みを提案している．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;文章の特徴を学習するようなタスクに対しては，これまで時系列情報を扱えるRecurrent Neural Networks(RNN)ベースのencoder-decoderが提案されてきた．
しかしながらRNNベースのモデルを用いた文章のデコード(再構成)では文章の長さが長くなるほど性能が落ちてしまうことが確認されている．&lt;/p&gt;

&lt;p&gt;本研究ではRNNを用いず，Convolutional Neural Network(CNN)をencoder，Deconvolutional Neural Network(DCNN)をdecoderに用いたencoder-decoderモデルで文章の特徴を学習させている．&lt;/p&gt;

&lt;p&gt;加えてCNNはRNNと比べて高い並列処理性を有するため，より計算効率よく学習させることができる．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Deconvolutional-Paragraph-Representation-Learning/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convolutional encoder
    &lt;ul&gt;
      &lt;li&gt;浅い層のフィルタはn-gram情報(画像における「エッジ」情報に類似)を学習し，深い層のフィルタはセマンティックな構文構造(画像における個々のオブジェクト情報)を学習している．&lt;/li&gt;
      &lt;li&gt;max-poolingといったpooling処理を用いずstrideを調整した畳み込みを用いている．これは畳み込み操作だけで空間的ダウンサンプリングを学習できるからだと考えられている．
        &lt;ul&gt;
          &lt;li&gt;本研究の初期実験ではmax-poolingを入れたモデルで精度があまり出なかった．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deconvolutional decoder
    &lt;ul&gt;
      &lt;li&gt;Convolutionの転置操作を行う．&lt;/li&gt;
      &lt;li&gt;RNNと比べて長い文における共起を捉えるのに優れているため，分類問題や文書要約のための特徴抽出に効果がある．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;本研究のConvolutional autoencodingを半教師ありの分類と要約タスクを解けるよう拡張
    &lt;ul&gt;
      &lt;li&gt;半教師ありタスクをマルチタスク学習として捉え，エンコーダーと教師ありモデルを同時に学習させる．
        &lt;ul&gt;
          &lt;li&gt;学習させた潜在的表現は高い再構成性や分類能力を保持する．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;以下のLoss関数を定義して半教師あり学習の枠組みを導入．&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  \mathcal{L}^{semi} = \alpha \sum_{d \in {\mathcal{D}_l + \mathcal{D}_u}}^{} \sum_{t}^{} \log{p(\hat{w}^{t}_d = w^{t}_d)} + \sum_{d \in \mathcal{D}_l}^{} \mathcal{L}^{sup} (f({\bf h}_d), y_d)
\end{align*}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;ラベル付きデータとラベル無しデータを用いてautoencoderのlossとclassifierのlossを最小化するよう学習．&lt;/li&gt;
          &lt;li&gt;ハイパーパラメータαを導入して学習初期は文の概要を捉えるよう焦点を当て，学習が進むにつれて細部を学習するようにする．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Deconvolutional-Paragraph-Representation-Learning/table1.png&quot; alt=&quot;Table 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;encoder-decoderの構造それぞれCNN-DCNN，CNN-LSTM，LSTM-LSTMとして各タスクで評価を行っている．&lt;/li&gt;
  &lt;li&gt;文章の再構成タスクでは，Hotel Reviews Datasetを用いてモデルを学習させ，ROUGEおよびBLEUスコアの比較を行っている．&lt;/li&gt;
  &lt;li&gt;文字レベル・単語レベルの訂正タスクでは，Yahoo! Answer datasetを用いてモデルを学習させ，Character Error Rate(CER)およびWord Error Rate(WER)の比較を行っている．&lt;/li&gt;
  &lt;li&gt;半教師ありの文書分類および要約タスクでは，DB Pedia，Yahoo! Answers，そしてYelp Review Polarityの各データセットを用いて評価を行っている．&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;言語生成タスクにおいては．RNN decoderは通常Deconvolution decoderと比較してより辻褄の合った文章を生成する．
    &lt;ul&gt;
      &lt;li&gt;RNN decoderから単語列を生成するためには，文全体から再帰的に学習された隠れベクトルが必要である．&lt;/li&gt;
      &lt;li&gt;Deconvolution decoderは予め指定されている単語列の構造をカプセル化するような表現を学習している．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.04729&quot;&gt;Zhang, Yizhe, et al. “Deconvolutional Paragraph Representation Learning.” arXiv preprint arXiv:1708.04729 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Visualizing and Understanding Convolutional Networks</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Visualizing-and-Understanding-Convolutional-Networks" rel="alternate" type="text/html" title="Visualizing and Understanding Convolutional Networks" /><published>2017-10-15T00:00:00+00:00</published><updated>2017-10-15T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Visualizing-and-Understanding-Convolutional-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Visualizing-and-Understanding-Convolutional-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;学習済みのConvolutional Neural Networks(CNN)における中間層の機能とモデルの動作を洞察することができる可視化手法を提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;CNNは画像認識の分野で素晴らしい成果を上げているが，なぜそのようなパフォーマンスが発揮されているか，またどのような過程を経て予測を行っているのかが明らかになっていない．&lt;/p&gt;

&lt;p&gt;先行研究ではCNNではないDeepなNeural Networkについて可視化を行っているものや，ネットワークのパラメータに対してヘッセ行列を計算し，どのような振る舞いをしているのか考察を行っているものがある．
これらは上位の層で学習されたとても複雑な不変量に対してのみ行われているものであり，単純な2次近似から特徴を考察しているものである．&lt;/p&gt;

&lt;p&gt;本研究ではノンパラメトリックな不変量への考察と，学習データに対する特徴マップの活性化度合いを可視化することで，CNNモデルを理解しようとするものである．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h3 id=&quot;deconvnetを用いた可視化&quot;&gt;Deconvnetを用いた可視化&lt;/h3&gt;

&lt;p&gt;CNNの動きを理解するためには中間層の働きを解釈する必要があるが，この中間層の働きを可視化するためにdeconvnetを利用する．
CNNの各層に対応するようdecnovnetを構築する． CNNで得た特徴マップをDeconvolutionやUmpooling，Rectificationを繰り返して入力の画像空間へ再構築する．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;unpooling&quot;&gt;Unpooling&lt;/h4&gt;

&lt;p&gt;Maxpoolingの逆操作をUnpoolingで行う．deconvnetでは，CNNでpoolingされた位置と値を保持しておくことでunpooling操作を可能にしている．&lt;/p&gt;

&lt;h4 id=&quot;rectification&quot;&gt;Rectification&lt;/h4&gt;

&lt;p&gt;特徴マップの再構成に有効なシグナルを得られるよう，unpoolingされた値に対してReLUを適用する．&lt;/p&gt;

&lt;h4 id=&quot;filtering&quot;&gt;Filtering&lt;/h4&gt;

&lt;p&gt;convolutionは学習されたフィルタを使用して，前の層の特徴マップを畳み込む．この逆操作を行うため，deconvnetでは同じフィルタを転置したものを利用する．なおReLUに通した後にdeconvolutionを適用する．&lt;/p&gt;

&lt;h3 id=&quot;cnnの可視化&quot;&gt;CNNの可視化&lt;/h3&gt;

&lt;h5 id=&quot;学習した特徴の可視化&quot;&gt;学習した特徴の可視化&lt;/h5&gt;

&lt;p&gt;学習済みのモデルの可視化を行った．&lt;/p&gt;

&lt;p&gt;各層の特徴マップのうち，特に活性化の度合いが強いもの9つを選択して再構築したものと，それに対応する原画像を並べている．&lt;/p&gt;

&lt;p&gt;各層ごとに階層的に特徴を学習していることが分かる．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第2層目：コーナーやエッジ，色の交差&lt;/li&gt;
  &lt;li&gt;第3層目：メッシュパターンやテキストパターンといったより複雑な模様&lt;/li&gt;
  &lt;li&gt;第4層目：第3層よりもバリエーションが豊富な模様で，より詳細にカテゴリの特徴を表している&lt;/li&gt;
  &lt;li&gt;第5層目：全体のオブジェクトが現れている．キーボードや犬など&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure2-1-2.png&quot; alt=&quot;Figure 2-1-2&quot; /&gt;
&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure2-3.png&quot; alt=&quot;Figure 2-3&quot; /&gt;
&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure2-4-5.png&quot; alt=&quot;Figure 2-4-5&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;学習時に学習される特徴の変化&quot;&gt;学習時に学習される特徴の変化&lt;/h5&gt;

&lt;p&gt;全学習データを学習している際に，学習が行われて変化していく特徴マップの過底を示している．&lt;/p&gt;

&lt;p&gt;モデルの浅い層では数epochで収束するのに対し，深い層では40-50epochをかけて収束していくのが分かる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;特徴の不変性&quot;&gt;特徴の不変性&lt;/h5&gt;

&lt;p&gt;5つのサンプル画像に対して，変形操作を行っていないものと回転や平行移動，拡大縮小の操作を行った画像をモデルに入力した際の挙動を図示している．&lt;/p&gt;

&lt;p&gt;小さな変形はモデルの第1層で劇的な効果を持つが，深い層での影響は小さい．変形やスケーリングに対してモデルの出力は安定している．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;モデルアーキテクチャの選択&quot;&gt;モデルアーキテクチャの選択&lt;/h4&gt;

&lt;p&gt;モデルの可視化によってモデルの動作を理解することができた．&lt;/p&gt;

&lt;p&gt;AlexNetの第1層および第2層を可視化することで，いくつか問題が見つかった．&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;第1層目：高頻度および低頻度の情報のみしか学習しておらず，中程度で現れるような特徴に対しては学習しきれていない．&lt;/li&gt;
  &lt;li&gt;第2層目：ギザギザとしたノイズが乗ってしまう「エイリアシング」が発生していることがわかる．これは第1層目のConvolutionのstrideが4と大きいためだと考えられる．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解決策として以下の2つが挙げられる．&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;第1層目のフィルタサイズを11x11から7x7に変更する&lt;/li&gt;
  &lt;li&gt;第2層目のConvolutionのstrideを4から2に変更する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;結果的にモデルの予測精度が向上している．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure6.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;対象をマスクした場合の挙動&quot;&gt;対象をマスクした場合の挙動&lt;/h4&gt;

&lt;p&gt;モデルが画像を分類する際，対象オブジェクトの位置は有効であるか，それとも周辺の情報が有効であるか実験を行った．&lt;/p&gt;

&lt;p&gt;入力画像の一部をグレーで塗りつぶした矩形で隠した後，分類を行った結果を考察している．&lt;/p&gt;

&lt;p&gt;対象オブジェクトを隠した場合に予測精度は大幅に減少した．
また特徴マップの活性化度合いも大幅に低下していることが確認できた．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure7.png&quot; alt=&quot;Figure 7&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;対応分析&quot;&gt;対応分析&lt;/h3&gt;

&lt;p&gt;深い層のモデルは，明示的に顔にある目や鼻といったオブジェクト間の対応を明示的に学習していない．しかしながら，浅い層のモデルでは暗黙的にそういった対応関係を学習している可能性がある．
この仮定を調べるため，5つの犬の画像のサンプルに対して，すべて左目だけ隠す，といったように体系的にマスクを施してモデルの挙動を確認した．&lt;/p&gt;

&lt;p&gt;目と鼻を隠した場合にスコアが低くなったが，これはモデルが暗黙的に顔のパーツの対応を確立していると言える．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure8.png&quot; alt=&quot;Figure 8&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Visualizing-and-Understanding-Convolutional-Networks/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ImageNetデータセットに対して，本研究の可視化手法で分かったAlexNetの欠点を改善した上図Figure 3のモデルを適用したところSoTAな結果を出すことができている．&lt;/p&gt;

&lt;p&gt;またImageNetで学習したモデルが一般性のある特徴を学習しているかを確認するため，Caltech-101，Caltech-256，およびPASCAL VOC2012を用いて精度を確認している．
この時一番最終のsoftmax層部分のみを再学習させている．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PASCALデータに対しては複数物体を捉えられるような損失関数に変更することでより良い精度が出せるのではないだろうか&lt;/li&gt;
  &lt;li&gt;上記が達成できれば物体認識にモデルを適用することができそう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;Deconvnetについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://uoguelph.ca/~gwtaylor/publications/zeilertaylorfergus_iccv2011.pdf&quot;&gt;Zeiler, Matthew D., Graham W. Taylor, and Rob Fergus. “Adaptive deconvolutional networks for mid and high level feature learning.” Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011.&lt;/a&gt;
CNNではないDeepなNeural Networkについて可視化を試みたもの&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf&quot;&gt;Erhan, Dumitru, et al. “Visualizing higher-layer features of a deep network.” University of Montreal 1341 (2009): 3.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1311.2901&quot;&gt;Zeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” European conference on computer vision. Springer, Cham, 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Which-Encoding-is-the-Best-for-Text-Classification-in-Chinese-English-Japanese-and-Korean" rel="alternate" type="text/html" title="Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?" /><published>2017-09-24T00:00:00+00:00</published><updated>2017-09-24T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Which-Encoding-is-the-Best-for-Text-Classification-in-Chinese-English-Japanese-and-Korean</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Which-Encoding-is-the-Best-for-Text-Classification-in-Chinese-English-Japanese-and-Korean">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;中国語・日本語・韓国語においてテキストのエンコーディング単位(UTF-8 bytes，文字，単語，ローマ字化した単語)が文書分類でそれぞれどのような効果を出すか実証的に比較を行った．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;英語の文では単語同士の区切りが明確であるが，中国語や日本語，韓国語(CJK)といった言語では単語と単語の境界線が不明瞭である．これは単語をベースとした様々な自然言語処理の手法を適用しようとする際に困難となる．
先行研究では文字単位のエンコーディングを用いてConvolutional Neural Networks(CNN)で処理を行うものが提案されているが，CJKでは文字種の多さが原因で直接適応することは難しい．&lt;/p&gt;

&lt;p&gt;本研究では各テキストエンコーディングに対して，473のモデルと14の大規模なデータセットを使用して詳細に検証を行っている．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;複数のエンコーディングと分類器を網羅的に検証
    &lt;ul&gt;
      &lt;li&gt;CNNを用いた文字表現のエンコーディングと分類
        &lt;ul&gt;
          &lt;li&gt;GlyphNet
            &lt;ul&gt;
              &lt;li&gt;文字を画像とした文字画像を畳み込んで分類を行う．&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;OneHotNet
            &lt;ul&gt;
              &lt;li&gt;テキストをbyte列とみなしてbyteレベルにエンコードを行い，畳み込んで分類を行う．&lt;/li&gt;
              &lt;li&gt;ローマ字化を用いてエンコードを行い，畳み込んで分類を行う．&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;EmbedNet
            &lt;ul&gt;
              &lt;li&gt;OneHotNetの前段にEmbedding層を追加している．&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;線形モデルを用いた分類
        &lt;ul&gt;
          &lt;li&gt;ロジスティック回帰を用いて分類を行う．&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;fastTextを用いた分類&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;以下のデータセットと前処理方法と前処理を利用してモデルの検証を行っている．&lt;/p&gt;

&lt;h3 id=&quot;検証したモデルについて&quot;&gt;検証したモデルについて&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;GlyphNet
    &lt;ul&gt;
      &lt;li&gt;Large&lt;/li&gt;
      &lt;li&gt;Small&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;OneHostNet
    &lt;ul&gt;
      &lt;li&gt;Byte (Large/Small)&lt;/li&gt;
      &lt;li&gt;Romanized (Large/Small)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;EmbedNet
    &lt;ul&gt;
      &lt;li&gt;Character (Large/Small)&lt;/li&gt;
      &lt;li&gt;Byte (Large/Small)&lt;/li&gt;
      &lt;li&gt;Romanized (Large/Small)&lt;/li&gt;
      &lt;li&gt;Word (Large/Small)&lt;/li&gt;
      &lt;li&gt;Romanized Word (Large/Small)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Linear model
    &lt;ul&gt;
      &lt;li&gt;Character
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Word
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Romanized Word
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fastText
    &lt;ul&gt;
      &lt;li&gt;Character
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Word
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Romanized Word
        &lt;ul&gt;
          &lt;li&gt;1-gram (plain/tfidf)&lt;/li&gt;
          &lt;li&gt;5-gram (plain/tfidf)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;データセット一覧&quot;&gt;データセット一覧&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;データセット&lt;/th&gt;
      &lt;th&gt;詳細&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;言語&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Dianping&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.dianping.com/&quot;&gt;https://www.dianping.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;中国語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;JD&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.jd.com/&quot;&gt;http://www.jd.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;中国語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rakuten&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.rakuten.co.jp/&quot;&gt;https://www.rakuten.co.jp/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;日本語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11st&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.11st.co.kr/&quot;&gt;http://www.11st.co.kr/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;韓国語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Amazon&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.amazon.com/&quot;&gt;https://www.amazon.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;英語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ifeng&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.ifeng.com/&quot;&gt;http://www.ifeng.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;中国語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Chinanews&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://chinanews.com/&quot;&gt;http://chinanews.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;中国語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NYTimes&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.nytimes.com/&quot;&gt;https://www.nytimes.com/&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;英語&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Joint&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://www.jd.com/&quot;&gt;JD&lt;/a&gt;, &lt;a href=&quot;https://www.rakuten.co.jp/&quot;&gt;Rakuten&lt;/a&gt;, &lt;a href=&quot;http://www.11st.co.kr/&quot;&gt;11st&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/&quot;&gt;Amazon&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;複数&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;単語分割とローマ字化前処理&quot;&gt;単語分割とローマ字化(前処理)&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;言語&lt;/th&gt;
      &lt;th&gt;分かち書き&lt;/th&gt;
      &lt;th&gt;ローマ字化&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;中国語&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/fxsjy/jieba&quot;&gt;jieba (ver. 0.38)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mozillazg/python-pinyin&quot;&gt;pypinyin (ver. 0.12)&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;日本語&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://taku910.github.io/mecab/&quot;&gt;MeCab (ver. 0.996)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.soimort.org/python-romkan/&quot;&gt;python-romkan (ver. 0.2.1)&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;韓国語&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://konlpy.org/en/v0.4.4/&quot;&gt;KoNLPy&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/suminb/hanja&quot;&gt;hanja (ver. 0.1.1)&lt;/a&gt;, &lt;a href=&quot;https://github.com/youknowone/hangul-romanize&quot;&gt;hangul-romanize&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CJKにおいて，文字レベルのn-gramを用いたfastTextが一番良い精度を記録した．また英語において，単語単位のn-gramを用いたfastTextが一番良い精度を記録した．&lt;/li&gt;
  &lt;li&gt;CJKの単語単位のエンコーディングは完璧な分かち書きがなされていなくても，線形モデルやfastTextで比較的良い結果となっている．&lt;/li&gt;
  &lt;li&gt;CNNにおける最適なエンコーディングはbyte単位のone hotエンコーディングであった．これはCNNが低レベルの表現からテキストを解釈できていると考えられ，複数言語に対しても一貫性のある統一された表現を学習していると考えられる．&lt;/li&gt;
  &lt;li&gt;fastTextは線形モデルよりも表現能力がないにもかかわらず，CNNよりも過学習しやすい傾向がある．&lt;/li&gt;
  &lt;li&gt;本研究で使用されたデータセットは &lt;a href=&quot;http://xzh.me/&quot;&gt;http://xzh.me/&lt;/a&gt; で公開される模様&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;文字画像を用いた文書分類について&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ucrel.lancs.ac.uk/bignlp2016/Shimada.pdf&quot;&gt;D. Shimada, R. Kotani, and H. Iyatomi. Document classification through image-based character embedding and wildcard training. In 2016 IEEE International Conference on Big Data (Big Data), pages 3922–3927, Dec 2016. doi: 10.1109/BigData.2016.7841067.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文字のone-hotエンコーディングを用いた文書分類について&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf&quot;&gt;Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649–657, 2015.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;fastTextについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.01759&quot;&gt;Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.02657&quot;&gt;Zhang, Xiang, and Yann LeCun. “Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?.” arXiv preprint arXiv:1708.02657 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Noisy-Softmax-Improving-the-Generalization-Ability-of-DCNN-via-Postponing-the-Early-Softmax-Saturation" rel="alternate" type="text/html" title="Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation" /><published>2017-09-20T00:00:00+00:00</published><updated>2017-09-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Noisy-Softmax-Improving-the-Generalization-Ability-of-DCNN-via-Postponing-the-Early-Softmax-Saturation</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Noisy-Softmax-Improving-the-Generalization-Ability-of-DCNN-via-Postponing-the-Early-Softmax-Saturation">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;学習時にsoftmaxへの入力に対して効果的にノイズを加えることで活性化状態の飽和を防ぎ，モデルの汎化性能を上げるNoisy Softmaxを提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Convoluional Neural Networks(CNN)といった層の深いモデルを学習させる場合，勾配消失などが原因で出力が飽和してしまい，悪い局所解に落ちてしまう問題がある．&lt;/p&gt;

&lt;p&gt;先行研究では活性化関数のSigmoidやReLUに対してノイズを追加したり，パラメータに対してノイズを加えたりするものがある．またsoftmax後にノイズを追加するDisturbLabelがある．&lt;/p&gt;

&lt;p&gt;本研究では全結合層の出力に対して効果的にノイズを追加しsoftmaxに入力を行うことで良い局所解に収束し，汎化性能を向上させるNoisy Softmaxを提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Noisy Softmax
    &lt;ul&gt;
      &lt;li&gt;annealed noise
        &lt;ul&gt;
          &lt;li&gt;学習中の入力とパラメータに対して適応的なノイズの追加&lt;/li&gt;
          &lt;li&gt;パラメータ&lt;code class=&quot;highlighter-rouge&quot;&gt;W&lt;/code&gt;と入力&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;との角度&lt;code class=&quot;highlighter-rouge&quot;&gt;θ&lt;/code&gt;を用いることでガウスノイズ&lt;code class=&quot;highlighter-rouge&quot;&gt;ξ&lt;/code&gt;を適応的に付加&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  f^{noise}_{y_{i}} = f_{y_i} - \alpha \|W_{y_i}\| \|X_i\|(1 - \cos{\theta_{y_i}}) |\xi|
\end{align*}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;softmaxに対してシンプルな変更で導入可能&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  L = - \frac{1}{N} \sum_{i}^{} \log{\frac{e^{f_{y_i} - \alpha \|W_{y_i}\| \|X_i\|(1-\cos{\theta_{y_i}} |\xi|)}}{\sum_{j \neq y_i}^{} e^{f_j} + e^{f_{y_j} - \alpha \|W_{y_i}\| \|X_i\|(1 - \cos{\theta_{y_i}}) |\xi|}}}
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;通常のsoftmax，および固定で正のノイズを追加するNormal，負のノイズを追加するNegativeと提案手法であるNoisy Softmaxについて比較を行っている．
VGGライクなネットワークに対してノイズを付加する各手法を適用し，MNIST，CIFAR 10/100，LFW，FGLFW，YTFの各データセットを用いて評価を行っている．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ノイズの効力を決めるパラメータ &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; について
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; の値を大きくし過ぎるとbackpropするときに大きな勾配が伝搬してしまい，収束しづらい．&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; の値を小さく設定するとモデルの汎化性能が上がる．&lt;/li&gt;
      &lt;li&gt;特に &lt;script type=&quot;math/tex&quot;&gt;\alpha = 0.1&lt;/script&gt; 程度で効果が出ている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;正のノイズか負のノイズか
    &lt;ul&gt;
      &lt;li&gt;正のノイズ &lt;script type=&quot;math/tex&quot;&gt;n = \alpha \xi&lt;/script&gt; と負のノイズ &lt;script type=&quot;math/tex&quot;&gt;n = - \alpha \|\xi\|&lt;/script&gt; を比較している．&lt;/li&gt;
      &lt;li&gt;負のノイズは通常のsoftmaxよりも悪い結果になってしまっている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;annealed noiseとfixed noiseについて
    &lt;ul&gt;
      &lt;li&gt;固定のノイズを付加するfree noise &lt;script type=&quot;math/tex&quot;&gt;n = \alpha \|\xi\|&lt;/script&gt; ，amplitude noise &lt;script type=&quot;math/tex&quot;&gt;n = \alpha \|W\| \|X\| \|\xi\|&lt;/script&gt; とannealed noise &lt;script type=&quot;math/tex&quot;&gt;n = \alpha \|W\| \|X\| (1 - \cos{\theta}) \|\xi\|&lt;/script&gt; を比較している．&lt;/li&gt;
      &lt;li&gt;free noise・amplitude noiseは通常のsoftmaxより少しよい精度となっている．&lt;/li&gt;
      &lt;li&gt;amplitude noiseとannealed noiseを比べた場合，annealed noiseはだんだんとノイズの量が減るため，より良い局所解に落ちていくと考えられている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;正則化の働きについて
    &lt;ul&gt;
      &lt;li&gt;付加されるノイズが新しい学習データとみなすことができ，効果的なdata augmentationとなっている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;ReLUに対してノイズを加える&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf&quot;&gt;Nair, Vinod, and Geoffrey E. Hinton. “Rectified linear units improve restricted boltzmann machines.” Proceedings of the 27th international conference on machine learning (ICML-10). 2010.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sigmoidに対してノイズを加える&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/gulcehre16.html&quot;&gt;Gulcehre, Caglar, et al. “Noisy activation functions.” International Conference on Machine Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DisturbLabel&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper.pdf&quot;&gt;Xie, Lingxi, et al. “Disturblabel: Regularizing cnn on the loss layer.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.03769&quot;&gt;Chen, Binghui, Weihong Deng, and Junping Du. “Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation.” arXiv preprint arXiv:1708.03769 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Learning Chinese Word Representations From Glyphs Of Characters</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters" rel="alternate" type="text/html" title="Learning Chinese Word Representations From Glyphs Of Characters" /><published>2017-08-27T00:00:00+00:00</published><updated>2017-08-27T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字の文字画像からConvolutional AutoEncoder(convAE)を利用して文字表現を獲得し，その文字表現を用いた中国語の単語表現を獲得する手法を提案している．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;先行研究における単語の分散表現の学習はCBOWやSkipgram，GloVeといったものがある．また中国語における単語の分散表現の学習は「漢字」の形状的特徴を活用する，Character-enhanced word embedding(CWE)やMulti-granularity Embedding(MGE)といったものが提案されている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本研究ではCWEやMGEといった先行研究と同様に，漢字の形状に着目し文字の形状情報を抽出したのち，単語表現の学習に利用する2つの手法を提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;文字表現の獲得
    &lt;ul&gt;
      &lt;li&gt;漢字の形状的特徴を考慮した文字表現を抽出するconvAE&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure6.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;単語表現の獲得
    &lt;ul&gt;
      &lt;li&gt;Glyph-Enchanced Word Embedding(GWE)
        &lt;ul&gt;
          &lt;li&gt;Enhanced by Context Word Glyphs(ctxG)
            &lt;ul&gt;
              &lt;li&gt;文字表現を組み合わせた単語表現ctxGからターゲット単語の表現を学習する&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure7.png&quot; alt=&quot;Figure 7&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Enhanced by Target Word Glyphs(tarG)
            &lt;ul&gt;
              &lt;li&gt;ターゲット単語の文字表現の平均値を用いてターゲット単語の表現を学習する&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure8.png&quot; alt=&quot;Figure 8&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Directly Learn From Character Glyph Features
        &lt;ul&gt;
          &lt;li&gt;RNN-Skipgram
            &lt;ul&gt;
              &lt;li&gt;GRUを用いて直接文字表現を並べた単語からターゲット単語の表現を学習する&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure9.png&quot; alt=&quot;Figure 9&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;RNN-GloVe
            &lt;ul&gt;
              &lt;li&gt;ターゲット単語と異なる単語を用いて共起からターゲット単語の表現を学習する&lt;br /&gt;
&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure10.png&quot; alt=&quot;Figure 10&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;提案手法で学習した単語表現を用いて，単語の類似度，単語の類推タスクを行っている．&lt;/p&gt;

&lt;p&gt;学習した文字表現をt-SNEを用いて2次元まで落とし，可視化したものが以下のようになっている．形状の似ている漢字が集まっていることが分かる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters/figure12.png&quot; alt=&quot;Figure 12&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;単語の類似度を検証する際に利用したデータセットであるWordSim-240とWordSim-296，SimLex-999は英語から中国語に翻訳したものを利用しており，翻訳したデータセットは公開している．&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;p&gt;CWEについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/11000/10834&quot;&gt;Chen, Xinxiong, et al. “Joint Learning of Character and Word Embeddings.” IJCAI. 2015.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MGEについて&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D/D16/D16-1100.pdf&quot;&gt;Yin, Rongchao, et al. “Multi-Granularity Chinese Word Embedding.” EMNLP. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.04755&quot;&gt;Su, Tzu-Ray, et al. “Learning Chinese Word Representations From Glyphs Of Characters.”, arXiv preprint arXiv:1708.04755 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>