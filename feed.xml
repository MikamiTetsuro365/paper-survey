<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2018-02-19T07:51:09+00:00</updated><id>https://shunk031.github.io/paper-survey/</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Variable Importance Using Decision Tree</title><link href="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees" rel="alternate" type="text/html" title="Variable Importance Using Decision Tree" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;決定木とランダムフォレストは優れたパフォーマンスと示すだけでなく、Feature importanceといった情報が分かる、確立されたモデルである。 不純度ベースで変数の重要度を知ることができるため使われることが多いモデルであるが、これらの重要度は理論的に不明な点が多い。&lt;/p&gt;

&lt;p&gt;本研究ではDSTUMPを提案し、様々な仮定におけるモデリングの下で高次元のデータ利用で有限のサンプルパフォーマンス保証性を導き出すことによって、ツリー系のモデルの性能についての考察を行っている。またこれら不純度ベースの手法の有効性について、広範囲の実験を下に有効性を実証している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Variable-Importance-Using-Decision-Trees/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6646-variable-importance-using-decision-trees&quot;&gt;Kazemitabar, Jalil, et al. “Variable Importance using Decision Trees.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</title><link href="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data" rel="alternate" type="text/html" title="Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では教師なしの連続データに対して解釈可能な表現を学習するFactrized hierarchical variational autoencoderを提案している。具体的には潜在変数のことなるデータに対し、事前確率としてSequence-dependent priorsとSequence-independent priorsをFactorized hierarchical graphical model内で組み合わせることで、連続データが持つマルチスケールな情報を利用するモデルとなっている。&lt;/p&gt;

&lt;p&gt;本モデルは2つの音声コーパスTIMITとAurora-4を用いて評価を行っている。具体的には異なる潜在変数の組を使って、スピーカーや言語コンテンツを変換する能力を定性的に評価している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6784-unsupervised-learning-of-disentangled-latent-representations-from-sequential-data&quot;&gt;Hsu, Wei-Ning, Yu Zhang, and James Glass. “Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Sparse Embedded k-Means Clustering</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering" rel="alternate" type="text/html" title="Sparse Embedded k-Means Clustering" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;K-meansクラスタリングは広く知られている素晴らしいアルゴリズムであるが、高次元のデータに対しては、計算コストの高さゆえに様々な分野への応用を妨げている現状がある。一般的には次元削減の手法を用いて対処することが多いが、近年Random projection(RP)などの手法を用いて高速なK-meansクラスタリングを実現することができる。しかしながらこの手法は他の次元削減手法よりも多くの改善点が存在している。例として特異値分解(SVD)に基づく特徴抽出手法と比較して、RPは近似を行いつつ、データ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; で特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; のデータ &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbb{R}^{n \times d}&lt;/script&gt; に対して &lt;script type=&quot;math/tex&quot;&gt;\min{(n, d)}\epsilon^{2} \frac{\log{(d)}}{k}&lt;/script&gt; だけ実行時間を削減している。&lt;/p&gt;

&lt;p&gt;これらの改善を経てもなお行列の乗算には &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O} \left(\frac{ndk}{\epsilon^{2} \log{(d)}} \right)&lt;/script&gt; だけ必要であり、特にデータ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; や特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; が大きい場合にはとても大きな計算コストとなってしまう。これらのボトルネックを解消するために、本研究では &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(nnz(X))&lt;/script&gt; ( &lt;script type=&quot;math/tex&quot;&gt;nnz(X)&lt;/script&gt; は &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 内における非ゼロの数を表している) を必要とする高速な行列の乗算を行う枠組みを用いて、スパースな埋め込み表現に対してk-meansクラスタリングを行う手法を提案をしている。また本研究ではRPの近似精度についても改善を行っている。ILSVRC2012等のデータセットに対して従来の次元削減手法を次元を落としてからk-meansクラスタリングをした結果と、提案手法の高速な次元圧縮を利用したクラスタリング結果を比較している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Sparse-Embedded-k-Means-Clustering/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6924-sparse-embedded-k-means-clustering.pdf&quot;&gt;Liu, Weiwei, Xiaobo Shen, and Ivor Tsang. “Sparse Embedded $ k $-Means Clustering.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks</title><link href="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks" rel="alternate" type="text/html" title="SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では特異値分解を利用した、とても大きいボキャブラリを持つSoftmax関数を高速に近似する手法を提案している。SVD-Softmaxはニューラル言語モデルの推論時に最上位となりうる単語について、高速かつ正確に確率推定を行うことを目的としている。提案手法ではSVDを使って出力ベクトルの計算に用いた重み行列を変換する。各単語の近似確率については、いくつかの大きな特異値を使用することで単語の大部分の性質を持たせることができ、これを利用して重み行列を推定できると主張している。&lt;/p&gt;

&lt;p&gt;本研究の手法を言語モデリングとニューラル機械翻訳に適用することで、提案手法で導入されている近似手法が効果を発揮していることを検証している。本アルゴリズムでは800,000個の語彙の場合においても、約20%程度の算術演算しか必要とせず、GPUを利用することで3倍以上スピードアップしていることが分かっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks&quot;&gt;Shim, Kyuhong, et al. “SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization" rel="alternate" type="text/html" title="Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;過学習はDeep neural networkの学習における課題の1つであり、汎化性能を向上させるためにさまざまな正則化手法が提案されてきた。中でも学習中にHidden unitに対してノイズを乗せるDropoutは有名な正則化手法として知られているが、こうした正則化手法がなぜ効果があるのかは不明である。&lt;/p&gt;

&lt;p&gt;本研究では従来のノイズ付加による正則化手法が真の目的関数の下限に対して最適化すること、Stochastic gradient descentにおいて、より制約の強い下限にフィットするよう複数のノイズを導入する手法を提案している。CIFAR10等のデータセットを用いて提案手法の効果を確かめている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7096-regularizing-deep-neural-networks-by-noise-its-interpretation-and-optimization&quot;&gt;Noh, Hyeonwoo, et al. “Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Few-Shot Adversarial Domain Adaption</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation" rel="alternate" type="text/html" title="Few-Shot Adversarial Domain Adaption" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究ではDeepモデルを用いた教師ありのドメイン適応の問題に対して対処するフレームワークを提案している。主なアイディアは埋め込み表現を学習する際にAdversarialな学習を導入し、2つの異なるドメインの特徴を保持するように埋め込みつつ、同時に埋め込み表現が意味をなすように配置させるところである。教師ありタスクの場合、一般的には大量のラベル付与済みデータが必要であるが、ラベルを付与すべきデータが少なく済むことでより扱いやすい問題となる。こういったFew-shotな学習の場合にデータ欠損があると、埋め込み空間に対して埋め込み表現の配置と分離というのは困難を極める。&lt;/p&gt;

&lt;p&gt;提案モデルでは典型的な2値のAdversarial discriminatorが4つの異なるクラスを分離するためにData augmentationに工夫することで、教師ありのドメイン適応問題対して有効であることを見つけている。加えて本手法ではラベル付与済みデータがとても少ない場合、特にカテゴリあたり1サンプルであったとしても高速に適応できていることが示されている。MNISTやSVHNデータセット等を用いて本手法のドメイン適応の効果を確認している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Few-Shot-Adversarial-Domain-Adaptation/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7244-few-shot-adversarial-domain-adaptation&quot;&gt;Motiian, Saeid, et al. “Few-Shot Adversarial Domain Adaptation.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">DropoutNet: Addressing Cold Start in Recommender Systems</title><link href="https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System" rel="alternate" type="text/html" title="DropoutNet: Addressing Cold Start in Recommender Systems" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;潜在的な意味を捉えるモデルは、精度の良さやスケーラビリティを有するため、レコメンドシステムを導入する際にデフォルトの選択肢の1つとなっている。しかしながらレコメンドといった分野の先行研究では、主にユーザーとアイテムの関係をモデル化したものが多く、データが少ない場合に起こる「コールドスタート問題」に対する解決策を提示しているモデルは少ない。Deep learningは近年様々な入力に対して優れた成功を収めている。そういったモデルを利用し、レコメンドシステムにおけるコールドスタート問題を解決するため、ニューラルネットワークベースのDropoutNetを提案している。&lt;/p&gt;

&lt;p&gt;コンテンツベースの目的関数を追加した既存の手法と異なり、本研究では最適化手法に焦点を当てたのと、Dropoutを応用して明示的にコールドスタートに対してモデルのトレーニングを行う方法を示している。提案モデルは既存のモデルに対して適用することができ、コールドスタートに対して効果を発揮している。コールドスタート問題を解決しているかを評価できるCiteULikeデータセットやACM RecSys 2017 challengeデータセットを用いて、提案モデルが優れた結果を出していることを確認している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7081-dropoutnet-addressing-cold-start-in-recommender-systems.pdf&quot;&gt;Volkovs, Maksims, Guangwei Yu, and Tomi Poutanen. “DropoutNet: Addressing Cold Start in Recommender Systems.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Dilated Recurrent Neural Networks</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks" rel="alternate" type="text/html" title="Dilated Recurrent Neural Networks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;長い文章等に対してRNNを学習させることはとても難しい問題となっている。特に複雑な依存関係、勾配消失/爆発問題、並列化が難しい点が挙げられている。本研究ではシンプルで効率的なRNNのアーキテクチャであるDilatedRNNを提案している。本アーキテクチャは複数のDilated recurrent skip connectionを組み合わせており、さまざまなRNNセルとフレキシブルに組み合わせることができる。またDilatedRNNは必要なパラメータ数を減らしつつ学習効率を大幅に向上させることができ、とても長いスパンで依存性のあるデータやタスクで優れたパフォーマンスを発揮している。&lt;/p&gt;

&lt;p&gt;提案モデルの利点を理論的に定量化するため、Memory capacity measureを導入している。これは長いスキップコネクションを持つRNNに対して既存の指標よりも適していることが示されており、LSTMなど他のRNNアーキテクチャのMemory capacity measureを比較することでDilatedRNNが優れていること証明している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Dilated-Recurrent-Neural-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6613-dilated-recurrent-neural-networks&quot;&gt;Chang, Shiyu, et al. “Dilated recurrent neural networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Deep Subspace Clustering Netoworks</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Deep-Subspace-Clustering-Networks" rel="alternate" type="text/html" title="Deep Subspace Clustering Netoworks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Deep-Subspace-Clustering-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Deep-Subspace-Clustering-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では教師なしサブスペースクラスタリングのためのDeep neural networkアーキテクチャを提案している。本アーキテクチャは入力データを潜在空間に非線形で写像するDeep auto-encoderをベースとしている。本研究での重要なポイントとして、従来のサブスペースクラスタリングで効果的であると証明されている「Self-expressiveness」という特性を模倣するため、EncoderとDecoderとの間に新しくSelf expressiveレイヤーを導入している点が挙げられる。&lt;/p&gt;

&lt;p&gt;本研究で導入されたSelf expressiveレイヤーは、一般的なBackpropagationを通じてすべてのデータ点間の類似性をシンプルかつ効果的に学習するものとなっている。またこのレイヤーは非線形であるため、本研究のアーキテクチャは複雑な構造を持つデータ点を容易にクラスタリングできる。加えて、Subspace clutering networkのパラメータを効率的に学習させるためのPre-trainingとFine-tuningを提案している。Subspace clustering networkが先行研究の教師なしサブスペースクラスタリング手法よりも遥かに優れていることを示している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Deep-Subspace-Clustering-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6608-deep-subspace-clustering-network&quot;&gt;Ji, Pan, et al. “Deep subspace clustering networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Deanonymization in the Bitcoin P2P Network</title><link href="https://shunk031.github.io/paper-survey/summary/others/Deanonymization-in-the-Bitcoin-P2P-Network" rel="alternate" type="text/html" title="Deanonymization in the Bitcoin P2P Network" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Deanonymization-in-the-Bitcoin-P2P-Network</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Deanonymization-in-the-Bitcoin-P2P-Network">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ビットコインにおけるP2Pネットワークに対する近年の攻撃では、ネットワーク一貫性を確保するために使用されるTransaction-floodingプロトコルを悪用することによって、ユーザの匿名性を奪うことがが可能となってしまっていた。2015年にビットコインコミュニティでは、このプロトコルをDiffusionと呼ばれるプロトコルに変更することで、こうした攻撃に対応した。&lt;/p&gt;

&lt;p&gt;しかしながらこのDiffusionプロトコルが実際にシステムに対して匿名性を向上させるかどうかは明確には分かっていない。本論文ではビットコインのネットワークをモデル化し、2015年前後の匿名性について分析を行っている。中心となる問題はグラフ上での流行源の推論/特定の1つとなっている。観測モデルと拡散メカニズムについてはビットコインの実装から分かるが、注目すべきはこうしたモデルは先行研究で研究されていない点である。本研究では最適な流行源推定量を特定し、分析する内容となっている。&lt;/p&gt;

&lt;p&gt;分析結果として、2015年前後のビットコインネットワークプロトコルは通常のネットワーク上で脆弱な匿名性を提供していたことが判明している。実際に2015年のビットコインP2Pネットワークのスナップショットでシュミレーションを行い、本研究での主張を確認している。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6735-deanonymization-in-the-bitcoin-p2p-network&quot;&gt;Fanti, Giulia, and Pramod Viswanath. “Deanonymization in the bitcoin p2p network.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>