<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2018-05-03T01:55:36+00:00</updated><id>https://shunk031.github.io/paper-survey/</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Utilizing Visual Forms of Japanese Characters for Neural Review Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Utilizing-Visual-Forms-of-Japanese-Characters-for-Neural-Review-Classification" rel="alternate" type="text/html" title="Utilizing Visual Forms of Japanese Characters for Neural Review Classification" /><published>2018-04-30T00:00:00+00:00</published><updated>2018-04-30T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Utilizing-Visual-Forms-of-Japanese-Characters-for-Neural-Review-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Utilizing-Visual-Forms-of-Japanese-Characters-for-Neural-Review-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;文字の見た目を考慮した文字embeddingを用いて日本語の評判分析を行う&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;日本語や中国語の文字は表意文字であり、文字自身が意味を持っている。通常の自然言語処理の手法では、文字の見た目の情報は無視し、文字IDの羅列として扱う。
本研究では表意文字や記号の形状を考慮した日本語の評判分析を行うモデルを提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Utilizing-Visual-Forms-of-Japanese-Characters-for-Neural-Review-Classification/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Character-based Hierarchal Attention Networks (HAN) をベースとしたモデル
    &lt;ul&gt;
      &lt;li&gt;HANと比べて文字embeddingのパラメータ数が大幅に減少している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;文字を文字画像に変換し、そこからConvolutional Neural Networks (CNN) を通して文字の形状情報を捉えた文字embeddingを取り出す&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;6段階の評価と7カテゴリが付与されている&lt;a href=&quot;https://www.nii.ac.jp/dsc/idr/en/rakuten/rakuten.html&quot;&gt;Raluten Travel review&lt;/a&gt;を用いて提案手法の性能を評価している。
ベースラインとして先行研究のHANを利用し、前処理として&lt;a href=&quot;https://github.com/ikegami-yukino/neologdn&quot;&gt;neologdn&lt;/a&gt;を用いてNFKCのユニコード標準化を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Visual attentionを適用して評判分析の際に文字のどの部分に着目しているのか可視化したい&lt;/li&gt;
  &lt;li&gt;従来の部首の辞書を用いた特徴を利用すれば未知の文字に対しても有効に特徴を取得できるのではないだろうか&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;HANについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/N16-1174&quot;&gt;Yang, Zichao, et al. “Hierarchical attention networks for document classification.” Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/I17-2064&quot;&gt;Toyama, Yota, Makoto Miwa, and Yutaka Sasaki. “Utilizing Visual Forms of Japanese Characters for Neural Review Classification.” Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Vol. 2. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Realistic Evaluation of Semi-Supervised Learning Algorithms</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Realistic-Evaluation-of-Semi-Supervised-Learning-Algorithms" rel="alternate" type="text/html" title="Realistic Evaluation of Semi-Supervised Learning Algorithms" /><published>2018-04-28T00:00:00+00:00</published><updated>2018-04-28T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Realistic-Evaluation-of-Semi-Supervised-Learning-Algorithms</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Realistic-Evaluation-of-Semi-Supervised-Learning-Algorithms">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;現在SoTAである半教師あり学習のアルゴリズムについて、平等なテスト環境で性能を比較した。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Deep neural networkを学習させるためには大量の教師データが必要になるが、実際はデータが取りづらかったり、コストがかかる。
そこで教師ラベルのないデータセットも有効に活用する、半教師あり学習(SSL)が提案されている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Realistic-Evaluation-of-Semi-Supervised-Learning-Algorithms/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;先行研究で成果を上げているモデルは実際の使用環境を想定したモデルになっているかが疑問点としてあげられている。
本研究では現在デファクトスタンダートである半教師あり学習アルゴリズムに対して、それぞれ現実世界を想定した平等なテスト環境で性能を比較している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;従来の半教師あり学習の評価方法を見直し、現実世界での適用を想定した以下の評価方法を用いて先行研究のモデルを評価した。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shared Implementation
    &lt;ul&gt;
      &lt;li&gt;パラメータの初期化方法やデータの前処理、augmentation、正則化等を標準化する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-Quality Supervised Baseline
    &lt;ul&gt;
      &lt;li&gt;SSLのゴールは &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; と &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}_{UL}&lt;/script&gt; を用いて学習させたモデルが、 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; のみを用いて学習したモデルより良い精度を出すことである&lt;/li&gt;
      &lt;li&gt;そこで比較対象であるベースラインのモデルは &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; のみを用いて学習させるべき&lt;/li&gt;
      &lt;li&gt;ベースラインのモデルのパラメータ探索もSSLモデルと同様の回数探索するように設定&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Comparison to Transfer Learning
    &lt;ul&gt;
      &lt;li&gt;学習済みモデルをfine-tuningした結果はあまり報告されないので、本研究ではベースラインとして精度を報告する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Consider Class Distribution Mismatch
    &lt;ul&gt;
      &lt;li&gt;ラベル付きデータとラベルなしデータの分布の違いによる影響について報告する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Varying the Amount of Labeled and Unlabeled Data
    &lt;ul&gt;
      &lt;li&gt;ラベルなしデータは巨大である(インターネット上から取得)場合か、医療画像のようにデータの規模が小さい場合が考えられる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Realistically Small Validation Set
    &lt;ul&gt;
      &lt;li&gt;先行研究ではtrainingセットの中から一部ラベルを落としたデータを用いて学習させ、validationセットでモデルのチューニングをしていた。このときラベル有りデータの数はvalidationセットのほうが遥かに多い&lt;/li&gt;
      &lt;li&gt;現実世界ではラベルを多く含むデータセットで学習を行うため、先行研究の評価方法では実践的な評価ができていないため、本研究ではtrainingセットより小さいvalidationセットを用いてパラメータをチューニングする&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h3 id=&quot;使用したsslアルゴリズムについて&quot;&gt;使用したSSLアルゴリズムについて&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Author&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Stochastic Perturbations&lt;/td&gt;
      &lt;td&gt;Consistency Regularization&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6332-regularization-with-stochastic-transformations-and-perturbations-for-deep-semi-supervised-learning&quot;&gt;Sajjadi et al., 2016&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ⅱ-Model&lt;/td&gt;
      &lt;td&gt;Consistency Regularization&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02242&quot;&gt;Laine &amp;amp; Aila, 2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Temporal Embsembling&lt;/td&gt;
      &lt;td&gt;Consistency Regularization&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02242&quot;&gt;Laine &amp;amp; Aila, 2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mean Teacher&lt;/td&gt;
      &lt;td&gt;Consistency Regularization&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results&quot;&gt;Tarvainen &amp;amp; Valpola, 2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Virtual Adversarial Training&lt;/td&gt;
      &lt;td&gt;Consistency Regularization&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.03976&quot;&gt;Miyato et al., 2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Entropy-Based&lt;/td&gt;
      &lt;td&gt;Entropy-Based&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf&quot;&gt;Grandvalet &amp;amp; Bengio, 2005&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pseudo-Labeling&lt;/td&gt;
      &lt;td&gt;Pseudo-Labeling&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/blobs/download/forum-message-attachment-files/746/pseudo_label_final.pdf&quot;&gt;Lee, 2013&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;評価方法について&quot;&gt;評価方法について&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Reproduction
    &lt;ul&gt;
      &lt;li&gt;ベースモデルにWide ResNet (WRN-28-2) を使用&lt;/li&gt;
      &lt;li&gt;Google Cloud Machine Learning’s hyperparameter tuning serviceを用いてGaussian Process-based black box optimizationを行った&lt;/li&gt;
      &lt;li&gt;評価用データセットとしてSVHNとCIFAR-10を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully-Supervised Baselines&lt;/li&gt;
  &lt;li&gt;Transfer Learning&lt;/li&gt;
  &lt;li&gt;Class Distribution Mismatch&lt;/li&gt;
  &lt;li&gt;Varying Data Amounts&lt;/li&gt;
  &lt;li&gt;Small Validation Sets&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SSLの各アルゴリズムに対して分布が違うラベルなしデータを学習に使うと学習が上手く進まなかった&lt;/li&gt;
  &lt;li&gt;ラベルありデータと同様の分布からサンプリングされるラベルなしデータを使用すべきである&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Stochastic Perturbationsについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6332-regularization-with-stochastic-transformations-and-perturbations-for-deep-semi-supervised-learning&quot;&gt;Sajjadi, Mehdi, Mehran Javanmardi, and Tolga Tasdizen. “Regularization with stochastic transformations and perturbations for deep semi-supervised learning.” Advances in Neural Information Processing Systems. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ⅱ-Model / Temporal Ensemblingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02242&quot;&gt;Laine, Samuli, and Timo Aila. “Temporal ensembling for semi-supervised learning.” arXiv preprint arXiv:1610.02242 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Mean Teacherについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results&quot;&gt;Tarvainen, Antti, and Harri Valpola. “Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Virtual Adversarial Trainingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.03976&quot;&gt;Miyato, Takeru, et al. “Virtual adversarial training: a regularization method for supervised and semi-supervised learning.” arXiv preprint arXiv:1704.03976 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Entropy-basedな手法について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf&quot;&gt;Grandvalet, Yves, and Yoshua Bengio. “Semi-supervised learning by entropy minimization.” Advances in neural information processing systems. 2005.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pseudo-Labelingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/blobs/download/forum-message-attachment-files/746/pseudo_label_final.pdf&quot;&gt;Lee, Dong-Hyun. “Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.” Workshop on Challenges in Representation Learning, ICML. Vol. 3. 2013.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.09170&quot;&gt;Oliver, Avital, et al. “Realistic Evaluation of Semi-Supervised Learning Algorithms.” arXiv preprint arXiv:1804.09170 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Word Embedding Perturbation for Sentence Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Word-Embedding-Perturbation-for-Sentence-Classification" rel="alternate" type="text/html" title="Word Embedding Perturbation for Sentence Classification" /><published>2018-04-27T00:00:00+00:00</published><updated>2018-04-27T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Word-Embedding-Perturbation-for-Sentence-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Word-Embedding-Perturbation-for-Sentence-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;入力される単語embeddingに対していくつかのノイズで摂動を与え、文書分類における精度の検証する&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語処理では単語は離散的であり、連続空間では単語表現を変更できないため、一般的にdata augmentationは利用されてこなかった。&lt;/p&gt;

&lt;p&gt;近年ではシソーラスを用いた単語の置換や、2つの単語間の依存関係の向きを逆にすることで学習データを2倍に増やす手法が提案されている。これらは外部の知識体系が必要であったり、洗練されたNLPツールが必要である。&lt;/p&gt;

&lt;p&gt;またBag-of-Wordsに対してランダムにdropoutさせる手法を用いて、文書分類で性能を向上させたものがある。しかしながらノイズの与え方を比較していなかったり、単語の分散表現空間に対して適用していない問題がある。&lt;/p&gt;

&lt;p&gt;本研究では連続空間上の単語embeddingに対して複数種類のノイズで摂動を与え、文書分類における精度の検証を行っている。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;Word embeddingレイヤーに対して &lt;code class=&quot;highlighter-rouge&quot;&gt;word embedding perturbation&lt;/code&gt; を適用。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gaussian Noise
    &lt;ul&gt;
      &lt;li&gt;入力される単語embeddingに対して、ガウスノイズ &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; を適用する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_{emb} \leftarrow X_{emb} \odot e, e \sim \mathcal{N}(I, \sigma^{2}I)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Bernoulli Noise Augmentation
    &lt;ul&gt;
      &lt;li&gt;先行研究で提案されているDropoutを参考に、確率 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; で単語embeddingを0にする&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_{emb} \leftarrow \frac{1}{p} X_{emb} \odot e, e \sim \mathcal{B}(n, p)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Adversarial Training
    &lt;ul&gt;
      &lt;li&gt;loss関数が最大になるような方向にノイズを加える&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e \leftarrow e + \sigma \frac{g}{||g||}, g = \nabla_{e} L(X:\theta)&lt;/script&gt;

&lt;p&gt;文書の意味を考慮したrandom perturbationについて。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word Dropout
    &lt;ul&gt;
      &lt;li&gt;ベルヌーイ分布に従ってランダムに単語 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; をdropoutさせる&lt;/li&gt;
      &lt;li&gt;dropoutさせた単語は &lt;code class=&quot;highlighter-rouge&quot;&gt;UNK&lt;/code&gt; と同じ表現にする&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X \leftarrow X \odot \overrightarrow{e}, \overrightarrow{e} \sim \mathcal{B}(n, p)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic Dropout
    &lt;ul&gt;
      &lt;li&gt;単語embeddingの各次元はそれぞれセマンティックな意味を持っていると考えられる&lt;/li&gt;
      &lt;li&gt;単語間の共起を覚えさせるのではなく意味的な特徴を捉えてほしいため、単語embeddingの各次元をランダムにdropoutさせる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adversarial Noise
    &lt;ul&gt;
      &lt;li&gt;Gaussian adversarial noise
        &lt;ul&gt;
          &lt;li&gt;ガウス分布から &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; をサンプルして、adversarial trainingを適用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bernoulli adversarial noise
        &lt;ul&gt;
          &lt;li&gt;ベルヌーイ分布から &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; をサンプルして、adversarial trainingを適用&lt;/li&gt;
          &lt;li&gt;Adversarial dropoutを適用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;Moview review (MR)、The Stanford Sentiment Treebank (SST2)、Customer revirew (CR)、Question type (TREC)、SemEval2010 Task8 (RE)、Answer selection (TreeQA) の
データセットを用いてノイズを用いた各data augmentationの効果を確認している。&lt;/p&gt;

&lt;p&gt;モデルはmulti-channel CNN (Kim, 2014)を利用しており、300次元の学習済みword2vecを単語embeddingとして用いている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;連続性ノイズと離散性ノイズ
    &lt;ul&gt;
      &lt;li&gt;連続性ノイズ (Gaussian noise, Gaussian adversarial noise) のほうが僅かに離散性ノイズ (Bernoulli noise, Adversarial dropout) より良い結果になった&lt;/li&gt;
      &lt;li&gt;離散性ノイズは効果が強すぎたと考えられる&lt;/li&gt;
      &lt;li&gt;連続性ノイズのほうがエントロピーが大きく、学習に効果があると考えられる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学習データを少なくしたときの効果
    &lt;ul&gt;
      &lt;li&gt;データセットが少ない場合に、ベースラインよりもノイズの摂動が効果を発揮していることが分かる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Word-Embedding-Perturbation-for-Sentence-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;シソーラスを用いた単語の置換
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.01710&quot;&gt;Zhang, Xiang, and Yann LeCun. “Text understanding from scratch.” arXiv preprint arXiv:1502.01710 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;単語間の依存関係に着目したdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1601.03651&quot;&gt;Xu, Yan, et al. “Improved relation classification by deep recurrent neural networks with data augmentation.” arXiv preprint arXiv:1601.03651 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BoWに対してdropoutを適用
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-1162&quot;&gt;Iyyer, Mohit, et al. “Deep unordered composition rivals syntactic methods for text classification.” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vol. 1. 2015.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-50496-4_59&quot;&gt;Zhang, Dongxu, Tianyi Luo, and Dong Wang. “Learning from LDA using deep neural networks.” Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016. 657-664.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adversarial Dropout
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.03631&quot;&gt;Park, Sungrae, et al. “Adversarial Dropout for Supervised and Semi-supervised Learning.” arXiv preprint arXiv:1707.03631 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ベースラインの multi-channel CNN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1408.5882&quot;&gt;Kim, Yoon. “Convolutional neural networks for sentence classification.” arXiv preprint arXiv:1408.5882 (2014).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.08166&quot;&gt;Zhang, Dongxu, and Zhichao Yang. “Word Embedding Perturbation for Sentence Classification.” arXiv preprint arXiv:1804.08166 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Learning to Compute Word Embeddings On the Fly</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Learning-to-Compute-Word-Embeddings-On-the-Fly" rel="alternate" type="text/html" title="Learning to Compute Word Embeddings On the Fly" /><published>2018-04-01T00:00:00+00:00</published><updated>2018-04-01T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Learning-to-Compute-Word-Embeddings-On-the-Fly</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Learning-to-Compute-Word-Embeddings-On-the-Fly">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;OOV問題に対してWordNetの単語定義文をエンコードし未知語に対処する，On the Fly Embdddingsを提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語では頻繁に出現する単語もあるが，ほとんどがZipfian分布に従うような，あまり現れない単語から形成されている．
こうした単語の低頻度の単語はout-of-vocabulary (OOV) 問題として扱われる．&lt;/p&gt;

&lt;p&gt;先行研究ではボキャブラリ外の単語を固定のランダムベクトルで代用する研究があり，効果を示している．
またスペル情報からBi-directional LSTMを用いて，ボキャブラリ外の単語を一般化する先行研究がある．
本研究に最も近い先行研究では，対象となる単語埋め込みに近い辞書定義埋め込みを生成するものがある．&lt;/p&gt;

&lt;p&gt;本研究では特定のタスク対して辞書定義エンコーダをend-to-endで学習を行い，
エンコードした定義文と補助ベクトルを用いてボキャブラリ外の未知語に対応するモデルを提案する．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Learning-to-Compute-Word-Embeddings-On-The-Fly/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On the Fly embeddings
    &lt;ul&gt;
      &lt;li&gt;辞書 (WordNet) 定義文をエンコードする &lt;em&gt;definition reader&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;simple mean pooling (MP)&lt;/li&gt;
          &lt;li&gt;mean pooling (MP-L)
            &lt;ul&gt;
              &lt;li&gt;学習可能な行列を辞書内単語ベクトルに掛ける&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;LSTMの最終ステート&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;補助ベクトル
        &lt;ul&gt;
          &lt;li&gt;WordNetの定義分からなるベクトル表現&lt;/li&gt;
          &lt;li&gt;単語の文字列情報 (&lt;code class=&quot;highlighter-rouge&quot;&gt;word&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;d&lt;/code&gt;)&lt;/li&gt;
          &lt;li&gt;大規模なデータセットで学習済みのGloVeベクトル&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;p&gt;提案手法に対して Question Answering，Semantic Entailment Classification，Language Modellingの3つのタスクで評価を行っている．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Question Answering
    &lt;ul&gt;
      &lt;li&gt;Stanford Question Answering Dataset (SQuAD)を用いて提案手法の評価を行っている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Semantic Entailment Classification
    &lt;ul&gt;
      &lt;li&gt;Stanford Natural Language Inference (SNLI) コーパスとMulti-Genre Natural Language Inference (MultiNLI) コーパスを用いて提案手法の評価を行っている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Language Modelling
    &lt;ul&gt;
      &lt;li&gt;One Billion Words (OBW) language modellingタスクで提案手法の評価を行っている．&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Future work
    &lt;ul&gt;
      &lt;li&gt;複数単語からなる熟語は考慮されていない
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;give up&lt;/code&gt; などの熟語&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;San Francisco&lt;/code&gt; のような地名&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;辞書定義中の未知語への対応&lt;/li&gt;
      &lt;li&gt;循環参照してしまっている単語への対応
        &lt;ul&gt;
          &lt;li&gt;(例 &lt;code class=&quot;highlighter-rouge&quot;&gt;hoge&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;fuga&lt;/code&gt;のこと，&lt;code class=&quot;highlighter-rouge&quot;&gt;fuga&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;hoge&lt;/code&gt;のこと)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;OOV単語に対して固定のランダムベクトルを適用
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.00993&quot;&gt;Dhingra, Bhuwan, et al. “A comparative study of word embeddings for reading comprehension.” arXiv preprint arXiv:1703.00993 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;スペルからOOV単語を一般化する試み
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.02096&quot;&gt;Ling, Wang, et al. “Finding function in form: Compositional character models for open vocabulary word representation.” arXiv preprint arXiv:1508.02096 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;対象となる単語埋め込みに近い辞書定義埋め込みを生成
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/Q16-1002&quot;&gt;Hill, Felix, et al. “Learning to Understand Phrases by Embedding the Dictionary.” Transactions of the Association for Computational Linguistics 4 (2016): 17-30.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.00286&quot;&gt;Bahdanau, Dzmitry, et al. “Learning to Compute Word Embeddings On the Fly.” arXiv preprint arXiv:1706.00286 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Glyph-aware Embedding of Chinese Characters</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters" rel="alternate" type="text/html" title="Glyph-aware Embedding of Chinese Characters" /><published>2018-03-26T00:00:00+00:00</published><updated>2018-03-26T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字特有の文字の表意性を明示的に組み込み，文字形状を意識した文字の埋め込み手法を提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;英語と比べて中国語は多数の文字が使用され，なおかつ単語と単語の境目が明確ではない．
漢字には形状的な特徴があり，特に”へん”や”つくり”などの部分的な構造が集まり，それら自身が文字の意味・構文的役割・発音などの情報を有している．&lt;/p&gt;

&lt;p&gt;本研究ではこれら漢字の文字形状に着目することで，優れた文字表現の獲得を目指す &lt;em&gt;glyph-aware embedding&lt;/em&gt; を提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Glyph-aware-Embedding-of-Chinese-Characters/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN embedder
    &lt;ul&gt;
      &lt;li&gt;文字形状を考慮した文字の埋め込みをCNNを用いて実現&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ID embedder
    &lt;ul&gt;
      &lt;li&gt;文脈を考慮した文字の埋め込みをLookup tableを用いて実現&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文字形状のみ考慮した場合，「土」⇆「士」や「人」⇆「入」といった文字形状が似ている文字が近い表現となってしまう場合が考えられる．
そこで文脈を考慮できるLookup tableによるID embeddingを併せて用いることで，この問題を解消している．&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;中国語の言語モデリングタスクと中国語の単語分割タスクについて提案手法の精度を検証している．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;言語モデリングタスク
    &lt;ul&gt;
      &lt;li&gt;embedderで文字表現を得た後，後段のGRUで言語モデルタスクを解いている&lt;/li&gt;
      &lt;li&gt;ベースラインとして全結合層のみからなるlinear embedderを設定&lt;/li&gt;
      &lt;li&gt;Microsoft Research dataset (MSR) を用いて，CNN embedder・ID embedder・ID+CNN embedderそれぞれのperplexityを比較している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;単語分割タスク
    &lt;ul&gt;
      &lt;li&gt;embedderで文字表現を得た後，後段のGRU/Bidirectional LSTMで単語分割タスクを解いている&lt;/li&gt;
      &lt;li&gt;Peking University dataset (PKU) および MSRを用いて，CNN embedder・ID embedder・ID+CNN embedderそれぞれの精度を比較している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各タスクともCNN embedderが優れた性能を示している．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;入力文字画像にdata augmentationを加えることで精度が向上&lt;/li&gt;
  &lt;li&gt;CNN embedder / ID embedderのパラメータ数について
    &lt;ul&gt;
      &lt;li&gt;埋め込み次元 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; ，ボキャブラリサイズ &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の場合&lt;/li&gt;
      &lt;li&gt;CNN embedder: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(N+K)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;ID embedder: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(NK)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;CNN embedderのほうがID embdderよりも少ないパラメータ数で良いパフォーマンスを出していることが分かる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-2098&quot;&gt;Shi, Xinlei, et al. “Radical embedding: Delving deeper to chinese radicals.” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Vol. 2. 2015.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04859&quot;&gt;Liu, Frederick, et al. “Learning character-level compositionality with visual features.” arXiv preprint arXiv:1704.04859 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/s10590-017-9196-0&quot;&gt;Costa-Jussà, Marta R., David Aldón, and José AR Fonollosa. “Chinese–spanish neural machine translation enhanced with character and word bitmap fonts.” Machine Translation 31.1-2 (2017): 35-47.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.00028&quot;&gt;Dai, Falcon Z., and Zheng Cai. “Glyph-aware Embedding of Chinese Characters.” arXiv preprint arXiv:1709.00028 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">A New Method of Region Embedding for Text Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification" rel="alternate" type="text/html" title="A New Method of Region Embedding for Text Classification" /><published>2018-02-20T00:00:00+00:00</published><updated>2018-02-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;CNNやRNNを必要とせずに語順を考慮することができるLocal Context Unitを利用し、タスク固有の単語埋め込み表現を学習するRegion Embeddingを提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;文書分類などのタスクにおいて単語の語順を考慮した単語表現にn-gramが用いられることが多いが、
特に &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; の値が大きいn-gramの場合、モデルが大きくなってしまったり、データスパースネス問題が起こる恐れがある。&lt;/p&gt;

&lt;p&gt;近年ではn-gramを考慮した単語の分散表現を獲得するFastTextが提案されている。
また&lt;a href=&quot;/paper-survey/summry/nlp/Semi-supervised-Convolutional-Neural-Networks-for-Text-Categorization-via-Region-Embedding&quot;&gt;Johnson &amp;amp; Zhang (2015)&lt;/a&gt;では
CNNベースのモデルを用いて単語表現を獲得するregion embeddingという手法を提案しているが、本研究のregion embeddingとは異なり、タスク依存でない点や、教師なし学習の枠組みで学習されている点で異なっている。&lt;/p&gt;

&lt;p&gt;Attentionのみを使用したニューラル機械翻訳モデルであるTransformerは、CNNやRNNを用いずに語順を考慮し、文脈の特徴を学習できていることが示されている。&lt;/p&gt;

&lt;p&gt;本研究ではTransformer参考に、ある単語の周辺のコンテキストを考慮できるLocal context unitを用いて単語の埋め込み表現を獲得するRegion Embeddingを提案し、文書分類タスクで精度が上がることを示している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Region Embedding
    &lt;ul&gt;
      &lt;li&gt;テキスト中の小さな範囲(region)から、局所的な特徴を保持した表現を獲得したい&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Local Context Unit
    &lt;ul&gt;
      &lt;li&gt;ある単語の語順と周辺のコンテキストを学習するパラメータ&lt;/li&gt;
      &lt;li&gt;通常のlook up tableを用いたword embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf e}_{w_i}&lt;/script&gt; とlocal context unit &lt;script type=&quot;math/tex&quot;&gt;{\bf K}_{W_i}&lt;/script&gt;を組み合わせた埋め込み表現 &lt;script type=&quot;math/tex&quot;&gt;p^{i}_{w_i}&lt;/script&gt; を学習する&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  p^{i}_{w_i + t} = {\bf K}_{w_i, t} \odot {\bf e}_{w_i + t}
\end{align*}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Word-Context Region Embedding
    &lt;ul&gt;
      &lt;li&gt;regionの中心の語が前後のコンテキストから受ける影響に焦点を当てたemnedding手法&lt;/li&gt;
      &lt;li&gt;語の出現順によって(特に否定語や強調語など)、意味が逆転する場合を上手く学習することを期待&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Context-Word Region Embedding
    &lt;ul&gt;
      &lt;li&gt;Word-Context Region Embeddingとは逆に、コンテキストがregionの中心語から受ける影響に焦点を当てている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Region Embeddingを全結合層で分類&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;8つのデータセットを用いて感情分析、新聞記事分類、QAなどのタスクに対して精度を比較している。
ベースラインのn-gram・TFIDFなどの従来の単語表現を用いたモデル、Char-CNN、Char-CRNN、VDCNN、D-LSTM、bigram-FastTextと先行研究のRegion Embeddingを用いた分類器を比較している。
8つのデータセットのうち6つのデータセットで最先端の結果を達成していることが示されている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;region size / embedding sizeを変えた場合について
    &lt;ul&gt;
      &lt;li&gt;本研究ではregion sizeを7、embedding sizeを128に設定している&lt;/li&gt;
      &lt;li&gt;複数サイズのregion embeddingを組み合わせることで僅かに精度が向上している&lt;/li&gt;
      &lt;li&gt;emnedding sizeを大きくすると従来のFastTextやCNNなどは過学習が見られるが、Region Embeddingはほかと比べてロバストであることが示されている
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ある単語における周辺単語の共起について
    &lt;ul&gt;
      &lt;li&gt;“however”についてはhoweverの後が文書分類に重要であることを示している&lt;/li&gt;
      &lt;li&gt;“very”についてはveryの直後の語が重要である etc&lt;/li&gt;
      &lt;li&gt;local context unitが局所的な潜在意味を捉えていることが分かる
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;感情分析におけるlocal context unitの効果の可視化
    &lt;ul&gt;
      &lt;li&gt;context unitがある場合、正しく形容詞が正しく係り、positive/negativeの判定が正しく行われるようになったことが示されている
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/table4.png&quot; alt=&quot;Table 4&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;FastTextを用いたembeddingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;Joulin, Armand, et al. “Bag of tricks for efficient text classification.” arXiv preprint arXiv:1607.01759 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNNを用いたembeddingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.1058&quot;&gt;Johnson, Rie, and Tong Zhang. “Effective use of word order for text categorization with convolutional neural networks.” arXiv preprint arXiv:1412.1058 (2014).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformerについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ベースラインのモデルについて
    &lt;ul&gt;
      &lt;li&gt;Char-CNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica&quot;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Char-CRNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.00367&quot;&gt;Xiao, Yijun, and Kyunghyun Cho. “Efficient character-level document classification by combining convolution and recurrent layers.” arXiv preprint arXiv:1602.00367 (2016).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;VDCNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01781&quot;&gt;Conneau, Alexis, et al. “Very deep convolutional networks for natural language processing.” arXiv preprint arXiv:1606.01781 (2016).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;D-LSTMについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01898&quot;&gt;Yogatama, Dani, et al. “Generative and discriminative text classification with recurrent neural networks.” arXiv preprint arXiv:1703.01898 (2017).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;予測における寄与単語の可視化について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01066&quot;&gt;Li, Jiwei, et al. “Visualizing and understanding neural models in NLP.” arXiv preprint arXiv:1506.01066 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BkSDMA36Z&quot;&gt;Chao Qiao, Bo Huang, Guocheng Niu, Daren Li, Daxiang Dong, Wei He, Dianhai Yu, Hua Wu, “A New Method of Region Embedding for Text Classification,” International Conference on Learning Representations, 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Variable Importance Using Decision Tree</title><link href="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees" rel="alternate" type="text/html" title="Variable Importance Using Decision Tree" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;決定木とランダムフォレストは優れたパフォーマンスと示すだけでなく、Feature importanceといった情報が分かる、確立されたモデルである。 不純度ベースで変数の重要度を知ることができるため使われることが多いモデルであるが、これらの重要度は理論的に不明な点が多い。&lt;/p&gt;

&lt;p&gt;本研究ではDSTUMPを提案し、様々な仮定におけるモデリングの下で高次元のデータ利用で有限のサンプルパフォーマンス保証性を導き出すことによって、ツリー系のモデルの性能についての考察を行っている。またこれら不純度ベースの手法の有効性について、広範囲の実験を下に有効性を実証している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Variable-Importance-Using-Decision-Trees/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6646-variable-importance-using-decision-trees&quot;&gt;Kazemitabar, Jalil, et al. “Variable Importance using Decision Trees.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</title><link href="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data" rel="alternate" type="text/html" title="Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では教師なしの連続データに対して解釈可能な表現を学習するFactrized hierarchical variational autoencoderを提案している。具体的には潜在変数のことなるデータに対し、事前確率としてSequence-dependent priorsとSequence-independent priorsをFactorized hierarchical graphical model内で組み合わせることで、連続データが持つマルチスケールな情報を利用するモデルとなっている。&lt;/p&gt;

&lt;p&gt;本モデルは2つの音声コーパスTIMITとAurora-4を用いて評価を行っている。具体的には異なる潜在変数の組を使って、スピーカーや言語コンテンツを変換する能力を定性的に評価している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6784-unsupervised-learning-of-disentangled-latent-representations-from-sequential-data&quot;&gt;Hsu, Wei-Ning, Yu Zhang, and James Glass. “Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Sparse Embedded k-Means Clustering</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering" rel="alternate" type="text/html" title="Sparse Embedded k-Means Clustering" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;K-meansクラスタリングは広く知られている素晴らしいアルゴリズムであるが、高次元のデータに対しては、計算コストの高さゆえに様々な分野への応用を妨げている現状がある。一般的には次元削減の手法を用いて対処することが多いが、近年Random projection(RP)などの手法を用いて高速なK-meansクラスタリングを実現することができる。しかしながらこの手法は他の次元削減手法よりも多くの改善点が存在している。例として特異値分解(SVD)に基づく特徴抽出手法と比較して、RPは近似を行いつつ、データ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; で特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; のデータ &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbb{R}^{n \times d}&lt;/script&gt; に対して &lt;script type=&quot;math/tex&quot;&gt;\min{(n, d)}\epsilon^{2} \frac{\log{(d)}}{k}&lt;/script&gt; だけ実行時間を削減している。&lt;/p&gt;

&lt;p&gt;これらの改善を経てもなお行列の乗算には &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O} \left(\frac{ndk}{\epsilon^{2} \log{(d)}} \right)&lt;/script&gt; だけ必要であり、特にデータ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; や特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; が大きい場合にはとても大きな計算コストとなってしまう。これらのボトルネックを解消するために、本研究では &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(nnz(X))&lt;/script&gt; ( &lt;script type=&quot;math/tex&quot;&gt;nnz(X)&lt;/script&gt; は &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 内における非ゼロの数を表している) を必要とする高速な行列の乗算を行う枠組みを用いて、スパースな埋め込み表現に対してk-meansクラスタリングを行う手法を提案をしている。また本研究ではRPの近似精度についても改善を行っている。ILSVRC2012等のデータセットに対して従来の次元削減手法を次元を落としてからk-meansクラスタリングをした結果と、提案手法の高速な次元圧縮を利用したクラスタリング結果を比較している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Sparse-Embedded-k-Means-Clustering/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6924-sparse-embedded-k-means-clustering.pdf&quot;&gt;Liu, Weiwei, Xiaobo Shen, and Ivor Tsang. “Sparse Embedded $ k $-Means Clustering.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks</title><link href="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks" rel="alternate" type="text/html" title="SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では特異値分解を利用した、とても大きいボキャブラリを持つSoftmax関数を高速に近似する手法を提案している。SVD-Softmaxはニューラル言語モデルの推論時に最上位となりうる単語について、高速かつ正確に確率推定を行うことを目的としている。提案手法ではSVDを使って出力ベクトルの計算に用いた重み行列を変換する。各単語の近似確率については、いくつかの大きな特異値を使用することで単語の大部分の性質を持たせることができ、これを利用して重み行列を推定できると主張している。&lt;/p&gt;

&lt;p&gt;本研究の手法を言語モデリングとニューラル機械翻訳に適用することで、提案手法で導入されている近似手法が効果を発揮していることを検証している。本アルゴリズムでは800,000個の語彙の場合においても、約20%程度の算術演算しか必要とせず、GPUを利用することで3倍以上スピードアップしていることが分かっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks&quot;&gt;Shim, Kyuhong, et al. “SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>