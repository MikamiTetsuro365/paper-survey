<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2019-02-23T15:48:33+00:00</updated><id>https://shunk031.github.io/paper-survey/feed.xml</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">SC-FEGAN:Face Editing Generative Adversarial Networks with Users Sketch and Color</title><link href="https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color" rel="alternate" type="text/html" title="SC-FEGAN:Face Editing Generative Adversarial Networks with Users Sketch and Color" /><published>2019-02-21T00:00:00+00:00</published><updated>2019-02-21T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ユーザーのインタラクティブなスケッチや色指定が可能なニューラルネットワークベースの顔画像編集システム SC-FEGAN を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;従来の Generative Adversarial Network (GAN) を用いた顔画像編集システムは低画質でエッジを適切に捉えるのが難しかった。
また顔編集を行う際のユーザの入力に対して質の高い応答を返すのは困難であった。&lt;/p&gt;

&lt;p&gt;Deepfillv2 や Guided inpainting はこうしたユーザーが入力するマスキングや他の画像を入力として編集を可能としたが、編集時の色の指定ができなかったり、詳細な細かい復元が可能ではなかった。&lt;/p&gt;

&lt;p&gt;本研究では UNet ベースのアーキテクチャに対して gated convolutional layer を導入したネットワークアーキテクチャをベースにユーザーのスケッチや色指定を可能とした、
自由で高品質な顔編集システムである SC-FEGAN を提案した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;学習データに対する処理&quot;&gt;学習データに対する処理&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;高品質な瞳の編集
    &lt;ul&gt;
      &lt;li&gt;学習時にランダムに目周辺にマスキングを適用することで細かい瞳の復元を可能とする&lt;/li&gt;
      &lt;li&gt;同時に generative face completion (GFC) を適用する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ユーザーが入力するスケッチや色指定への対応
    &lt;ul&gt;
      &lt;li&gt;FaceShop と同様の手法を導入
        &lt;ul&gt;
          &lt;li&gt;スケッチデータをビットマップからベクターへと変換する&lt;a href=&quot;http://autotrace.sourceforge.net/&quot;&gt;AutoTrace&lt;/a&gt;は用いていない&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HED によるエッジ検出器を使用して、ユーザーの入力からスケッチデータを生成&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;提案顔編集システムのアーキテクチャ&quot;&gt;提案顔編集システムのアーキテクチャ&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generator
    &lt;ul&gt;
      &lt;li&gt;U-net ベースの generator を採用
        &lt;ul&gt;
          &lt;li&gt;すべての畳み込み層に gated convolution を導入している&lt;/li&gt;
          &lt;li&gt;畳み込みの後に local signal normalization (LRN)を用いている&lt;/li&gt;
          &lt;li&gt;入力は&lt;script type=&quot;math/tex&quot;&gt;512 \times 512 \times 9&lt;/script&gt;である
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RGB (3 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;マスク (1 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;スケッチ(1 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;色指定マップ(3 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;ノイズ(1 チャンネル)&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminator
    &lt;ul&gt;
      &lt;li&gt;SN-patchGAN ベースの discriminator を採用
        &lt;ul&gt;
          &lt;li&gt;複数の loss 関数を最小化する
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;per-pixcel loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;perceptual loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;style loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;total variance loss&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;CelebA-HO データセットにおいてランダムに学習用とテスト用で分割したものに対して提案システムである SC-FEGAN による顔編集の質を検討している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;generator-の違いによる瞳の編集精度&quot;&gt;Generator の違いによる瞳の編集精度&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generator を U-net と Coarse-Refined net で変えたときに、瞳領域をマスクして復元したときの結果。U-net 構造のほうが細かい瞳の復元に成功している&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;perceptual-loss-の有無による編集精度&quot;&gt;Perceptual loss の有無による編集精度&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Perceptual loss である VGG loss を導入することにより、髪部分が正確に編集できている&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;先行研究との復元精度の比較&quot;&gt;先行研究との復元精度の比較&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure6.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;先行研究の Deepfillv1 に対して提案システムである SC-FEGAN による復元精度が高いことがわかる&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;アクセサリーなどの小さい物体に対する編集の可能性&quot;&gt;アクセサリーなどの小さい物体に対する編集の可能性&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure8.png&quot; alt=&quot;Figure 8&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HED をもとにマスキング領域を拡張して学習することにより、イヤリングと共に顔の画像を生成するという特別な結果を得ることができた&lt;/li&gt;
  &lt;li&gt;ネットワークが小さな詳細を学習し、小さな入力に対しても妥当な結果を生み出すことができることを示している&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Deepfillv2 (SN-patchGAN, gated convolutional layer) について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;&gt;Yu, Jiahui, et al. “Free-form image inpainting with gated convolution.” arXiv preprint arXiv:1806.03589 (2018).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Guided inpainting について
    &lt;ul&gt;
      &lt;li&gt;[Zhao, Yinan, et al. “Guided image inpainting: Replacing an image region by pulling content from another image.” arXiv preprint arXiv:1803.08435 (2018).]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative Face Completion (GFC) について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2017/html/Li_Generative_Face_Completion_CVPR_2017_paper.html&quot;&gt;Li, Yijun, et al. “Generative face completion.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FaceShop について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.08972&quot;&gt;Portenier, Tiziano, et al. “Faceshop: Deep sketch-based face image editing.” ACM Transactions on Graphics (TOG) 37.4 (2018): 99.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;U-net について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~jeanoh/16-785/papers/ronnenberger-miccai2015-u-net.pdf&quot;&gt;Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.06838&quot;&gt;Youngjoo Jo, Jongyoul Park. SC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color. arXiv:1902.06838, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Field-aware Probabilistic Embedding Neural Network for CTR Prediction</title><link href="https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction" rel="alternate" type="text/html" title="Field-aware Probabilistic Embedding Neural Network for CTR Prediction" /><published>2018-12-06T00:00:00+00:00</published><updated>2018-12-06T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;確率的埋め込みを導入し特徴表現の確率的振る舞いを学習する CTR 予測のためのモデル Field-aware Probabilistic Embedding Neural Network を提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Click Though Rate (CTR) 予測に通常用いられるロジスティック回帰 (LR) や Factorization Machine (FM) は一般的な線形の特徴を捉えることが可能でシンプルな実装であることが知られているが、高次元特徴の表現能力に乏しい。&lt;/p&gt;

&lt;p&gt;本件研究では特徴の embedding に対して点推定ではなく分布を推定する Probabilistic Embedding という新たな embedding 手法を提案し、事前知識を導入することで先行研究よりもロバストなモデルを構築した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Field-aware Probabilistic Embedding Neural Networks (FPENN)
    &lt;ul&gt;
      &lt;li&gt;Field-aware Probabilistic Embedding (FPE) による確率的埋め込みの学習
        &lt;ul&gt;
          &lt;li&gt;埋め込み行列 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の平均・分散からなる確率分布を用いて確率的な振る舞いを導入&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多様な特徴を学習するコンポーネント
        &lt;ul&gt;
          &lt;li&gt;Linear term (LT)
            &lt;ul&gt;
              &lt;li&gt;低次元の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Quadratic term (QDR)
            &lt;ul&gt;
              &lt;li&gt;2 次の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Deep NN term (DNN)
            &lt;ul&gt;
              &lt;li&gt;高次元の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;埋め込みを確率分布としたときの学習手法の適用
        &lt;ul&gt;
          &lt;li&gt;Reparameterization trick
            &lt;ul&gt;
              &lt;li&gt;埋め込み行列 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の平均・分散を持つ正規分布から埋め込み表現をサンプリングする場合誤差逆伝播が不可能&lt;/li&gt;
              &lt;li&gt;Reparameterization trick でこの問題を解決&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;CTR 予測のデータセットとして Avazu dataset と Criteo dataset を用いて提案する FPENN とベースラインの LR や FM、CNN ベースの CCPM や DeepFM と比較を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;提案手法が 50 〜 100 ミリ秒以内に 200 〜 300 アプリの CTR 値を予測との記述あり&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CCPM について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2806603&quot;&gt;Liu, Qiang, et al. “A convolutional click prediction model.” Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DeepFM について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04247&quot;&gt;Guo, Huifeng, et al. “Deepfm: a factorization-machine based neural network for ctr prediction.” arXiv preprint arXiv:1703.04247 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3240396&quot;&gt;Weiwen Liu, Ruiming Tang, Jiajin Li, Jinkai Yu, Huifeng Guo, Xiuqiang He, and Shengyu Zhang. 2018. Field-aware probabilistic embedding neural network for CTR prediction. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys ‘18). ACM, New York, NY, USA, 412-416.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/shunk031/paper-summary-of-field-aware-probabilistic-embedding-neural-network-for-ctr-prediction&quot;&gt;[論文紹介] Field-aware Probabilistic Embedding Neural Network for CTR Prediction [RecSys 2018] / Paper summary of Field-aware Probabilistic Embedding Neural Network for CTR Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shunk031.github.io/paper-survey/assets/img/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction/figure1.png" /></entry><entry><title type="html">Context-Dependent Sentiment Analysis in User-Generated Videos</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos" rel="alternate" type="text/html" title="Context-Dependent Sentiment Analysis in User-Generated Videos" /><published>2018-10-22T00:00:00+00:00</published><updated>2018-10-22T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;マルチモーダル (テキスト・音声・動画) における発音レベルの特徴量を用いた感情分析を行うモデルを提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;マルチモーダルを用いた感情分析を行うモデルは複数提案されてきたが、先行研究では発話レベルでの依存関係を無視したモデルのみしか提案されていなかった。
本研究では発話中の呼吸や小休止で区切り 1 つの単位とする &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; (発話レベル) の特徴量にフォーカスを当て、これらの系列を元にマルチモーダルの特徴を用いて感情分析を行った。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;
&lt;p&gt;マルチモーダル(テキスト・音声・動画)から &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; の特徴を抽出し、&lt;code class=&quot;highlighter-rouge&quot;&gt;Contextual-LSTM&lt;/code&gt; を用いてこれらの系列を考慮した感情分析モデルを構築&lt;/p&gt;

&lt;h3 id=&quot;発話レベルの特徴量抽出&quot;&gt;発話レベルの特徴量抽出&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;テキストからの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;text-CNN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;学習済み word2vec を入力し、複数種類のカーネルを持つ 2 層の CNN でテキストのセマンティクスを抽出&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;音声からの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;openSMILE&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;オープンソースのソフトウェアである openSMILE を用いて、音声のピッチや強度といった特徴量を抽出&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;動画からの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3D-CNN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;通常の 2D の CNN に対して時間軸方向を考慮した 3D の CNN を使用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;コンテキストを考慮した特徴抽出&quot;&gt;コンテキストを考慮した特徴抽出&lt;/h3&gt;
&lt;p&gt;抽出した &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; の特徴量に対して、発話ごとの関係性を学習する LSTM (&lt;code class=&quot;highlighter-rouge&quot;&gt;Contextual-LSTM&lt;/code&gt;) を用いる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Contextual-LSTM は複数種類の LSTM を比較&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sc-LSTM&lt;/code&gt; (simple contextual LSTM)
    &lt;ul&gt;
      &lt;li&gt;順方向のみのシンプルな LSTM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;h-LSTM&lt;/code&gt; (hidden LSTM)
    &lt;ul&gt;
      &lt;li&gt;LSTM の後に全結合層を入れずにそのまま隠れ状態を後段に渡す&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bc-LSTM&lt;/code&gt; (bi-directional LSTM)
    &lt;ul&gt;
      &lt;li&gt;発話に対して双方向の特徴を考慮できる bi-directional LSTM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;uni-SVM&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ベースラインとして SVM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;複数のモダリティ特徴を用いた感情分析&quot;&gt;複数のモダリティ特徴を用いた感情分析&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;複数のモダリティに対して階層構造を考慮する (Hierarchical Framework)
    &lt;ul&gt;
      &lt;li&gt;Level-1
        &lt;ul&gt;
          &lt;li&gt;各モダリティ (テキスト・音声・動画) の特徴量に対してそれぞれ独立で LSTM に通し、対象モダリティ独自のコンテキストをを学習&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Level-2
        &lt;ul&gt;
          &lt;li&gt;Level-1 で学習した各モダリティ独立した特徴を concat して LSTM に通すことでマルチモーダルなコンテキストを学習&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Level-1 と Level-2 は end-to-end での学習ではない&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;階層構造を考慮しない枠組み (Non-hierarchical Framework) についても比較
    &lt;ul&gt;
      &lt;li&gt;単一のモダリティ特徴を contextual-LSTM (sc-LSTM や bc-LSTM) に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;p&gt;マルチモーダルな感情分析を行う先行研究の大部分の検証実験において、学習データとテストデータに同じ話者が入っている場合が多い。
話者が同じ場合であっても、感情表現やセマンティクスはデータによって異なると考えられているためである。
しかしながら感情分析に対して一般性・話者に依存しないモデリングを行うことは重要である。&lt;/p&gt;

&lt;p&gt;本研究での検証実験では現実世界に即したアプリケーションを目指すため、未知の話者に対してもモデルがロバストになるように、
話者独立でデータセットの train/test 分割を行っている。&lt;/p&gt;

&lt;h3 id=&quot;マルチモーダルな感情分析を検証するためのデータセットについて&quot;&gt;マルチモーダルな感情分析を検証するためのデータセットについて&lt;/h3&gt;
&lt;h4 id=&quot;mosi&quot;&gt;MOSI&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;93 人が英語である話題についてレビューしている動画が含まれる&lt;/li&gt;
  &lt;li&gt;5 人のアノテーターが &lt;code class=&quot;highlighter-rouge&quot;&gt;-3&lt;/code&gt;〜&lt;code class=&quot;highlighter-rouge&quot;&gt;+3&lt;/code&gt; の感情値を付与
    &lt;ul&gt;
      &lt;li&gt;ラベルの平均値を計算し、 &lt;code class=&quot;highlighter-rouge&quot;&gt;positive&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;negative&lt;/code&gt; の 2 クラスとして使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;moud&quot;&gt;MOUD&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;55 人がスペイン語である製品についてレビューしている動画が含まれる&lt;/li&gt;
  &lt;li&gt;Google Translate API を用いてスペイン語から英語に変換&lt;/li&gt;
  &lt;li&gt;positive, negative, neutral のラベルが付与されているが、&lt;code class=&quot;highlighter-rouge&quot;&gt;positive&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;negative&lt;/code&gt;のみ使用&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;iemocap&quot;&gt;IEMOCAP&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;10 人が英語で対話している動画データセット&lt;/li&gt;
  &lt;li&gt;データセットには以下 9 つのラベルが付与されている: &lt;code class=&quot;highlighter-rouge&quot;&gt;anger&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;happiness&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sadness&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;neutral&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;excitement&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;frustration&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;fear&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;surprise&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;other&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;先行研究と比較するために上記最初の 4 つのクラスだけをラベルとして使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;8 人を学習データ、2 人をテストデータとして使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;また汎化性能を確認するため、MOSI データセットで学習したモデルを MOUD データセットで評価する &lt;code class=&quot;highlighter-rouge&quot;&gt;cross-dataset&lt;/code&gt; の枠組みを導入している。&lt;/p&gt;

&lt;h3 id=&quot;モデルのパフォーマンス比較&quot;&gt;モデルのパフォーマンス比較&lt;/h3&gt;
&lt;h4 id=&quot;階層的-vs-非階層的な学習フレームワーク&quot;&gt;階層的 vs 非階層的な学習フレームワーク&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;非階層的なフレームワークはベースラインの uni-SVM を超えたが、階層的フレームワークが一番良かった&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;lstm-の種類の違いによる比較&quot;&gt;LSTM の種類の違いによる比較&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;sc-LSTM、bc-LSTM ともに良い結果を出している&lt;/li&gt;
  &lt;li&gt;bc-LSTM は順方向・逆方向のコンテキストを考慮できるため sc-LSTM より優れていた&lt;/li&gt;
  &lt;li&gt;全結合層がない場合 (h-LSTM) よりある場合のほうが性能は高い&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ベースラインとの比較&quot;&gt;ベースラインとの比較&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM を用いたネットワークはすべてのデータセットに対してベースラインの uni-SVM を超える性能を出した
    &lt;ul&gt;
      &lt;li&gt;発話間の文脈依存性をモデリングすることで感情分類性のを高めている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IEMOCAP データセットでは特にベースラインとの性能差が現れた
    &lt;ul&gt;
      &lt;li&gt;より広い依存関係を適切に捉える必要があるため、LSTM ネットワークが効果的であった&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SoTA のモデルとの比較
    &lt;ul&gt;
      &lt;li&gt;SoTA モデルは実験の際に話者が独立するように train/test の分割をしていない&lt;/li&gt;
      &lt;li&gt;発話の文脈依存を考慮していない&lt;/li&gt;
      &lt;li&gt;提案手法は SoTA モデルを上回る性能を示した&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;各モダリティの重要度&quot;&gt;各モダリティの重要度&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;unimodal な特徴より bimodal や trimodal な特徴を用いたほうが性能は良かった&lt;/li&gt;
  &lt;li&gt;音声特徴は視覚特徴よりも効果があった&lt;/li&gt;
  &lt;li&gt;MOSI データセットと IEMOCAP データセットにおいてはテキスト特徴が有効であった&lt;/li&gt;
  &lt;li&gt;MOUD データセットにおいてはスペイン語から英語に翻訳した影響により、テキスト特徴より音声特徴のほうが有効であった
    &lt;ul&gt;
      &lt;li&gt;スペイン語の word vector を用いることでテキスト特徴における性能が向上&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;モデルの汎化性能について&quot;&gt;モデルの汎化性能について&lt;/h4&gt;
&lt;p&gt;MOSI データセットで学習を行い、MOUD データセットを用いて評価を行った。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;音声およびテキスト特徴を用いた場合性能が低下した
    &lt;ul&gt;
      &lt;li&gt;英語のモデルでスペイン語を予測していたから&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;視覚特徴を用いた場合、音声・テキスト特徴よりも性能が良かった
    &lt;ul&gt;
      &lt;li&gt;クロスリンガルであっても視覚特徴は一般的な特徴を学習できる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;発話の文脈依存を考慮できることで対象の発話を正確に分析することができている&lt;/li&gt;
  &lt;li&gt;音声特徴では正しく分類できない場合でも、テキスト特徴で正しく分類する例がある
    &lt;ul&gt;
      &lt;li&gt;逆に、テキストではポジティブな文脈でも、怒りっぽい音声特徴から感情を捉えてネガティブであると正しく分類する例もある&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ベースラインについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D15-1303&quot;&gt;Poria, Soujanya, Erik Cambria, and Alexander Gelbukh. “Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis.” Proceedings of the 2015 conference on empirical methods in natural language processing. 2015.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6411794/&quot;&gt;Rozgic, Viktor, et al. “Ensemble of svm trees for multimodal emotion recognition.” Signal &amp;amp; Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific. IEEE, 2012.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P17-1081&quot;&gt;Poria, Soujanya, et al. “Context-dependent sentiment analysis in user-generated videos.” Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shunk031.github.io/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure2.png" /></entry><entry><title type="html">Subcharacter Information in Japanese Embeddings: When Is It Worth It?</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It" rel="alternate" type="text/html" title="Subcharacter Information in Japanese Embeddings: When Is It Worth It?" /><published>2018-09-28T00:00:00+00:00</published><updated>2018-09-28T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字が有する部首のコンポーネントを分解し、サブキャラクターとして扱った際の言語タスクにおける性能を、新たに提案するデータセットも含めて調査を行った。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;漢字は自身に&lt;code class=&quot;highlighter-rouge&quot;&gt;へん&lt;/code&gt;や&lt;code class=&quot;highlighter-rouge&quot;&gt;つくり&lt;/code&gt;といった複数のコンポーネントを有している。
これらサブキャラクターとして埋め込みを学習することで、中国語のいくつかの言語処理タスクで良い精度となることが報告されている。
本研究ではこうしたサブキャラクターの情報を日本語に対して適用した場合の効果を調査している。&lt;/p&gt;

&lt;p&gt;中国語で効果のあるサブキャラクター情報が日本語においては限定的であり、
多くの場合 character-level ngram や character-level のモデルよりも良い性能を示す結果となっている。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;前処理&quot;&gt;前処理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;文字から部首を取り出す
    &lt;ul&gt;
      &lt;li&gt;部首データベースである&lt;a href=&quot;https://github.com/cjkvi/cjkvi-ids&quot;&gt;IDS&lt;/a&gt;を用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;使用モデル&quot;&gt;使用モデル&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Skip-gram + kanji モデル (character-level)
    &lt;ul&gt;
      &lt;li&gt;入力単語に対して skip-gram で得たベクトルと文字に分割して得たベクトルを用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Skip-gram + kanji + bushu モデル (subcharacter-level)
    &lt;ul&gt;
      &lt;li&gt;上記のモデルに対して漢字を部首に分割して得たベクトルを用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;評価データセット-jbats-の提案&quot;&gt;評価データセット jBATS の提案&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/table1.png&quot; alt=&quot;Table 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;40 の言語的関連性が定義されたデータセットである jBATS を提案。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;単語の類似度、jBATS を用いたアナロジータスク、感情分析の 3 つの実験を行っている。&lt;/p&gt;

&lt;p&gt;実験結果では subcharacter-level な入力より character-level の入力を用いたモデルのほうが良い精度となる場合が多かった。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/table7.png&quot; alt=&quot;Table 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;疒&lt;/code&gt;(やまいだれ)や&lt;code class=&quot;highlighter-rouge&quot;&gt;豸&lt;/code&gt;(なじなへん)に対して近いベクトル表現を得た場合の結果。
やまいだれを持つ&lt;code class=&quot;highlighter-rouge&quot;&gt;症&lt;/code&gt;が近い表現になっていることはもちろんのこと、&lt;code class=&quot;highlighter-rouge&quot;&gt;患&lt;/code&gt;や&lt;code class=&quot;highlighter-rouge&quot;&gt;腫&lt;/code&gt;といった病気に関わる単語が近い表現になっている。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;日本語におけるサブキャラクターを用いた言語モデリングについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://aclanthology.info/papers/W17-4122/w17-4122&quot;&gt;Nguyen, Viet, Julian Brooke, and Timothy Baldwin. “Sub-character Neural Language Modelling in Japanese.” Proceedings of the First Workshop on Subword and Character Level Models in NLP. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jBATS について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://vecto.space/projects/jBATS/&quot;&gt;The Japanese Bigger Analogy Test Set (jBATS)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.info/papers/W18-2905/w18-2905&quot;&gt;Karpinska, Marzena, et al. “Subcharacter Information in Japanese Embeddings: When Is It Worth It?.” Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP. 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Semi-supervised Deep Learning by Metric Embedding</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding" rel="alternate" type="text/html" title="Semi-supervised Deep Learning by Metric Embedding" /><published>2018-08-25T00:00:00+00:00</published><updated>2018-08-25T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;少ないラベル付きデータとラベルなしデータを元に距離埋め込み (neighbor embedding) を学習する、半教師あり学習を提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;従来のニューラルネットワークの学習では学習データに対してラベルを推定する枠組みであったが、ラベル付きの学習データが少ない場合に容易に過学習を引き起こす。
本研究ではラベル付きの学習データに対して距離埋め込み (neighbor embedding) を推定する枠組みを導入することで、ラベルなしデータも含めて学習を行い精度を向上させた。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Embedding同士の距離比較による学習 (neighbor embedding)
    &lt;ul&gt;
      &lt;li&gt;学習データ &lt;script type=&quot;math/tex&quot;&gt;x \in \mathcal{X}&lt;/script&gt; バッチサイズ分サンプリング&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; クラスのラベル付きデータ &lt;script type=&quot;math/tex&quot;&gt;z_1, \cdots, z_c \in X_L&lt;/script&gt; を各クラス1サンプルずつサンプリング&lt;/li&gt;
      &lt;li&gt;学習データをembeddingした &lt;script type=&quot;math/tex&quot;&gt;F(x)&lt;/script&gt; と ラベル付きデータをembeddingした &lt;script type=&quot;math/tex&quot;&gt;F(z_1), \cdots, F(z_c)&lt;/script&gt; に対してそれぞれ &lt;code class=&quot;highlighter-rouge&quot;&gt;L2 norm squared&lt;/code&gt; の逆数を計算&lt;/li&gt;
      &lt;li&gt;計算した距離の逆数に対して &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; 値を計算&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x; z_1, \cdots, z_c)_{i} = \frac{e^{-||F(x) - F(z_i)||^2}}{\sum_{j=1}^{c} e^{-||F(x) - F(z_i)||^2}}, i \in \{1 \cdots c \}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; 値と学習データの教師ラベルとの &lt;code class=&quot;highlighter-rouge&quot;&gt;cross entropy&lt;/code&gt; を計算する。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;半教師あり学習への応用
    &lt;ul&gt;
      &lt;li&gt;ラベルなしデータ群 &lt;script type=&quot;math/tex&quot;&gt;X_U&lt;/script&gt; からサンプリングして以下を計算&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, z_1, \cdots, z_c)_U = - \sum_{i=1}^{c} \frac{ e^{-||F(x) - F(z_i)||^2} }{ \sum_{j=1}^{c} e^{-||F(x) - F(z_j)||^2} } \cdot \log{\frac{ e^{-||F(x) - F(z_i)||^2} }{ \sum_{j=1}^{c} e^{-||F(x) - F(z_j)||^2}}}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;MNISTおよびCIFAR10に対して先行研究のモデル(EmbedCNN, SWWAE, Ladder network, Conv-CatGAN / Spike-and-Slab Sparse Coding, View-Invariant k-means, Exampler-CNN, Ladder network, Conv-CatGan, Improved GAN)と提案手法の比較を行っている。&lt;/p&gt;

&lt;p&gt;MNISTに対しては各クラス100枚ずつにのみ教師ラベルを付与し、CIFAR10に対しては各クラス400枚ずつにのみ教師ラベルを付与し実験を行っている。&lt;/p&gt;

&lt;p&gt;学習時にdata agumentationは行わず、テスト時には出力したembeddingに対してk-NNを用いてk={1, 3, 5}の場合の予測結果をaveragingしている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;MNISTデータセットに対して、実験で使用したモデルに2次元のembeddingを出力する全結合層を追加し、可視化を行った結果である。色付きの点は教師ラベルありのサンプルであり、グレーの点は教師ラベルなしのサンプルである。
ラベルありデータは1つのクラスタを形成しており、ラベルなしデータは大部分において各クラスタに属するような形で分布していることがわかる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Semi-supervised-deep-learning-by-metric-embedding/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;EmbedCNNについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-35289-8_34&quot;&gt;Weston, Jason, et al. “Deep learning via semi-supervised embedding.” Neural Networks: Tricks of the Trade. Springer, Berlin, Heidelberg, 2012. 639-655.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SWWAEについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02351&quot;&gt;Zhao, J., et al. “Stacked What-Where Auto-encoders. arXiv 2015.” arXiv preprint arXiv:1506.02351.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ladder networkについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks&quot;&gt;Rasmus, Antti, et al. “Semi-supervised learning with ladder networks.” Advances in Neural Information Processing Systems. 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conv-CatGANについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06390&quot;&gt;Springenberg, Jost Tobias. “Unsupervised and semi-supervised learning with categorical generative adversarial networks.” arXiv preprint arXiv:1511.06390 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spike-and-Slab Sparse Codingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1206.6407&quot;&gt;Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. “Large-scale feature learning with spike-and-slab sparse coding.” arXiv preprint arXiv:1206.6407 (2012).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;View-Invatiant k-meansについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v28/yuhui13.pdf&quot;&gt;Hui, Ka Yu. “Direct modeling of complex invariances for visual object features.” International Conference on Machine Learning. 2013.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Exampler-CNNについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/7312476/&quot;&gt;Dosovitskiy, Alexey, et al. “Discriminative unsupervised feature learning with exemplar convolutional neural networks.” IEEE transactions on pattern analysis and machine intelligence 38.9 (2016): 1734-1747.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ImprovedGanについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03498&quot;&gt;Salimans, Tim, et al. “Improved techniques for training gans.” Advances in Neural Information Processing Systems. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01449&quot;&gt;Hoffer, Elad, and Nir Ailon. “Semi-supervised deep learning by metric embedding.” arXiv preprint arXiv:1611.01449 (2016).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Joint Embedding of Words and Labels for Text Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification" rel="alternate" type="text/html" title="Joint Embedding of Words and Labels for Text Classification" /><published>2018-06-17T00:00:00+00:00</published><updated>2018-06-17T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;テキスト分類の際に教師ラベルのembeddingと単語のembeddingを組み合わせたattentionの枠組みを用いる、Label-Embedding Attentive Model (LEAM) を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;画像認識分野および自然言語処理分野において、label embeddingを用いたさまざまな枠組みが提案されてきた。近年の自然言語処理分野では単語embeddingやattentionを用いることで、テキスト分類等のタスクの精度向上が示されてきた。
本研究では効果的なattentionモデル構築のためにlabel embeddingを学習する、LEAMを提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;単語embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf V}&lt;/script&gt; と label embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf C}&lt;/script&gt; から &lt;code class=&quot;highlighter-rouge&quot;&gt;compatibility&lt;/code&gt; &lt;script type=&quot;math/tex&quot;&gt;{\bf G}&lt;/script&gt; を計算&lt;/li&gt;
  &lt;li&gt;softmaxを用いて &lt;script type=&quot;math/tex&quot;&gt;{\bf G}&lt;/script&gt; をnormalizeしたattention &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; を計算&lt;/li&gt;
  &lt;li&gt;単語embeddingとattentionの重み付け平均を計算したdocument embedding &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; を用いてテキスト分類を行う&lt;/li&gt;
  &lt;li&gt;テスト時にはlabel embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf C}&lt;/script&gt; において、すべてのクラスのembeddingを利用する&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;AGNews、Yelp Review Full、Yelp Review Polarity、DBPedia、Yahoo! Answers Topicの5つのデータセットを用いている。
ベースラインのモデルとしてBag-of-words、Shallow/Large word CNN、LSTM、SA-LSTM、Deep CNN、SWEM、fastText、HAN、Bi-BloSANとテキスト分類の精度を比較している。&lt;/p&gt;

&lt;p&gt;上記に加えて医療テキストデータセットであるMIMIC-IIIを用いた実践的な評価を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;モデルのパラメータ数と学習時間について
    &lt;ul&gt;
      &lt;li&gt;SWEMに次いで少ないパラメータ数と学習時間を実現している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;label embeddingの有効性について
    &lt;ul&gt;
      &lt;li&gt;学習から得られたlabel embeddingとdocument embeddingをt-SNEで可視化すると、クラスに対応するlabel embeddingとdocument embeddingに強い相関が見られた
&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;医療テキストに対する有効性について
    &lt;ul&gt;
      &lt;li&gt;attentionを可視化すると、医療に関連する語がハイライトされていることが示されている。
&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;画像認識分野におけるlabel embedding
    &lt;ul&gt;
      &lt;li&gt;画像分類
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/7293699/&quot;&gt;Akata, Zeynep, et al. “Label-embedding for image classification.” IEEE transactions on pattern analysis and machine intelligence 38.7 (2016): 1425-1438.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;マルチモーダル
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model&quot;&gt;Frome, Andrea, et al. “Devise: A deep visual-semantic embedding model.” Advances in neural information processing systems. 2013.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.2539&quot;&gt;Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. “Unifying visual-semantic embeddings with multimodal neural language models.” arXiv preprint arXiv:1411.2539 (2014).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;画像中のテキスト認識
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.433.2642&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Rodriguez-Serrano, Jose A., Florent Perronnin, and France Meylan. “Label embedding for text recognition.” Proceedings of the British Machine Vision Conference. 2013.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zero-shot learning
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/3650-zero-shot-learning-with-semantic-output-codes&quot;&gt;Palatucci, Mark, et al. “Zero-shot learning with semantic output codes.” Advances in neural information processing systems. 2009.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-2048&quot;&gt;Yogatama, Dani, Daniel Gillick, and Nevena Lazic. “Embedding methods for fine grained entity type classification.” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Vol. 2. 2015.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/C16-1017&quot;&gt;Ma, Yukun, Erik Cambria, and Sa Gao. “Label embedding for zero-shot fine-grained named entity typing.” Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;自然言語処理におけるlabel embedding
    &lt;ul&gt;
      &lt;li&gt;Heterogeneous networkによるlabel embedding
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2783307&quot;&gt;Tang, Jian, Meng Qu, and Qiaozhu Mei. “Pte: Predictive text embedding through large-scale heterogeneous text networks.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;マルチタスク学習
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7005-deconvolutional-paragraph-representation-learning&quot;&gt;Zhang, Yizhe, et al. “Deconvolutional paragraph representation learning.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ベースラインのモデルについて
    &lt;ul&gt;
      &lt;li&gt;Bag-of-words、Shallow/Large word CNN、LSTM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica&quot;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;SA-LSTM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning&quot;&gt;Dai, Andrew M., and Quoc V. Le. “Semi-supervised sequence learning.” Advances in Neural Information Processing Systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deep CNN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/E17-1104&quot;&gt;Conneau, Alexis, et al. “Very deep convolutional networks for text classification.” Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Vol. 1. 2017.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;SWEM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.09843&quot;&gt;Shen, Dinghan, et al. “Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms.” arXiv preprint arXiv:1805.09843 (2018).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HAN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/N16-1174&quot;&gt;Yang, Zichao, et al. “Hierarchical attention networks for document classification.” Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bi-BloSAN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.00857&quot;&gt;Shen, Tao, et al. “Bi-directional block self-attention for fast and memory-efficient sequence modeling.” arXiv preprint arXiv:1804.00857 (2018).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.04174&quot;&gt;Wang, Guoyin, et al. “Joint Embedding of Words and Labels for Text Classification.” arXiv preprint arXiv:1805.04174 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Learn to Pay Attention</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Learn-to-Pay-Attention" rel="alternate" type="text/html" title="Learn to Pay Attention" /><published>2018-06-16T00:00:00+00:00</published><updated>2018-06-16T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Learn-to-Pay-Attention</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Learn-to-Pay-Attention">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;画像認識に対して学習可能なattention機構をCNNに導入し、baseline手法を超える精度を実現&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Convolutional Neural Network (CNN) は画像処理分野で素晴らしい結果を残しているが、こうした問題に対してモデルが推論する過程が不透明であり、結果の考察が難しい。
そこで先行研究ではモデルの解釈性の向上のために、推論する画像のどの部分に注目しているかを可視化する手法が複数提案されている。
しかしながらこれらの手法は学習済みのモデルに対してのみ適用可能という制限がある。&lt;/p&gt;

&lt;p&gt;Attention機構は学習時に入力のどの部分に注視するかを学習することが可能であり、機械翻訳や画像に対する説明文自動生成(キャプショニング)、VQAなどにおいて精度向上に寄与している。&lt;/p&gt;

&lt;p&gt;Attentionを計算する場合にクエリが必要な画像キャプションやVQAに対して、本研究ではattentionを推定するためにglobalな画像表現を利用し、分類問題においてもattention機構を導入することに成功している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Learn-to-Pay-Attention/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;local feature vector&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;global feature vector&lt;/code&gt; を用いたattention機構を実現した。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;畳み込み層から活性化関数を通して得られるlocal feature vector &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}&lt;/script&gt; と最終全結合層の出力であるglobal feature vector &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}&lt;/script&gt; から、&lt;code class=&quot;highlighter-rouge&quot;&gt;compatibility score&lt;/code&gt; &lt;script type=&quot;math/tex&quot;&gt;C\left(\hat{\mathcal{L}}, \mathcal{G} \right)&lt;/script&gt; を計算し、各local feature vectorの重要度 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}&lt;/script&gt; (attention) を算出する&lt;/li&gt;
  &lt;li&gt;重要度 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}&lt;/script&gt; とlocal feature vectorとの重み付き平均 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}_a&lt;/script&gt; を計算する&lt;/li&gt;
  &lt;li&gt;各畳み込み層から得られる複数の &lt;script type=&quot;math/tex&quot;&gt;\mathcal{G}_a&lt;/script&gt; をconcatしたベクトルを用いて分類を行う&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;compatibility socre&lt;/code&gt; を計算する際に用いる &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; は ドット積 を利用した&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;ベースラインとして、先行研究のVGG-GAPおよびVGG-PAN、ResNet164と、VGG/ResNetに対して本研究のattention機構を導入できるようパラメータを調整したネットワークを比較している。
global feature vectorとlocal feature vectorに対してcompatibility scoreを計算する際にドット積を用いた&lt;code class=&quot;highlighter-rouge&quot;&gt;dp&lt;/code&gt;と、パラメータを用いた&lt;code class=&quot;highlighter-rouge&quot;&gt;pc&lt;/code&gt;を比較している。&lt;/p&gt;

&lt;p&gt;評価に用いるデータセットはCIFAR10/100、CUB-200-2011、SVHN等を利用している。また導入したattention機構がadversarialなサンプルに対してもロバストであることを示す実験も行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Learn-to-Pay-Attention/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;提案手法 (proposed) と既存手法 (existing) それぞれのattention mapを可視化した結果である。提案手法がよりdiscriminativeな形で物体を認識していることが示されている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Learn-to-Pay-Attention/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CUB-200データセットで学習した提案手法の結果である。10層目は目の特徴を捉えており、13層目は体全体を捉えていることが示されている。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;モデルの解釈性向上のための可視化手法
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6034&quot;&gt;Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013).&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_iccv_2015/papers/Cao_Look_and_Think_ICCV_2015_paper.pdf&quot;&gt;Cao, Chunshui, et al. “Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks.” Proceedings of the IEEE International Conference on Computer Vision. 2015.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Global average pooling (GAP)
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&quot;&gt;Zhou, Bolei, et al. “Learning deep features for discriminative Computer.” localization Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on. IEEE, 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention機構が用いられている先行研究
    &lt;ul&gt;
      &lt;li&gt;属性予測 (Progressive Attention Networks (PAN))
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://adsabs.harvard.edu/abs/2016arXiv160602393H&quot;&gt;Hongsuck Seo, Paul, et al. “Progressive Attention Networks for Visual Attribute Prediction.” arXiv preprint arXiv:1606.02393 (2016).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;機械翻訳
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate.” arXiv preprint arXiv:1409.0473 (2014).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;画像に対する説明文自動生成
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf&quot;&gt;Xu, Kelvin, et al. “Show, attend and tell: Neural image caption generation with visual attention.” International Conference on Machine Learning. 2015.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf&quot;&gt;You, Quanzeng, et al. “Image captioning with semantic attention.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14888/14305&quot;&gt;Mun, Jonghwan, Minsu Cho, and Bohyung Han. “Text-Guided Attention Model for Image Captioning.” AAAI. 2017.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Visual question answering (VQA)
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-46478-7_28&quot;&gt;Xu, Huijuan, and Kate Saenko. “Ask, attend and answer: Exploring question-guided spatial attention for visual question answering.” European Conference on Computer Vision. Springer, Cham, 2016.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S01-03.pdf&quot;&gt;Yang, Zichao, et al. “Stacked attention networks for image question answering.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.02391&quot;&gt;Jetley, Saumya, et al. “Learn to pay attention.” arXiv preprint arXiv:1804.02391 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)</title><link href="https://shunk031.github.io/paper-survey/summary/others/Skin-Lesion-Analysis-Toward-Melanoma-Detection-A-Challenge-at-the-2017-International-Symposium-on-Biomedical-Imaging-ISBI-Hosted-by-the-International-Skin-Imaging-Collaboration-ISIC" rel="alternate" type="text/html" title="Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC) " /><published>2018-06-03T00:00:00+00:00</published><updated>2018-06-03T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Skin-Lesion-Analysis-Toward-Melanoma-Detection-A-Challenge-at-the-2017-International-Symposium-on-Biomedical-Imaging-ISBI-Hosted-by-the-International-Skin-Imaging-Collaboration-ISIC</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Skin-Lesion-Analysis-Toward-Melanoma-Detection-A-Challenge-at-the-2017-International-Symposium-on-Biomedical-Imaging-ISBI-Hosted-by-the-International-Skin-Imaging-Collaboration-ISIC">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ISIC2017で行われたメラノーマ画像の分析チャレンジの内容をまとめたもの&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごい&quot;&gt;2. 先行研究と比べてどこがすごい？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ISIC2017では3種類のタスクが公開された
    &lt;ul&gt;
      &lt;li&gt;Lesion Segmentation Task&lt;/li&gt;
      &lt;li&gt;Dermoscopic Feature Classification&lt;/li&gt;
      &lt;li&gt;Disease Classification Task
        &lt;ul&gt;
          &lt;li&gt;3カテゴリ (melanoma, nevus, seborrheic keratosis) をそれぞれ分類するタスク&lt;/li&gt;
          &lt;li&gt;melanoma (train: 374, val: 30, test: 117)&lt;/li&gt;
          &lt;li&gt;nevus (train: 1372, val: 78, test: 393)&lt;/li&gt;
          &lt;li&gt;seborrheic keratosis (train: 254, val: 42, test: 90)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;データセットは以下から入手できる
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://challenge2017.isic-archive.com/&quot;&gt;http://challenge2017.isic-archive.com/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこ&quot;&gt;3. 技術や手法のキモはどこ？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;平均スコアがベストだったモデル
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.03108&quot;&gt;Matsunaga, Kazuhisa, et al. “Image classification of melanoma, nevus and seborrheic keratosis by deep neural network ensemble.” arXiv preprint arXiv:1703.03108 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Melanoma予測精度がベストだったモデル
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01976&quot;&gt;Díaz, Iván González. “Incorporating the knowledge of dermatologists to convolutional neural networks for the diagnosis of skin lesions.” arXiv preprint arXiv:1703.01976 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Seborrheic keratosis予測精度がベストだったモデル
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04819&quot;&gt;Menegola, Afonso, et al. “RECOD titans at ISIC challenge 2017.” arXiv preprint arXiv:1703.04819 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;予測モデルのトレンド&quot;&gt;予測モデルのトレンド&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;深層学習モデルを複数アンサンブルしている
    &lt;ul&gt;
      &lt;li&gt;学習データに追加で外部データを用いている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Seborrheic keratosiの分類はMelanomaの分類に比べて容易な傾向だった
    &lt;ul&gt;
      &lt;li&gt;病気の性質・データの偏りから生じたものでは&lt;/li&gt;
      &lt;li&gt;一番ベストなモデルを作ったチームは追加でヒューリスティックなラベリングを追加で行っている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;平均スコアがベストだったモデルは、各カテゴリ分類ではベストなモデルではなかった&lt;/li&gt;
  &lt;li&gt;一番複雑なモデルはパフォーマンスを下げており、シンプルなモデルは全体のパフォーマンスを上げている&lt;/li&gt;
  &lt;li&gt;予測の閾値は重要そう。確率的なスコア標準化(Probablistic score normalization)はsensitivityおよびspecificityのスコアをあげるために効果がありそう [&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/8030303/&quot;&gt;Codella+&lt;/a&gt;, &lt;a href=&quot;https://www.jaad.org/article/S0190-9622(17)32202-8/fulltext&quot;&gt;Marchetti+&lt;/a&gt;]。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;評価尺度
    &lt;ul&gt;
      &lt;li&gt;AUC, specificity (melanoma classification)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はある&quot;&gt;5. 議論はある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Classficationタスクについて
    &lt;ul&gt;
      &lt;li&gt;モデルのアンサンブルと追加の外部データ使用が高いパフォーマンスを出すカギになる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文は&quot;&gt;6. 次に読むべき論文は？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.03108&quot;&gt;Matsunaga, Kazuhisa, et al. “Image classification of melanoma, nevus and seborrheic keratosis by deep neural network ensemble.” arXiv preprint arXiv:1703.03108 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01976&quot;&gt;Díaz, Iván González. “Incorporating the knowledge of dermatologists to convolutional neural networks for the diagnosis of skin lesions.” arXiv preprint arXiv:1703.01976 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04819&quot;&gt;Menegola, Afonso, et al. “RECOD titans at ISIC challenge 2017.” arXiv preprint arXiv:1703.04819 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/8030303/&quot;&gt;Codella, Noel CF, et al. “Deep learning ensembles for melanoma recognition in dermoscopy images.” IBM Journal of Research and Development 61.4 (2017): 5-1.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jaad.org/article/S0190-9622(17)32202-8/fulltext&quot;&gt;Marchetti, Michael A., et al. “Results of the 2016 International Skin Imaging Collaboration International Symposium on Biomedical Imaging challenge: Comparison of the accuracy of computer algorithms to dermatologists for the diagnosis of melanoma from dermoscopic images.” Journal of the American Academy of Dermatology 78.2 (2018): 270-277.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.05006&quot;&gt;Codella, Noel CF, et al. “Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic).” arXiv preprint arXiv:1710.05006 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">RECOD Titans at ISIC Challenge 2017</title><link href="https://shunk031.github.io/paper-survey/summary/others/RECOD-Titans-at-ISIC-Challenge-2017" rel="alternate" type="text/html" title="RECOD Titans at ISIC Challenge 2017" /><published>2018-06-02T00:00:00+00:00</published><updated>2018-06-02T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/RECOD-Titans-at-ISIC-Challenge-2017</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/RECOD-Titans-at-ISIC-Challenge-2017">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ISIC2017メラノーマ画像分析でSeborrheic keratosis分類タスクでベストな精度を出したモデルの解説&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごい&quot;&gt;2. 先行研究と比べてどこがすごい？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;以前までの問題点
    &lt;ul&gt;
      &lt;li&gt;学習データ量が足りない&lt;/li&gt;
      &lt;li&gt;モデルの深さ&lt;/li&gt;
      &lt;li&gt;計算コスト&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;これらに対して、外部の追加データの使用・ResNet等の深いネットワークの使用・クラウドベースの計算機を使用している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこ&quot;&gt;3. 技術や手法のキモはどこ？&lt;/h2&gt;

&lt;h3 id=&quot;外部データを用いて学習データを増やす&quot;&gt;外部データを用いて学習データを増やす&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;# of melanoma&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;# of seborrheic keratoses&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;# of benign nevi&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ISIC 2017 Challenge&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;374&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;254&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISIC Archive&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;(13000 dermoscopic images)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Interactive Atlas of Dermoscopy&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;270&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;49&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dermofit Image Library&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;257&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IRMA Skin Lesion Dataset&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;187&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PH2 Datset&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;すべてのデータについて、アノテーションされているラベルを考慮してマージする
    &lt;ul&gt;
      &lt;li&gt;以下のデータについては除いている
        &lt;ul&gt;
          &lt;li&gt;ISIC Archiveの診断結果がない画像&lt;/li&gt;
          &lt;li&gt;Atlasの &lt;code class=&quot;highlighter-rouge&quot;&gt;miscellaneous&lt;/code&gt; クラス&lt;/li&gt;
          &lt;li&gt;IRMAの &lt;code class=&quot;highlighter-rouge&quot;&gt;benign&lt;/code&gt; クラス&lt;/li&gt;
          &lt;li&gt;PH2の &lt;code class=&quot;highlighter-rouge&quot;&gt;atypical nevi&lt;/code&gt; クラス&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ISIC Archiveにおいて、benignのデータの多くが15歳の患者だった
    &lt;ul&gt;
      &lt;li&gt;これらを取り除いたらスコアが微増&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;各データセット間で重複してるデータが存在している
    &lt;ul&gt;
      &lt;li&gt;trainとvalidationに分けるときに注意しないといけない&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;deploy&lt;/code&gt; データ群と &lt;code class=&quot;highlighter-rouge&quot;&gt;semi&lt;/code&gt; データ群を学習用に作成
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;deploy&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;6つのデータセットからなる9640枚の学習画像
            &lt;ul&gt;
              &lt;li&gt;keratosis分類ではこのデータ群で学習したほうがAUCスコアが良かった&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;semi&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;3つのデータセット (ISIC2017, ISIC Archive, Interactive Atlas) からなる7544枚の学習画像
            &lt;ul&gt;
              &lt;li&gt;melanoma分類ではこのデータ群で学習したほうがAUCスコアが良かった&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;使用モデル&quot;&gt;使用モデル&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ImageNetモデルのfine-tuning
    &lt;ul&gt;
      &lt;li&gt;ResNet-101&lt;/li&gt;
      &lt;li&gt;Inception-v4&lt;/li&gt;
      &lt;li&gt;~Inception-ResNet~
        &lt;ul&gt;
          &lt;li&gt;計算コストが大きいがスコアは微増しただけだったため使用を見送った&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;各クラス独立に学習を行っていたが、3クラス分類に変更した&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ベースラインのVGG16とResNet101やInception-v4を比べる&lt;/li&gt;
  &lt;li&gt;標準的なサイズの画像 (224x224) とより大きな高解像度の画像を入力したときの精度の比較&lt;/li&gt;
  &lt;li&gt;class-weightやsample-weightの考慮&lt;/li&gt;
  &lt;li&gt;curriculum-learningの有無
    &lt;ul&gt;
      &lt;li&gt;最初は簡単なデータで学習させ、学習が進んだら難しいデータで学習させる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最終conv層をそのままニューラルネットにするかSVMにするか&lt;/li&gt;
  &lt;li&gt;年齢や性別といった患者データを使うかどうか&lt;/li&gt;
  &lt;li&gt;用いるoptimizerの比較&lt;/li&gt;
  &lt;li&gt;異なるper-sample normalizationnの実施&lt;/li&gt;
  &lt;li&gt;アンサンブルやスタッキングの有無&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はある&quot;&gt;5. 議論はある？&lt;/h2&gt;
&lt;h3 id=&quot;効果がなかったこと&quot;&gt;効果がなかったこと&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;画像の解像度について
    &lt;ul&gt;
      &lt;li&gt;高解像度は効果なし&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;class-weightやsample-weightについて
    &lt;ul&gt;
      &lt;li&gt;no weighting was the best weighting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;validationデータにおけるearly stoppingについて
    &lt;ul&gt;
      &lt;li&gt;特にスコアに対するインパクトはなかった&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;患者データの利用
    &lt;ul&gt;
      &lt;li&gt;効果があるときと無い時がある&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;curriculum learningの使用
    &lt;ul&gt;
      &lt;li&gt;シンプルなトレーニングのほうがよかった&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;セグメンテーションの情報の利用
    &lt;ul&gt;
      &lt;li&gt;今回は使えなかったけど、使うと効果が出るのではと考えられている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;効果があったこと&quot;&gt;効果があったこと&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;deepなモデルと規模の大きいデータセットを用いると効果が大きい&lt;/li&gt;
  &lt;li&gt;data augmentationは必須
    &lt;ul&gt;
      &lt;li&gt;テスト時にもdata augmentationするとよい (test time augmentation 的な)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mean subtractionは有効
    &lt;ul&gt;
      &lt;li&gt;標準偏差で割るnormalizationはスコアを悪化させた&lt;/li&gt;
      &lt;li&gt;Inception-v4では確認できた。ResNetについては不明&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Stackingは有効&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04819&quot;&gt;Menegola, Afonso, et al. “RECOD titans at ISIC challenge 2017.” arXiv preprint arXiv:1703.04819 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations</title><link href="https://shunk031.github.io/paper-survey/summary/nip/Contextual-Augmentation-Data-Augmentation-by-Words-with-Paradigmatic-Relations" rel="alternate" type="text/html" title="Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations" /><published>2018-05-27T00:00:00+00:00</published><updated>2018-05-27T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nip/Contextual-Augmentation-Data-Augmentation-by-Words-with-Paradigmatic-Relations</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nip/Contextual-Augmentation-Data-Augmentation-by-Words-with-Paradigmatic-Relations">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;文脈および感情値などの条件を考慮した単語置き換えでdata augmentationを実現するContextual Augmentationを提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;ニューラルネットベースのモデルは高い精度を示すが過学習しやすい。
Data augmentationは汎化性能を向上させるテクニックであり、画像認識分野では回転やフリップなどを用いてデータのかさ増しを行っている。&lt;/p&gt;

&lt;p&gt;しかしながら自然言語処理に対するdata augmentationの適用法は限られている。
一般的にはWordNetなどを用いた単語の置き換えやルールベースの手法が用いられるが、ドメインに特化している場合も多く、一般性が失われたものとなっている。&lt;/p&gt;

&lt;p&gt;本研究ではbi-directional言語モデル(LM)を用いて、文脈を考慮した単語置き換えでdata augmentationを実現するContextual Augmentationを提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Contextual-Augmentation-Data-Augmentation-by-Words-with-Paradigmatic-Relations/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文脈を考慮した単語予測
    &lt;ul&gt;
      &lt;li&gt;bi-directional LSTMを用いて、文脈に基づいて文中の位置 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; におけるword probabilityを計算&lt;/li&gt;
      &lt;li&gt;オンラインでdata augmentationするための単語をサンプリングする&lt;/li&gt;
      &lt;li&gt;data augmentationを制御するパラメータとしてtemperature &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; を導入する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;感情値などの条件を考慮
    &lt;ul&gt;
      &lt;li&gt;LMにおいて &lt;code class=&quot;highlighter-rouge&quot;&gt;good&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;bad&lt;/code&gt; は近い表現になりやすく、反義語がdata augmentationに使われてしまう場合がある
        &lt;ul&gt;
          &lt;li&gt;label-conditional LMを用いてpositive/negativeなどのラベルを考慮し、反義語を制御する&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;SST(SST2, SST5)、Subj、MPQA、RT、TRECの各データセットを用いている。モデルはLSTM、CNNをそれぞれ利用し、提案手法であるContextual Augmentationの効果を確認している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Contextual-Augmentation-Data-Augmentation-by-Words-with-Paradigmatic-Relations/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LMによって予測された単語は必ずしもシノニムになっていない&lt;/li&gt;
  &lt;li&gt;label-conditional LMを用いると、ラベルの性質に沿った単語の予測がなされている&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;シノニムを利用した単語置換によるdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica&quot;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D15-1306&quot;&gt;Wang, William Yang, and Diyi Yang. “That’s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using# petpeeve Tweets.” Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;文法推論 (grammar induction)を用いたdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03622&quot;&gt;Jia, Robin, and Percy Liang. “Data recombination for neural semantic parsing.” arXiv preprint arXiv:1606.03622 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;タスク固有のヒューリスティックを利用したdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1609091&quot;&gt;Fürstenau, Hagen, and Mirella Lapata. “Semi-supervised semantic role labeling.” Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/W17-3529&quot;&gt;Kafle, Kushal, Mohammed Yousefhussien, and Christopher Kanan. “Data augmentation for visual question answering.” Proceedings of the 10th International Conference on Natural Language Generation. 2017.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/K17-2010&quot;&gt;Silfverberg, Miikka, et al. “Data Augmentation for Morphological Reinflection.” Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection (2017): 90-99.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Autoencoderを用いたdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/K17-2002&quot;&gt;Bergmanis, Toms, et al. “Training Data Augmentation for Low-Resource Morphological Inflection.” Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection (2017): 31-39.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14299/14261&quot;&gt;Xu, Weidi, et al. “Variational Autoencoder for Semi-Supervised Text Classification.” AAAI. 2017.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v70/hu17e.html&quot;&gt;Hu, Zhiting, et al. “Toward controlled generation of text.” International Conference on Machine Learning. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encoder-Decoderを用いたdata augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.07947&quot;&gt;Kim, Yoon, and Alexander M. Rush. “Sequence-level knowledge distillation.” arXiv preprint arXiv:1606.07947 (2016).&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;&gt;Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Improving neural machine translation models with monolingual data.” arXiv preprint arXiv:1511.06709 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;本研究に一番近い立ち位置の先行研究
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2002793&quot;&gt;Kolomiyets, Oleksandr, Steven Bethard, and Marie-Francine Moens. “Model-portability experiments for textual temporal analysis.” Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Association for Computational Linguistics, 2011.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.00440&quot;&gt;Fadaee, Marzieh, Arianna Bisazza, and Christof Monz. “Data augmentation for low-resource neural machine translation.” arXiv preprint arXiv:1705.00440 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06201&quot;&gt;Kobayashi, Sosuke. “Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations.” arXiv preprint arXiv:1805.06201 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>