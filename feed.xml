<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2019-03-20T05:30:17+00:00</updated><id>https://shunk031.github.io/paper-survey/feed.xml</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Beyond News Contents: The Role of Social Context for Fake News Detection</title><link href="https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection" rel="alternate" type="text/html" title="Beyond News Contents: The Role of Social Context for Fake News Detection" /><published>2019-03-20T00:00:00+00:00</published><updated>2019-03-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;出版社・ニュース記事・ユーザの 3 つの関係をモデリングすることでフェイクニュース検出の精度向上を目指す TriFN を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;フェイクニュース検出は主にニュース記事にフォーカスしたものと、ユーザのソーシャルな行動にフォーカスしているものが多い。
これらは偽の情報を判断する言語的な観点やフェイク画像に対する視覚的観点に基づいたモデルの構築を行っている。
またユーザに対して行動ログベースやユーザ間のネットワークベースのモデリングを行っている事例も多い。&lt;/p&gt;

&lt;p&gt;本研究では、先行研究では考慮されてこなかった、&lt;code class=&quot;highlighter-rouge&quot;&gt;出版社&lt;/code&gt;・&lt;code class=&quot;highlighter-rouge&quot;&gt;ニュース記事&lt;/code&gt;・&lt;code class=&quot;highlighter-rouge&quot;&gt;ユーザ行動&lt;/code&gt;の 3 つの関係 &lt;code class=&quot;highlighter-rouge&quot;&gt;tri-relationship&lt;/code&gt; を考慮することでフェイクニュース検出の精度向上を目指す TriFN を提案している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection/figure1.png&quot; width=&quot;400px&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tri-relationship-を考慮したフェイクニュース検出モデル-trifn&quot;&gt;Tri-relationship を考慮したフェイクニュース検出モデル TriFN&lt;/h3&gt;
&lt;p&gt;TriFN は主に non-negative matrix factorization (NMF) をベースとした 5 つのコンポーネントから構成される。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;News Contents Embedding
    &lt;ul&gt;
      &lt;li&gt;Bag-of-words 表現なニュース記事から NMF でニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;User Embedding
    &lt;ul&gt;
      &lt;li&gt;ユーザのソーシャルな関係から NMF でユーザの潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;User-News Interaction Embedding
    &lt;ul&gt;
      &lt;li&gt;ユーザの信頼度によって得られるニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Publisher-News Relation Embedding
    &lt;ul&gt;
      &lt;li&gt;出版社の党派的なバイアスを考慮したニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Semi-supervised Linear Classifier
    &lt;ul&gt;
      &lt;li&gt;ニュース記事に対してフェイクニュース予測を行うための学習器を学習する&lt;/li&gt;
      &lt;li&gt;半教師あり学習の枠組みを同時に使用し、ラベルが付与されていないデータからも学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;FakeNewsNet データセットを用いて、&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;か&lt;code class=&quot;highlighter-rouge&quot;&gt;fake&lt;/code&gt;かの 2 値分類問題としてフェイクニュース検出に対する検出精度の比較を行っている。
比較対象として先行研究で提案されている複数の特徴抽出手法 (RST, LIWC, Castillo とこれらを組み合わせた RST + Castillo, LIWC + Castillo) とベースラインのモデル (LogReg, NBayes, DTree, RForest, XGBoost, AdaBoost, Gradient Boosting) で比較している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;早期のフェイクニュース検出におけるパフォーマンス
    &lt;ul&gt;
      &lt;li&gt;記事公開から 12 時間後 〜 96 時間後の各時間におけるフェイクニュース検出の精度比較を行った結果、提案手法の TriFN が一番良かった。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;モデルのパラメータ分析
    &lt;ul&gt;
      &lt;li&gt;ユーザのソーシャルな関係の寄与をコントロールするパラメータを大きくすると精度が向上した。
        &lt;ul&gt;
          &lt;li&gt;ユーザとニュース間の関係よりユーザのソーシャルな関係のほうが予測に寄与する。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;フェイクニュース検出における data augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8594871/&quot;&gt;Shu, Kai, et al. “Deep headline generation for clickbait detection.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.07709&quot;&gt;Shu, Kai, Suhang Wang, and Huan Liu. “Beyond News Contents: The Role of Social Context for Fake News Detection.” Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. ACM, 2019.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection" rel="alternate" type="text/html" title="How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection" /><published>2019-03-02T00:00:00+00:00</published><updated>2019-03-02T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;精度を保ったまま最小限の語彙を選択する variational vocabulary dropout (VDD) を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語処理 (NLP) のタスクに対して deep learning モデルを用いる場合、入力にあらかじめ定義された語彙を元に単語をベクトル化して入力する必要がある。
こうした語彙数は非常に大規模となる場合が多くパラメータ数も増大してしまうため、限られた計算リソース下での実行することは難しい。
したがってタスクを解く精度を保ったまま必要な語彙を選択する必要がある。&lt;/p&gt;

&lt;p&gt;本研究ではこの語彙選択問題に対して、タスクを考慮した語彙選択手法である variational vocabulary dropoput (VDD) を提案している。
また適切な語彙選択が行われているかを確認するため、AUC ベースの評価指標を導入し、提案手法の効果を確認している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;
&lt;h3 id=&quot;語彙選択に対する問題設定&quot;&gt;語彙選択に対する問題設定&lt;/h3&gt;
&lt;p&gt;元の embedding &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;と語彙選択を行った後の embedding &lt;script type=&quot;math/tex&quot;&gt;\hat{W}&lt;/script&gt; を使用した場合に、予測精度の差が閾値以上である語彙数を最小にする。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathop{\rm argmin}\limits_{\hat{W},\hat{\theta}} \textrm{#Row}(\hat{W})~~s.t. \textrm{Acc}(f_{\hat{\theta}}(x; \hat{W}), y) - \textrm{Acc}(f_{\theta}(x;W),y)\le \epsilon&lt;/script&gt;

&lt;h3 id=&quot;語彙選択に対する評価指標&quot;&gt;語彙選択に対する評価指標&lt;/h3&gt;
&lt;p&gt;AUC ベースの評価指標 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-X%&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;X ％のパフォーマンス低下が許される場合に必要な最小の語彙数を計算する
    &lt;ul&gt;
      &lt;li&gt;本研究では &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-3%&lt;/code&gt; および &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-5%&lt;/code&gt; を用いている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-vocabulary-dropout-vdd&quot;&gt;Variational Vocabulary Dropout (VDD)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bernouli Dropout
    &lt;ul&gt;
      &lt;li&gt;Onehot ベクトルに対してベルヌーイノイズ&lt;script type=&quot;math/tex&quot;&gt;\textbf{b}&lt;/script&gt;を適用する
        &lt;ul&gt;
          &lt;li&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{b}) = (\textbf{b} \odot \textrm{OneHot}(x)) \cdot W&lt;/script&gt;
            &lt;ul&gt;
              &lt;li&gt;しかしながらベルヌーイ分布を reparameterize するのは難しい&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gaussian Relaxation
    &lt;ul&gt;
      &lt;li&gt;ベルヌーイノイズの代わりにガウシアンノイズ &lt;script type=&quot;math/tex&quot;&gt;z_i \sim \mathcal{N}(1, \alpha_{i} = \frac{p_i}{1 - p_i})&lt;/script&gt; を適用する
        &lt;ul&gt;
          &lt;li&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{z}) = (\textbf{z} \odot \textrm{OneHot}(x) \cdot W)&lt;/script&gt;
            &lt;ul&gt;
              &lt;li&gt;Reparameterization trick に従うと、&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; は多変量ガウス分布 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; を用いて以下のようになる
                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{z}) = \textrm{OneHot}(x) \cdot B&lt;/script&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dropout 率 &lt;script type=&quot;math/tex&quot;&gt;\alpha_i&lt;/script&gt; は 語彙の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語が必要かどうかを示す指標となる。
ここで&lt;script type=&quot;math/tex&quot;&gt;\alpha_i&lt;/script&gt;より数値が大きい場合は&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の語彙をドロップしてもパフォーマンス低下の要因にならないことを意味する。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;文書分類 (document classification: DC)、自然言語理解 (natural language understanding: NLU)、自然言語推論 (natural language inference: NLI) のデータセットを用いてそれぞれのタスクにおける VVD の効果を確認している。
提案手法の VVD に対して、ベースラインとして頻度に基づいた語彙形成、TF-IDF に基づいた語彙形成、そして group lasso に基づいた語彙形成を比較している。
各タスクではそれぞれ DC では CNN ベースのモデル、NLU では attetion を導入した bi-directional LSTM モデル、NLI では ESIM モデルを用いている。&lt;/p&gt;

&lt;p&gt;すべてのタスクに対して提案手法の語彙選択手法である VVD が outperform する結果となっている。
また語彙選択を考慮した評価指標として Vocab@-X% を用いた場合では、特に提案手法のスコアが良くなっていることが示されており、効果的な語彙選択が行われていることがわかる。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;学習時の速度について&quot;&gt;学習時の速度について&lt;/h3&gt;
&lt;p&gt;VVD は確率的な振る舞いを扱うため、テキスト分類に対して学習を行う場合通常の cross entropy よりも時間がかかった。
フルサイズの語彙に対して VVD を適用すると計算時間がかかるため、前段階で精度低下が怒らない程度に語彙を制限したほうがよい。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Deep learning モデルに対するベイジアンベースのモデル圧縮について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning&quot;&gt;Louizos, Christos, Karen Ullrich, and Max Welling. “Bayesian compression for deep learning.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.10339&quot;&gt;Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, William Wang. How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection. arXiv:1902.10339, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Attentional Encoder Network for Targeted Sentiment Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification" rel="alternate" type="text/html" title="Attentional Encoder Network for Targeted Sentiment Classification" /><published>2019-03-01T00:00:00+00:00</published><updated>2019-03-01T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ターゲットに対する感情分析で使われてきた LSTM に変わる self-attention ベースの attentional encoder network (AEN) を提案した。
また付与されている感情ラベルに対する非信頼性に対して label smoothing を行う損失関数を導入した。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語処理 (NLP) のタスクに対して RNN ベースの LSTM モデルが広く用いられてきたが、
コンテキスト内のターゲットに対する感情を分析する fine-grained なターゲットに対する感情分類 (targeted sentiment classification) に対しては依然として改善の余地が残されている。
先行研究の問題点としてはテキストのモデリングにおいて RNN ベースのモデルに比重がおかれており、また学習速度も遅いことが挙げられる。
感情分類タスクにおいて attention は重要な役割を果たすが、モデルの後段部分でのみしか使用されてこなかった。
また感情分類タスクにおいて付与されている感情ラベルは信頼性が低い場合がある。特に &lt;em&gt;neutral&lt;/em&gt; な感情ラベルはとてもファジーでモデルの学習を難しくしている。&lt;/p&gt;

&lt;p&gt;本研究では fine-grained なターゲットに対する感情分析にタスクに対してターゲットとコンテキストの交互作用を self-attention で捉え、ファジーな感情ラベルに対して効果的な label smoothing を行う損失関数を導入した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;Attentional Encoder Network &lt;code class=&quot;highlighter-rouge&quot;&gt;(AEN)&lt;/code&gt; を提案&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;attentional-encoder-layer&quot;&gt;Attentional Encoder Layer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LSTM に代わる並列計算可能なレイヤー
    &lt;ul&gt;
      &lt;li&gt;コンテキストとターゲットの embedding に対して隠れ状態の表現を計算&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Multi-Head Attention&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;Point-wise Convolutional Transformation&lt;/code&gt; から構成される&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multi-head-attention-mha&quot;&gt;Multi-Head Attention (MHA)&lt;/h4&gt;
&lt;p&gt;入力されるコンテキストとターゲットに対して異なる MHA を適用する。
MHA は RNN と比較して短いネットワークで離れた単語のコンテキストを捉えることが可能。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Intra-MHA
    &lt;ul&gt;
      &lt;li&gt;コンテキスト embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^c&lt;/script&gt; を MHA に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{e}^{\textrm{intra}} = MHA(\textbf{e}^c, \textbf{e}^c)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Inter-MHA
    &lt;ul&gt;
      &lt;li&gt;コンテキスト embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^c&lt;/script&gt; と ターゲット embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^t&lt;/script&gt; を MHA に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{e}^{\textrm{inter}} = MHA(\textbf{e}^c, \textbf{e}^t)&lt;/script&gt;

&lt;h4 id=&quot;point-wise-convolution-transformation-pct&quot;&gt;Point-wise Convolution Transformation (PCT)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;2 つの 1D convolution と活性化関数から構成される&lt;/li&gt;
  &lt;li&gt;MHA で得られた表現に対して位置ごとにコンテキストを学習することが可能&lt;/li&gt;
  &lt;li&gt;Intra-MHA と inter-MHA それぞれに対して PCT を適用し、コンテキスト表現を得る&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;label-smoothing-regularization-lsr&quot;&gt;Label Smoothing Regularization (LSR)&lt;/h3&gt;

&lt;p&gt;感情分類タスクに対して &lt;em&gt;neutral&lt;/em&gt; というラベルはとてもファジーであるため、LSR を導入し、過学習を抑制している。
LSR は感情ラベルの分布とモデル予測の分布との KL 距離に等しいため、LSR 項は以下のように表せる。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{lsr} = - D_{\textrm{KL}} (u(k)||p_{\theta})&lt;/script&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;SemEval 2014 Task4 (レストランのレビューデータセット、ラップトップのレビューデータセット)、ACL 14 Twitter データセットの 3 つで比較を行っている。
これらのデータセットには 3 つの感情ラベル (positive/neutral/negative) が付与されている。&lt;/p&gt;

&lt;p&gt;ベースラインのモデルとして SVM、Rec-NN、TD-LSTM、ATAE-LSTM、IAN、MemNet、RAM を用いて提案手法の AEN との比較を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h3 id=&quot;label-smoothing-regularization-の効果&quot;&gt;Label smoothing regularization の効果&lt;/h3&gt;
&lt;p&gt;Label smoothing regularization を導入することで、導入していないモデルよりも良い精度であった。
信頼性の低いラベルに対して効果的に過学習を抑制できたと考えられる。&lt;/p&gt;

&lt;h3 id=&quot;recurrence-vsattention&quot;&gt;Recurrence vs.Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification/table3.png&quot; width=&quot;400px&quot; alt=&quot;Table 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AEN に対して Attention encoder layer を LSTM に代えたネットワークと比較すると同等程度の精度を記録している。
よって LSTM に比べて半数程度の少ないパラメータで同程度の予測精度を達成できるメリットがある。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-Head Atteition について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Label Smoothing Regularization について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html&quot;&gt;Szegedy, Christian, et al. “Rethinking the inception architecture for computer vision.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.09314&quot;&gt;Song, Youwei, et al. “Attentional Encoder Network for Targeted Sentiment Classification.” arXiv preprint arXiv:1902.09314 (2019).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Tell Me Where to Look: Guided Attention Inference Network</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network" rel="alternate" type="text/html" title="Tell Me Where to Look: Guided Attention Inference Network" /><published>2019-02-24T00:00:00+00:00</published><updated>2019-02-24T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;Attention map を用いた弱教師ありセマンティックセグメンテーションを高精度に行う guided attention inference networks (GAIN) を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;クラスラベルをもとに学習し予測することで得られる attention map は局所的な特徴やセグメンテーションの情報を保持している場合が多い。
こうした attention map を利用しセマンティックセグメンテーションといったタスクを解く弱教師あり学習 (weakly supervised learning) の枠組みは先行研究で複数手法が提案されてきた。
しかしながらクラスラベルから classification loss で学習した attention map は分類に必要最低限の領域にのみフォーカスしており、対象となる物体全体をカバーすることは難しい。&lt;/p&gt;

&lt;p&gt;また学習データセットのバイアスにより、未知のデータに対して正しくセグメンテーションを行うのは難しい。
具体的には&lt;code class=&quot;highlighter-rouge&quot;&gt;ボート&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;海&lt;/code&gt;は同時に撮影される場合が多く、高い相関がある。
したがって予測時に海面が移っていない&lt;code class=&quot;highlighter-rouge&quot;&gt;ボート&lt;/code&gt;のサンプルを正しくセグメンテーションするのは難しい場合が多い。&lt;/p&gt;

&lt;p&gt;本研究では end-to-end で attention map を学習し、classification loss に加えて attention の妥当性を考慮する loss を同時に学習する guided attention inference networks (GAIN) を提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h3 id=&quot;gain&quot;&gt;GAIN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;共通する 2 つのストリーム (&lt;script type=&quot;math/tex&quot;&gt;S_{cl}&lt;/script&gt; と &lt;script type=&quot;math/tex&quot;&gt;S_{am}&lt;/script&gt;)
    &lt;ul&gt;
      &lt;li&gt;2 つのストリームにおける conv 層と一部の fc 層のパラメータは共通&lt;/li&gt;
      &lt;li&gt;Classification stream &lt;script type=&quot;math/tex&quot;&gt;S_{cl}&lt;/script&gt;
        &lt;ul&gt;
          &lt;li&gt;クラスに分類するために役立つ領域を見つけるよう学習&lt;/li&gt;
          &lt;li&gt;マルチラベル・マルチクラスの &lt;code class=&quot;highlighter-rouge&quot;&gt;classification loss&lt;/code&gt; を最小化する&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Attention mining stream &lt;script type=&quot;math/tex&quot;&gt;S_{am}&lt;/script&gt;
        &lt;ul&gt;
          &lt;li&gt;クラスの決定に寄与する可能性があるすべての領域が attention に含まれるよう学習&lt;/li&gt;
          &lt;li&gt;対象クラスに対する attention 領域が大きくならないようにする &lt;code class=&quot;highlighter-rouge&quot;&gt;attention mining loss&lt;/code&gt; を最小化する&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gain_textrmext&quot;&gt;GAIN&lt;script type=&quot;math/tex&quot;&gt;_{\textrm{ext}}&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;追加で小規模のセグメンテーションマスクを学習に利用
    &lt;ul&gt;
      &lt;li&gt;External stream &lt;script type=&quot;math/tex&quot;&gt;S_{e}&lt;/script&gt; を追加
        &lt;ul&gt;
          &lt;li&gt;教師となるセグメンテーションマスクから attention map を学習&lt;/li&gt;
          &lt;li&gt;Attention map とセグメンテーションマスクの二乗誤差である &lt;code class=&quot;highlighter-rouge&quot;&gt;attention loss&lt;/code&gt; を最小化する&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;PASCAL VOC を用いた弱教師ありセグメンテーションタスクで先行研究の SoTA モデルとの mean intersection-over-union (mIoU) のスコアを比較している。
SoTA モデルである SEC に対して attention map を用いた提案手法を適用することにより精度向上が確認されている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SoTA モデルである SEC に対して GAIN を導入することにより、複数物体を正確に広い範囲で捉えることが可能となっている。
また追加でセグメンテーションマスクを用いることで、より精度のよいセグメンテーションが行われている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Tell-Me-Where-to-Look-Guided-Attention-Inference-Network/figure6.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;学習データセットにバイアスがある場合にベースラインの Grad-CAM と提案手法の GAIN がどのような予測をするか可視化を行った結果。
Grad-CAM は&lt;code class=&quot;highlighter-rouge&quot;&gt;海&lt;/code&gt;とともに現れやすい&lt;code class=&quot;highlighter-rouge&quot;&gt;ボート&lt;/code&gt;に過学習している傾向があるが、GAIN は&lt;code class=&quot;highlighter-rouge&quot;&gt;ボート&lt;/code&gt;を適切に捉えていることがわかる。
提案手法の GAIN がバイアスのあるデータセットに対してもロバストであることを示している。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SEC について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.06098&quot;&gt;Kolesnikov, Alexander, and Christoph H. Lampert. “Seed, expand and constrain: Three principles for weakly-supervised image segmentation.” European Conference on Computer Vision. Springer, Cham, 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.10171&quot;&gt;Li, Kunpeng, et al. “Tell me where to look: Guided attention inference network.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">SC-FEGAN: Face Editing Generative Adversarial Networks with User’s Sketch and Color</title><link href="https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color" rel="alternate" type="text/html" title="SC-FEGAN: Face Editing Generative Adversarial Networks with User's Sketch and Color" /><published>2019-02-21T00:00:00+00:00</published><updated>2019-02-21T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ユーザーのインタラクティブなスケッチや色指定が可能なニューラルネットワークベースの顔画像編集システム SC-FEGAN を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;従来の Generative Adversarial Network (GAN) を用いた顔画像編集システムは低画質でエッジを適切に捉えるのが難しかった。
また顔編集を行う際のユーザの入力に対して質の高い応答を返すのは困難であった。&lt;/p&gt;

&lt;p&gt;Deepfillv2 や Guided inpainting はこうしたユーザーが入力するマスキングや他の画像を入力として編集を可能としたが、編集時の色の指定ができなかったり、詳細な細かい復元が可能ではなかった。&lt;/p&gt;

&lt;p&gt;本研究では UNet ベースのアーキテクチャに対して gated convolutional layer を導入したネットワークアーキテクチャをベースにユーザーのスケッチや色指定を可能とした、
自由で高品質な顔編集システムである SC-FEGAN を提案した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;学習データに対する処理&quot;&gt;学習データに対する処理&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;高品質な瞳の編集
    &lt;ul&gt;
      &lt;li&gt;学習時にランダムに目周辺にマスキングを適用することで細かい瞳の復元を可能とする&lt;/li&gt;
      &lt;li&gt;同時に generative face completion (GFC) を適用する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ユーザーが入力するスケッチや色指定への対応
    &lt;ul&gt;
      &lt;li&gt;FaceShop と同様の手法を導入
        &lt;ul&gt;
          &lt;li&gt;スケッチデータをビットマップからベクターへと変換する&lt;a href=&quot;http://autotrace.sourceforge.net/&quot;&gt;AutoTrace&lt;/a&gt;は用いていない&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HED によるエッジ検出器を使用して、ユーザーの入力からスケッチデータを生成&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;提案顔編集システムのアーキテクチャ&quot;&gt;提案顔編集システムのアーキテクチャ&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generator
    &lt;ul&gt;
      &lt;li&gt;U-net ベースの generator を採用
        &lt;ul&gt;
          &lt;li&gt;すべての畳み込み層に gated convolution を導入している&lt;/li&gt;
          &lt;li&gt;畳み込みの後に local signal normalization (LRN)を用いている&lt;/li&gt;
          &lt;li&gt;入力は&lt;script type=&quot;math/tex&quot;&gt;512 \times 512 \times 9&lt;/script&gt;である
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RGB (3 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;マスク (1 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;スケッチ(1 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;色指定マップ(3 チャンネル)&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;ノイズ(1 チャンネル)&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminator
    &lt;ul&gt;
      &lt;li&gt;SN-patchGAN ベースの discriminator を採用
        &lt;ul&gt;
          &lt;li&gt;複数の loss 関数を最小化する
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;per-pixcel loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;perceptual loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;style loss&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;total variance loss&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;CelebA-HO データセットにおいてランダムに学習用とテスト用で分割したものに対して提案システムである SC-FEGAN による顔編集の質を検討している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;generator-の違いによる瞳の編集精度&quot;&gt;Generator の違いによる瞳の編集精度&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generator を U-net と Coarse-Refined net で変えたときに、瞳領域をマスクして復元したときの結果。U-net 構造のほうが細かい瞳の復元に成功している&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;perceptual-loss-の有無による編集精度&quot;&gt;Perceptual loss の有無による編集精度&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure5.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Perceptual loss である VGG loss を導入することにより、髪部分が正確に編集できている&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;先行研究との復元精度の比較&quot;&gt;先行研究との復元精度の比較&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure6.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;先行研究の Deepfillv1 に対して提案システムである SC-FEGAN による復元精度が高いことがわかる&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;アクセサリーなどの小さい物体に対する編集の可能性&quot;&gt;アクセサリーなどの小さい物体に対する編集の可能性&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SC-FEGAN-Face-Editing-Generative-Adversarial-Networks-with-Users-Sketch-and-Color/figure8.png&quot; alt=&quot;Figure 8&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HED をもとにマスキング領域を拡張して学習することにより、イヤリングと共に顔の画像を生成するという特別な結果を得ることができた&lt;/li&gt;
  &lt;li&gt;ネットワークが小さな詳細を学習し、小さな入力に対しても妥当な結果を生み出すことができることを示している&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Deepfillv2 (SN-patchGAN, gated convolutional layer) について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;&gt;Yu, Jiahui, et al. “Free-form image inpainting with gated convolution.” arXiv preprint arXiv:1806.03589 (2018).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Guided inpainting について
    &lt;ul&gt;
      &lt;li&gt;[Zhao, Yinan, et al. “Guided image inpainting: Replacing an image region by pulling content from another image.” arXiv preprint arXiv:1803.08435 (2018).]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative Face Completion (GFC) について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2017/html/Li_Generative_Face_Completion_CVPR_2017_paper.html&quot;&gt;Li, Yijun, et al. “Generative face completion.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FaceShop について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.08972&quot;&gt;Portenier, Tiziano, et al. “Faceshop: Deep sketch-based face image editing.” ACM Transactions on Graphics (TOG) 37.4 (2018): 99.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;U-net について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~jeanoh/16-785/papers/ronnenberger-miccai2015-u-net.pdf&quot;&gt;Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.06838&quot;&gt;Youngjoo Jo, Jongyoul Park. SC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color. arXiv:1902.06838, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Field-aware Probabilistic Embedding Neural Network for CTR Prediction</title><link href="https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction" rel="alternate" type="text/html" title="Field-aware Probabilistic Embedding Neural Network for CTR Prediction" /><published>2018-12-06T00:00:00+00:00</published><updated>2018-12-06T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;確率的埋め込みを導入し特徴表現の確率的振る舞いを学習する CTR 予測のためのモデル Field-aware Probabilistic Embedding Neural Network を提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Click Though Rate (CTR) 予測に通常用いられるロジスティック回帰 (LR) や Factorization Machine (FM) は一般的な線形の特徴を捉えることが可能でシンプルな実装であることが知られているが、高次元特徴の表現能力に乏しい。&lt;/p&gt;

&lt;p&gt;本件研究では特徴の embedding に対して点推定ではなく分布を推定する Probabilistic Embedding という新たな embedding 手法を提案し、事前知識を導入することで先行研究よりもロバストなモデルを構築した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Field-aware Probabilistic Embedding Neural Networks (FPENN)
    &lt;ul&gt;
      &lt;li&gt;Field-aware Probabilistic Embedding (FPE) による確率的埋め込みの学習
        &lt;ul&gt;
          &lt;li&gt;埋め込み行列 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の平均・分散からなる確率分布を用いて確率的な振る舞いを導入&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多様な特徴を学習するコンポーネント
        &lt;ul&gt;
          &lt;li&gt;Linear term (LT)
            &lt;ul&gt;
              &lt;li&gt;低次元の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Quadratic term (QDR)
            &lt;ul&gt;
              &lt;li&gt;2 次の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Deep NN term (DNN)
            &lt;ul&gt;
              &lt;li&gt;高次元の相互作用を捉える&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;埋め込みを確率分布としたときの学習手法の適用
        &lt;ul&gt;
          &lt;li&gt;Reparameterization trick
            &lt;ul&gt;
              &lt;li&gt;埋め込み行列 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の平均・分散を持つ正規分布から埋め込み表現をサンプリングする場合誤差逆伝播が不可能&lt;/li&gt;
              &lt;li&gt;Reparameterization trick でこの問題を解決&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;CTR 予測のデータセットとして Avazu dataset と Criteo dataset を用いて提案する FPENN とベースラインの LR や FM、CNN ベースの CCPM や DeepFM と比較を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;提案手法が 50 〜 100 ミリ秒以内に 200 〜 300 アプリの CTR 値を予測との記述あり&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CCPM について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2806603&quot;&gt;Liu, Qiang, et al. “A convolutional click prediction model.” Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DeepFM について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04247&quot;&gt;Guo, Huifeng, et al. “Deepfm: a factorization-machine based neural network for ctr prediction.” arXiv preprint arXiv:1703.04247 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3240396&quot;&gt;Weiwen Liu, Ruiming Tang, Jiajin Li, Jinkai Yu, Huifeng Guo, Xiuqiang He, and Shengyu Zhang. 2018. Field-aware probabilistic embedding neural network for CTR prediction. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys ‘18). ACM, New York, NY, USA, 412-416.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/shunk031/paper-summary-of-field-aware-probabilistic-embedding-neural-network-for-ctr-prediction&quot;&gt;[論文紹介] Field-aware Probabilistic Embedding Neural Network for CTR Prediction [RecSys 2018] / Paper summary of Field-aware Probabilistic Embedding Neural Network for CTR Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shunk031.github.io/paper-survey/assets/img/others/Field-aware-Probabilistic-Embedding-Neural-Network-for-CTR-Prediction/figure1.png" /></entry><entry><title type="html">Context-Dependent Sentiment Analysis in User-Generated Videos</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos" rel="alternate" type="text/html" title="Context-Dependent Sentiment Analysis in User-Generated Videos" /><published>2018-10-22T00:00:00+00:00</published><updated>2018-10-22T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Context-Dependent-Sentiment_analysis-in-User-Genereted-Videos">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;マルチモーダル (テキスト・音声・動画) における発音レベルの特徴量を用いた感情分析を行うモデルを提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;マルチモーダルを用いた感情分析を行うモデルは複数提案されてきたが、先行研究では発話レベルでの依存関係を無視したモデルのみしか提案されていなかった。
本研究では発話中の呼吸や小休止で区切り 1 つの単位とする &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; (発話レベル) の特徴量にフォーカスを当て、これらの系列を元にマルチモーダルの特徴を用いて感情分析を行った。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;
&lt;p&gt;マルチモーダル(テキスト・音声・動画)から &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; の特徴を抽出し、&lt;code class=&quot;highlighter-rouge&quot;&gt;Contextual-LSTM&lt;/code&gt; を用いてこれらの系列を考慮した感情分析モデルを構築&lt;/p&gt;

&lt;h3 id=&quot;発話レベルの特徴量抽出&quot;&gt;発話レベルの特徴量抽出&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;テキストからの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;text-CNN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;学習済み word2vec を入力し、複数種類のカーネルを持つ 2 層の CNN でテキストのセマンティクスを抽出&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;音声からの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;openSMILE&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;オープンソースのソフトウェアである openSMILE を用いて、音声のピッチや強度といった特徴量を抽出&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;動画からの特徴抽出
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3D-CNN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;通常の 2D の CNN に対して時間軸方向を考慮した 3D の CNN を使用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;コンテキストを考慮した特徴抽出&quot;&gt;コンテキストを考慮した特徴抽出&lt;/h3&gt;
&lt;p&gt;抽出した &lt;code class=&quot;highlighter-rouge&quot;&gt;utterance-level&lt;/code&gt; の特徴量に対して、発話ごとの関係性を学習する LSTM (&lt;code class=&quot;highlighter-rouge&quot;&gt;Contextual-LSTM&lt;/code&gt;) を用いる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Contextual-LSTM は複数種類の LSTM を比較&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sc-LSTM&lt;/code&gt; (simple contextual LSTM)
    &lt;ul&gt;
      &lt;li&gt;順方向のみのシンプルな LSTM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;h-LSTM&lt;/code&gt; (hidden LSTM)
    &lt;ul&gt;
      &lt;li&gt;LSTM の後に全結合層を入れずにそのまま隠れ状態を後段に渡す&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bc-LSTM&lt;/code&gt; (bi-directional LSTM)
    &lt;ul&gt;
      &lt;li&gt;発話に対して双方向の特徴を考慮できる bi-directional LSTM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;uni-SVM&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ベースラインとして SVM を使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;複数のモダリティ特徴を用いた感情分析&quot;&gt;複数のモダリティ特徴を用いた感情分析&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;複数のモダリティに対して階層構造を考慮する (Hierarchical Framework)
    &lt;ul&gt;
      &lt;li&gt;Level-1
        &lt;ul&gt;
          &lt;li&gt;各モダリティ (テキスト・音声・動画) の特徴量に対してそれぞれ独立で LSTM に通し、対象モダリティ独自のコンテキストをを学習&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Level-2
        &lt;ul&gt;
          &lt;li&gt;Level-1 で学習した各モダリティ独立した特徴を concat して LSTM に通すことでマルチモーダルなコンテキストを学習&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Level-1 と Level-2 は end-to-end での学習ではない&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;階層構造を考慮しない枠組み (Non-hierarchical Framework) についても比較
    &lt;ul&gt;
      &lt;li&gt;単一のモダリティ特徴を contextual-LSTM (sc-LSTM や bc-LSTM) に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;p&gt;マルチモーダルな感情分析を行う先行研究の大部分の検証実験において、学習データとテストデータに同じ話者が入っている場合が多い。
話者が同じ場合であっても、感情表現やセマンティクスはデータによって異なると考えられているためである。
しかしながら感情分析に対して一般性・話者に依存しないモデリングを行うことは重要である。&lt;/p&gt;

&lt;p&gt;本研究での検証実験では現実世界に即したアプリケーションを目指すため、未知の話者に対してもモデルがロバストになるように、
話者独立でデータセットの train/test 分割を行っている。&lt;/p&gt;

&lt;h3 id=&quot;マルチモーダルな感情分析を検証するためのデータセットについて&quot;&gt;マルチモーダルな感情分析を検証するためのデータセットについて&lt;/h3&gt;
&lt;h4 id=&quot;mosi&quot;&gt;MOSI&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;93 人が英語である話題についてレビューしている動画が含まれる&lt;/li&gt;
  &lt;li&gt;5 人のアノテーターが &lt;code class=&quot;highlighter-rouge&quot;&gt;-3&lt;/code&gt;〜&lt;code class=&quot;highlighter-rouge&quot;&gt;+3&lt;/code&gt; の感情値を付与
    &lt;ul&gt;
      &lt;li&gt;ラベルの平均値を計算し、 &lt;code class=&quot;highlighter-rouge&quot;&gt;positive&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;negative&lt;/code&gt; の 2 クラスとして使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;moud&quot;&gt;MOUD&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;55 人がスペイン語である製品についてレビューしている動画が含まれる&lt;/li&gt;
  &lt;li&gt;Google Translate API を用いてスペイン語から英語に変換&lt;/li&gt;
  &lt;li&gt;positive, negative, neutral のラベルが付与されているが、&lt;code class=&quot;highlighter-rouge&quot;&gt;positive&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;negative&lt;/code&gt;のみ使用&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;iemocap&quot;&gt;IEMOCAP&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;10 人が英語で対話している動画データセット&lt;/li&gt;
  &lt;li&gt;データセットには以下 9 つのラベルが付与されている: &lt;code class=&quot;highlighter-rouge&quot;&gt;anger&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;happiness&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sadness&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;neutral&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;excitement&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;frustration&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;fear&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;surprise&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;other&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;先行研究と比較するために上記最初の 4 つのクラスだけをラベルとして使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;8 人を学習データ、2 人をテストデータとして使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;また汎化性能を確認するため、MOSI データセットで学習したモデルを MOUD データセットで評価する &lt;code class=&quot;highlighter-rouge&quot;&gt;cross-dataset&lt;/code&gt; の枠組みを導入している。&lt;/p&gt;

&lt;h3 id=&quot;モデルのパフォーマンス比較&quot;&gt;モデルのパフォーマンス比較&lt;/h3&gt;
&lt;h4 id=&quot;階層的-vs-非階層的な学習フレームワーク&quot;&gt;階層的 vs 非階層的な学習フレームワーク&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;非階層的なフレームワークはベースラインの uni-SVM を超えたが、階層的フレームワークが一番良かった&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;lstm-の種類の違いによる比較&quot;&gt;LSTM の種類の違いによる比較&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;sc-LSTM、bc-LSTM ともに良い結果を出している&lt;/li&gt;
  &lt;li&gt;bc-LSTM は順方向・逆方向のコンテキストを考慮できるため sc-LSTM より優れていた&lt;/li&gt;
  &lt;li&gt;全結合層がない場合 (h-LSTM) よりある場合のほうが性能は高い&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ベースラインとの比較&quot;&gt;ベースラインとの比較&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM を用いたネットワークはすべてのデータセットに対してベースラインの uni-SVM を超える性能を出した
    &lt;ul&gt;
      &lt;li&gt;発話間の文脈依存性をモデリングすることで感情分類性のを高めている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IEMOCAP データセットでは特にベースラインとの性能差が現れた
    &lt;ul&gt;
      &lt;li&gt;より広い依存関係を適切に捉える必要があるため、LSTM ネットワークが効果的であった&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SoTA のモデルとの比較
    &lt;ul&gt;
      &lt;li&gt;SoTA モデルは実験の際に話者が独立するように train/test の分割をしていない&lt;/li&gt;
      &lt;li&gt;発話の文脈依存を考慮していない&lt;/li&gt;
      &lt;li&gt;提案手法は SoTA モデルを上回る性能を示した&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;各モダリティの重要度&quot;&gt;各モダリティの重要度&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;unimodal な特徴より bimodal や trimodal な特徴を用いたほうが性能は良かった&lt;/li&gt;
  &lt;li&gt;音声特徴は視覚特徴よりも効果があった&lt;/li&gt;
  &lt;li&gt;MOSI データセットと IEMOCAP データセットにおいてはテキスト特徴が有効であった&lt;/li&gt;
  &lt;li&gt;MOUD データセットにおいてはスペイン語から英語に翻訳した影響により、テキスト特徴より音声特徴のほうが有効であった
    &lt;ul&gt;
      &lt;li&gt;スペイン語の word vector を用いることでテキスト特徴における性能が向上&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;モデルの汎化性能について&quot;&gt;モデルの汎化性能について&lt;/h4&gt;
&lt;p&gt;MOSI データセットで学習を行い、MOUD データセットを用いて評価を行った。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;音声およびテキスト特徴を用いた場合性能が低下した
    &lt;ul&gt;
      &lt;li&gt;英語のモデルでスペイン語を予測していたから&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;視覚特徴を用いた場合、音声・テキスト特徴よりも性能が良かった
    &lt;ul&gt;
      &lt;li&gt;クロスリンガルであっても視覚特徴は一般的な特徴を学習できる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;発話の文脈依存を考慮できることで対象の発話を正確に分析することができている&lt;/li&gt;
  &lt;li&gt;音声特徴では正しく分類できない場合でも、テキスト特徴で正しく分類する例がある
    &lt;ul&gt;
      &lt;li&gt;逆に、テキストではポジティブな文脈でも、怒りっぽい音声特徴から感情を捉えてネガティブであると正しく分類する例もある&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ベースラインについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D15-1303&quot;&gt;Poria, Soujanya, Erik Cambria, and Alexander Gelbukh. “Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis.” Proceedings of the 2015 conference on empirical methods in natural language processing. 2015.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6411794/&quot;&gt;Rozgic, Viktor, et al. “Ensemble of svm trees for multimodal emotion recognition.” Signal &amp;amp; Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific. IEEE, 2012.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P17-1081&quot;&gt;Poria, Soujanya, et al. “Context-dependent sentiment analysis in user-generated videos.” Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shunk031.github.io/paper-survey/assets/img/nlp/Context-Dependent-Sentiment-Analysis-in-User-Generated-Videos/figure2.png" /></entry><entry><title type="html">Subcharacter Information in Japanese Embeddings: When Is It Worth It?</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It" rel="alternate" type="text/html" title="Subcharacter Information in Japanese Embeddings: When Is It Worth It?" /><published>2018-09-28T00:00:00+00:00</published><updated>2018-09-28T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字が有する部首のコンポーネントを分解し、サブキャラクターとして扱った際の言語タスクにおける性能を、新たに提案するデータセットも含めて調査を行った。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;漢字は自身に&lt;code class=&quot;highlighter-rouge&quot;&gt;へん&lt;/code&gt;や&lt;code class=&quot;highlighter-rouge&quot;&gt;つくり&lt;/code&gt;といった複数のコンポーネントを有している。
これらサブキャラクターとして埋め込みを学習することで、中国語のいくつかの言語処理タスクで良い精度となることが報告されている。
本研究ではこうしたサブキャラクターの情報を日本語に対して適用した場合の効果を調査している。&lt;/p&gt;

&lt;p&gt;中国語で効果のあるサブキャラクター情報が日本語においては限定的であり、
多くの場合 character-level ngram や character-level のモデルが subcharacter-level のモデルよりも良い性能を示す結果となっている。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;前処理&quot;&gt;前処理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;文字から部首を取り出す
    &lt;ul&gt;
      &lt;li&gt;部首データベースである&lt;a href=&quot;https://github.com/cjkvi/cjkvi-ids&quot;&gt;IDS&lt;/a&gt;を用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;使用モデル&quot;&gt;使用モデル&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Skip-gram + kanji モデル (character-level)
    &lt;ul&gt;
      &lt;li&gt;入力単語に対して skip-gram で得たベクトルと文字に分割して得たベクトルを用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Skip-gram + kanji + bushu モデル (subcharacter-level)
    &lt;ul&gt;
      &lt;li&gt;上記のモデルに対して漢字を部首に分割して得たベクトルを用いる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;評価データセット-jbats-の提案&quot;&gt;評価データセット jBATS の提案&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/table1.png&quot; alt=&quot;Table 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;40 の言語的関連性が定義されたデータセットである jBATS を提案。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;単語の類似度、jBATS を用いたアナロジータスク、感情分析の 3 つの実験を行っている。&lt;/p&gt;

&lt;p&gt;実験結果では subcharacter-level な入力より character-level の入力を用いたモデルのほうが良い精度となる場合が多かった。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Subcharacter-Information-in-Japanese-Embeddings-When-Is-It-Worth-It/table7.png&quot; alt=&quot;Table 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;疒&lt;/code&gt;(やまいだれ)や&lt;code class=&quot;highlighter-rouge&quot;&gt;豸&lt;/code&gt;(なじなへん)に対して近いベクトル表現を得た場合の結果。
やまいだれを持つ&lt;code class=&quot;highlighter-rouge&quot;&gt;症&lt;/code&gt;が近い表現になっていることはもちろんのこと、&lt;code class=&quot;highlighter-rouge&quot;&gt;患&lt;/code&gt;や&lt;code class=&quot;highlighter-rouge&quot;&gt;腫&lt;/code&gt;といった病気に関わる単語が近い表現になっている。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;日本語におけるサブキャラクターを用いた言語モデリングについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://aclanthology.info/papers/W17-4122/w17-4122&quot;&gt;Nguyen, Viet, Julian Brooke, and Timothy Baldwin. “Sub-character Neural Language Modelling in Japanese.” Proceedings of the First Workshop on Subword and Character Level Models in NLP. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jBATS について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://vecto.space/projects/jBATS/&quot;&gt;The Japanese Bigger Analogy Test Set (jBATS)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.info/papers/W18-2905/w18-2905&quot;&gt;Karpinska, Marzena, et al. “Subcharacter Information in Japanese Embeddings: When Is It Worth It?.” Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP. 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Semi-supervised Deep Learning by Metric Embedding</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding" rel="alternate" type="text/html" title="Semi-supervised Deep Learning by Metric Embedding" /><published>2018-08-25T00:00:00+00:00</published><updated>2018-08-25T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Semi-supervised-deep-learning-by-metric-embedding">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;少ないラベル付きデータとラベルなしデータを元に距離埋め込み (neighbor embedding) を学習する、半教師あり学習を提案&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;従来のニューラルネットワークの学習では学習データに対してラベルを推定する枠組みであったが、ラベル付きの学習データが少ない場合に容易に過学習を引き起こす。
本研究ではラベル付きの学習データに対して距離埋め込み (neighbor embedding) を推定する枠組みを導入することで、ラベルなしデータも含めて学習を行い精度を向上させた。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Embedding同士の距離比較による学習 (neighbor embedding)
    &lt;ul&gt;
      &lt;li&gt;学習データ &lt;script type=&quot;math/tex&quot;&gt;x \in \mathcal{X}&lt;/script&gt; バッチサイズ分サンプリング&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; クラスのラベル付きデータ &lt;script type=&quot;math/tex&quot;&gt;z_1, \cdots, z_c \in X_L&lt;/script&gt; を各クラス1サンプルずつサンプリング&lt;/li&gt;
      &lt;li&gt;学習データをembeddingした &lt;script type=&quot;math/tex&quot;&gt;F(x)&lt;/script&gt; と ラベル付きデータをembeddingした &lt;script type=&quot;math/tex&quot;&gt;F(z_1), \cdots, F(z_c)&lt;/script&gt; に対してそれぞれ &lt;code class=&quot;highlighter-rouge&quot;&gt;L2 norm squared&lt;/code&gt; の逆数を計算&lt;/li&gt;
      &lt;li&gt;計算した距離の逆数に対して &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; 値を計算&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x; z_1, \cdots, z_c)_{i} = \frac{e^{-||F(x) - F(z_i)||^2}}{\sum_{j=1}^{c} e^{-||F(x) - F(z_i)||^2}}, i \in \{1 \cdots c \}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt; 値と学習データの教師ラベルとの &lt;code class=&quot;highlighter-rouge&quot;&gt;cross entropy&lt;/code&gt; を計算する。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;半教師あり学習への応用
    &lt;ul&gt;
      &lt;li&gt;ラベルなしデータ群 &lt;script type=&quot;math/tex&quot;&gt;X_U&lt;/script&gt; からサンプリングして以下を計算&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, z_1, \cdots, z_c)_U = - \sum_{i=1}^{c} \frac{ e^{-||F(x) - F(z_i)||^2} }{ \sum_{j=1}^{c} e^{-||F(x) - F(z_j)||^2} } \cdot \log{\frac{ e^{-||F(x) - F(z_i)||^2} }{ \sum_{j=1}^{c} e^{-||F(x) - F(z_j)||^2}}}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;MNISTおよびCIFAR10に対して先行研究のモデル(EmbedCNN, SWWAE, Ladder network, Conv-CatGAN / Spike-and-Slab Sparse Coding, View-Invariant k-means, Exampler-CNN, Ladder network, Conv-CatGan, Improved GAN)と提案手法の比較を行っている。&lt;/p&gt;

&lt;p&gt;MNISTに対しては各クラス100枚ずつにのみ教師ラベルを付与し、CIFAR10に対しては各クラス400枚ずつにのみ教師ラベルを付与し実験を行っている。&lt;/p&gt;

&lt;p&gt;学習時にdata agumentationは行わず、テスト時には出力したembeddingに対してk-NNを用いてk={1, 3, 5}の場合の予測結果をaveragingしている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;MNISTデータセットに対して、実験で使用したモデルに2次元のembeddingを出力する全結合層を追加し、可視化を行った結果である。色付きの点は教師ラベルありのサンプルであり、グレーの点は教師ラベルなしのサンプルである。
ラベルありデータは1つのクラスタを形成しており、ラベルなしデータは大部分において各クラスタに属するような形で分布していることがわかる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Semi-supervised-deep-learning-by-metric-embedding/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;EmbedCNNについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-35289-8_34&quot;&gt;Weston, Jason, et al. “Deep learning via semi-supervised embedding.” Neural Networks: Tricks of the Trade. Springer, Berlin, Heidelberg, 2012. 639-655.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SWWAEについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02351&quot;&gt;Zhao, J., et al. “Stacked What-Where Auto-encoders. arXiv 2015.” arXiv preprint arXiv:1506.02351.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ladder networkについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks&quot;&gt;Rasmus, Antti, et al. “Semi-supervised learning with ladder networks.” Advances in Neural Information Processing Systems. 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conv-CatGANについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06390&quot;&gt;Springenberg, Jost Tobias. “Unsupervised and semi-supervised learning with categorical generative adversarial networks.” arXiv preprint arXiv:1511.06390 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spike-and-Slab Sparse Codingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1206.6407&quot;&gt;Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. “Large-scale feature learning with spike-and-slab sparse coding.” arXiv preprint arXiv:1206.6407 (2012).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;View-Invatiant k-meansについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v28/yuhui13.pdf&quot;&gt;Hui, Ka Yu. “Direct modeling of complex invariances for visual object features.” International Conference on Machine Learning. 2013.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Exampler-CNNについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/7312476/&quot;&gt;Dosovitskiy, Alexey, et al. “Discriminative unsupervised feature learning with exemplar convolutional neural networks.” IEEE transactions on pattern analysis and machine intelligence 38.9 (2016): 1734-1747.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ImprovedGanについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03498&quot;&gt;Salimans, Tim, et al. “Improved techniques for training gans.” Advances in Neural Information Processing Systems. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01449&quot;&gt;Hoffer, Elad, and Nir Ailon. “Semi-supervised deep learning by metric embedding.” arXiv preprint arXiv:1611.01449 (2016).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Joint Embedding of Words and Labels for Text Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification" rel="alternate" type="text/html" title="Joint Embedding of Words and Labels for Text Classification" /><published>2018-06-17T00:00:00+00:00</published><updated>2018-06-17T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;テキスト分類の際に教師ラベルのembeddingと単語のembeddingを組み合わせたattentionの枠組みを用いる、Label-Embedding Attentive Model (LEAM) を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;画像認識分野および自然言語処理分野において、label embeddingを用いたさまざまな枠組みが提案されてきた。近年の自然言語処理分野では単語embeddingやattentionを用いることで、テキスト分類等のタスクの精度向上が示されてきた。
本研究では効果的なattentionモデル構築のためにlabel embeddingを学習する、LEAMを提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;単語embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf V}&lt;/script&gt; と label embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf C}&lt;/script&gt; から &lt;code class=&quot;highlighter-rouge&quot;&gt;compatibility&lt;/code&gt; &lt;script type=&quot;math/tex&quot;&gt;{\bf G}&lt;/script&gt; を計算&lt;/li&gt;
  &lt;li&gt;softmaxを用いて &lt;script type=&quot;math/tex&quot;&gt;{\bf G}&lt;/script&gt; をnormalizeしたattention &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; を計算&lt;/li&gt;
  &lt;li&gt;単語embeddingとattentionの重み付け平均を計算したdocument embedding &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; を用いてテキスト分類を行う&lt;/li&gt;
  &lt;li&gt;テスト時にはlabel embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf C}&lt;/script&gt; において、すべてのクラスのembeddingを利用する&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;AGNews、Yelp Review Full、Yelp Review Polarity、DBPedia、Yahoo! Answers Topicの5つのデータセットを用いている。
ベースラインのモデルとしてBag-of-words、Shallow/Large word CNN、LSTM、SA-LSTM、Deep CNN、SWEM、fastText、HAN、Bi-BloSANとテキスト分類の精度を比較している。&lt;/p&gt;

&lt;p&gt;上記に加えて医療テキストデータセットであるMIMIC-IIIを用いた実践的な評価を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;モデルのパラメータ数と学習時間について
    &lt;ul&gt;
      &lt;li&gt;SWEMに次いで少ないパラメータ数と学習時間を実現している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;label embeddingの有効性について
    &lt;ul&gt;
      &lt;li&gt;学習から得られたlabel embeddingとdocument embeddingをt-SNEで可視化すると、クラスに対応するlabel embeddingとdocument embeddingに強い相関が見られた
&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;医療テキストに対する有効性について
    &lt;ul&gt;
      &lt;li&gt;attentionを可視化すると、医療に関連する語がハイライトされていることが示されている。
&lt;img src=&quot;/paper-survey/assets/img/nlp/Joint-Embedding-of-Words-and-Labels-for-Text-Classification/figure4.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;画像認識分野におけるlabel embedding
    &lt;ul&gt;
      &lt;li&gt;画像分類
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/7293699/&quot;&gt;Akata, Zeynep, et al. “Label-embedding for image classification.” IEEE transactions on pattern analysis and machine intelligence 38.7 (2016): 1425-1438.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;マルチモーダル
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model&quot;&gt;Frome, Andrea, et al. “Devise: A deep visual-semantic embedding model.” Advances in neural information processing systems. 2013.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.2539&quot;&gt;Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. “Unifying visual-semantic embeddings with multimodal neural language models.” arXiv preprint arXiv:1411.2539 (2014).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;画像中のテキスト認識
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.433.2642&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Rodriguez-Serrano, Jose A., Florent Perronnin, and France Meylan. “Label embedding for text recognition.” Proceedings of the British Machine Vision Conference. 2013.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zero-shot learning
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/3650-zero-shot-learning-with-semantic-output-codes&quot;&gt;Palatucci, Mark, et al. “Zero-shot learning with semantic output codes.” Advances in neural information processing systems. 2009.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-2048&quot;&gt;Yogatama, Dani, Daniel Gillick, and Nevena Lazic. “Embedding methods for fine grained entity type classification.” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Vol. 2. 2015.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/C16-1017&quot;&gt;Ma, Yukun, Erik Cambria, and Sa Gao. “Label embedding for zero-shot fine-grained named entity typing.” Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;自然言語処理におけるlabel embedding
    &lt;ul&gt;
      &lt;li&gt;Heterogeneous networkによるlabel embedding
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2783307&quot;&gt;Tang, Jian, Meng Qu, and Qiaozhu Mei. “Pte: Predictive text embedding through large-scale heterogeneous text networks.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;マルチタスク学習
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7005-deconvolutional-paragraph-representation-learning&quot;&gt;Zhang, Yizhe, et al. “Deconvolutional paragraph representation learning.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ベースラインのモデルについて
    &lt;ul&gt;
      &lt;li&gt;Bag-of-words、Shallow/Large word CNN、LSTM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica&quot;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;SA-LSTM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning&quot;&gt;Dai, Andrew M., and Quoc V. Le. “Semi-supervised sequence learning.” Advances in Neural Information Processing Systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deep CNN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/E17-1104&quot;&gt;Conneau, Alexis, et al. “Very deep convolutional networks for text classification.” Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Vol. 1. 2017.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;SWEM
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.09843&quot;&gt;Shen, Dinghan, et al. “Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms.” arXiv preprint arXiv:1805.09843 (2018).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HAN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/N16-1174&quot;&gt;Yang, Zichao, et al. “Hierarchical attention networks for document classification.” Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bi-BloSAN
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.00857&quot;&gt;Shen, Tao, et al. “Bi-directional block self-attention for fast and memory-efficient sequence modeling.” arXiv preprint arXiv:1804.00857 (2018).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.04174&quot;&gt;Wang, Guoyin, et al. “Joint Embedding of Words and Labels for Text Classification.” arXiv preprint arXiv:1805.04174 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>