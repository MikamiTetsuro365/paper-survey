<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2018-03-26T12:45:37+00:00</updated><id>https://shunk031.github.io/paper-survey/</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Glyph-aware Embedding of Chinese Characters</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters" rel="alternate" type="text/html" title="Glyph-aware Embedding of Chinese Characters" /><published>2018-03-26T00:00:00+00:00</published><updated>2018-03-26T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Glyph-aware-Embedding-of-Chinese-Characters">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;漢字特有の文字の表意性を明示的に組み込み，文字形状を意識した文字の埋め込み手法を提案．&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;英語と比べて中国語は多数の文字が使用され，なおかつ単語と単語の境目が明確ではない．
漢字には形状的な特徴があり，特に”へん”や”つくり”などの部分的な構造が集まり，それら自身が文字の意味・構文的役割・発音などの情報を有している．&lt;/p&gt;

&lt;p&gt;本研究ではこれら漢字の文字形状に着目することで，優れた文字表現の獲得を目指す &lt;em&gt;glyph-aware embedding&lt;/em&gt; を提案している．&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Glyph-aware-Embedding-of-Chinese-Characters/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN embedder
    &lt;ul&gt;
      &lt;li&gt;文字形状を考慮した文字の埋め込みをCNNを用いて実現&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ID embedder
    &lt;ul&gt;
      &lt;li&gt;文脈を考慮した文字の埋め込みをLookup tableを用いて実現&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文字形状のみ考慮した場合，「土」⇆「士」や「人」⇆「入」といった文字形状が似ている文字が近い表現となってしまう場合が考えられる．
そこで文脈を考慮できるLookup tableによるID embeddingを併せて用いることで，この問題を解消している．&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;中国語の言語モデリングタスクと中国語の単語分割タスクについて提案手法の精度を検証している．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;言語モデリングタスク
    &lt;ul&gt;
      &lt;li&gt;embedderで文字表現を得た後，後段のGRUで言語モデルタスクを解いている&lt;/li&gt;
      &lt;li&gt;ベースラインとして全結合層のみからなるlinear embedderを設定&lt;/li&gt;
      &lt;li&gt;Microsoft Research dataset (MSR) を用いて，CNN embedder・ID embedder・ID+CNN embedderそれぞれのperplexityを比較している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;単語分割タスク
    &lt;ul&gt;
      &lt;li&gt;embedderで文字表現を得た後，後段のGRU/Bidirectional LSTMで単語分割タスクを解いている&lt;/li&gt;
      &lt;li&gt;Peking University dataset (PKU) および MSRを用いて，CNN embedder・ID embedder・ID+CNN embedderそれぞれの精度を比較している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各タスクともCNN embedderが優れた性能を示している．&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;入力文字画像にdata augmentationを加えることで精度が向上&lt;/li&gt;
  &lt;li&gt;CNN embedder / ID embedderのパラメータ数について
    &lt;ul&gt;
      &lt;li&gt;埋め込み次元 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; ，ボキャブラリサイズ &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; の場合&lt;/li&gt;
      &lt;li&gt;CNN embedder: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(N+K)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;ID embedder: &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(NK)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;CNN embedderのほうがID embdderよりも少ないパラメータ数で良いパフォーマンスを出していることが分かる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P15-2098&quot;&gt;Shi, Xinlei, et al. “Radical embedding: Delving deeper to chinese radicals.” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Vol. 2. 2015.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04859&quot;&gt;Liu, Frederick, et al. “Learning character-level compositionality with visual features.” arXiv preprint arXiv:1704.04859 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/s10590-017-9196-0&quot;&gt;Costa-Jussà, Marta R., David Aldón, and José AR Fonollosa. “Chinese–spanish neural machine translation enhanced with character and word bitmap fonts.” Machine Translation 31.1-2 (2017): 35-47.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.00028&quot;&gt;Dai, Falcon Z., and Zheng Cai. “Glyph-aware Embedding of Chinese Characters.” arXiv preprint arXiv:1709.00028 (2017).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">A New Method of Region Embedding for Text Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification" rel="alternate" type="text/html" title="A New Method of Region Embedding for Text Classification" /><published>2018-02-20T00:00:00+00:00</published><updated>2018-02-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;CNNやRNNを必要とせずに語順を考慮することができるLocal Context Unitを利用し、タスク固有の単語埋め込み表現を学習するRegion Embeddingを提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;文書分類などのタスクにおいて単語の語順を考慮した単語表現にn-gramが用いられることが多いが、
特に &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; の値が大きいn-gramの場合、モデルが大きくなってしまったり、データスパースネス問題が起こる恐れがある。&lt;/p&gt;

&lt;p&gt;近年ではn-gramを考慮した単語の分散表現を獲得するFastTextが提案されている。
また&lt;a href=&quot;/paper-survey/summry/nlp/Semi-supervised-Convolutional-Neural-Networks-for-Text-Categorization-via-Region-Embedding&quot;&gt;Johnson &amp;amp; Zhang (2015)&lt;/a&gt;では
CNNベースのモデルを用いて単語表現を獲得するregion embeddingという手法を提案しているが、本研究のregion embeddingとは異なり、タスク依存でない点や、教師なし学習の枠組みで学習されている点で異なっている。&lt;/p&gt;

&lt;p&gt;Attentionのみを使用したニューラル機械翻訳モデルであるTransformerは、CNNやRNNを用いずに語順を考慮し、文脈の特徴を学習できていることが示されている。&lt;/p&gt;

&lt;p&gt;本研究ではTransformer参考に、ある単語の周辺のコンテキストを考慮できるLocal context unitを用いて単語の埋め込み表現を獲得するRegion Embeddingを提案し、文書分類タスクで精度が上がることを示している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Region Embedding
    &lt;ul&gt;
      &lt;li&gt;テキスト中の小さな範囲(region)から、局所的な特徴を保持した表現を獲得したい&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Local Context Unit
    &lt;ul&gt;
      &lt;li&gt;ある単語の語順と周辺のコンテキストを学習するパラメータ&lt;/li&gt;
      &lt;li&gt;通常のlook up tableを用いたword embedding &lt;script type=&quot;math/tex&quot;&gt;{\bf e}_{w_i}&lt;/script&gt; とlocal context unit &lt;script type=&quot;math/tex&quot;&gt;{\bf K}_{W_i}&lt;/script&gt;を組み合わせた埋め込み表現 &lt;script type=&quot;math/tex&quot;&gt;p^{i}_{w_i}&lt;/script&gt; を学習する&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  p^{i}_{w_i + t} = {\bf K}_{w_i, t} \odot {\bf e}_{w_i + t}
\end{align*}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Word-Context Region Embedding
    &lt;ul&gt;
      &lt;li&gt;regionの中心の語が前後のコンテキストから受ける影響に焦点を当てたemnedding手法&lt;/li&gt;
      &lt;li&gt;語の出現順によって(特に否定語や強調語など)、意味が逆転する場合を上手く学習することを期待&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Context-Word Region Embedding
    &lt;ul&gt;
      &lt;li&gt;Word-Context Region Embeddingとは逆に、コンテキストがregionの中心語から受ける影響に焦点を当てている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Region Embeddingを全結合層で分類&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;8つのデータセットを用いて感情分析、新聞記事分類、QAなどのタスクに対して精度を比較している。
ベースラインのn-gram・TFIDFなどの従来の単語表現を用いたモデル、Char-CNN、Char-CRNN、VDCNN、D-LSTM、bigram-FastTextと先行研究のRegion Embeddingを用いた分類器を比較している。
8つのデータセットのうち6つのデータセットで最先端の結果を達成していることが示されている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;region size / embedding sizeを変えた場合について
    &lt;ul&gt;
      &lt;li&gt;本研究ではregion sizeを7、embedding sizeを128に設定している&lt;/li&gt;
      &lt;li&gt;複数サイズのregion embeddingを組み合わせることで僅かに精度が向上している&lt;/li&gt;
      &lt;li&gt;emnedding sizeを大きくすると従来のFastTextやCNNなどは過学習が見られるが、Region Embeddingはほかと比べてロバストであることが示されている
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ある単語における周辺単語の共起について
    &lt;ul&gt;
      &lt;li&gt;“however”についてはhoweverの後が文書分類に重要であることを示している&lt;/li&gt;
      &lt;li&gt;“very”についてはveryの直後の語が重要である etc&lt;/li&gt;
      &lt;li&gt;local context unitが局所的な潜在意味を捉えていることが分かる
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;感情分析におけるlocal context unitの効果の可視化
    &lt;ul&gt;
      &lt;li&gt;context unitがある場合、正しく形容詞が正しく係り、positive/negativeの判定が正しく行われるようになったことが示されている
&lt;img src=&quot;/paper-survey/assets/img/nlp/A-New-Method-of-Region-Embedding-for-Text-Classification/table4.png&quot; alt=&quot;Table 4&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;FastTextを用いたembeddingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;Joulin, Armand, et al. “Bag of tricks for efficient text classification.” arXiv preprint arXiv:1607.01759 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNNを用いたembeddingについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.1058&quot;&gt;Johnson, Rie, and Tong Zhang. “Effective use of word order for text categorization with convolutional neural networks.” arXiv preprint arXiv:1412.1058 (2014).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformerについて
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ベースラインのモデルについて
    &lt;ul&gt;
      &lt;li&gt;Char-CNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica&quot;&gt;Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classification.” Advances in neural information processing systems. 2015.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Char-CRNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.00367&quot;&gt;Xiao, Yijun, and Kyunghyun Cho. “Efficient character-level document classification by combining convolution and recurrent layers.” arXiv preprint arXiv:1602.00367 (2016).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;VDCNNについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01781&quot;&gt;Conneau, Alexis, et al. “Very deep convolutional networks for natural language processing.” arXiv preprint arXiv:1606.01781 (2016).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;D-LSTMについて
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01898&quot;&gt;Yogatama, Dani, et al. “Generative and discriminative text classification with recurrent neural networks.” arXiv preprint arXiv:1703.01898 (2017).&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;予測における寄与単語の可視化について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01066&quot;&gt;Li, Jiwei, et al. “Visualizing and understanding neural models in NLP.” arXiv preprint arXiv:1506.01066 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BkSDMA36Z&quot;&gt;Chao Qiao, Bo Huang, Guocheng Niu, Daren Li, Daxiang Dong, Wei He, Dianhai Yu, Hua Wu, “A New Method of Region Embedding for Text Classification,” International Conference on Learning Representations, 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Variable Importance Using Decision Tree</title><link href="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees" rel="alternate" type="text/html" title="Variable Importance Using Decision Tree" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Variable-Importance-Using-Decision-Trees">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;決定木とランダムフォレストは優れたパフォーマンスと示すだけでなく、Feature importanceといった情報が分かる、確立されたモデルである。 不純度ベースで変数の重要度を知ることができるため使われることが多いモデルであるが、これらの重要度は理論的に不明な点が多い。&lt;/p&gt;

&lt;p&gt;本研究ではDSTUMPを提案し、様々な仮定におけるモデリングの下で高次元のデータ利用で有限のサンプルパフォーマンス保証性を導き出すことによって、ツリー系のモデルの性能についての考察を行っている。またこれら不純度ベースの手法の有効性について、広範囲の実験を下に有効性を実証している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Variable-Importance-Using-Decision-Trees/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6646-variable-importance-using-decision-trees&quot;&gt;Kazemitabar, Jalil, et al. “Variable Importance using Decision Trees.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</title><link href="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data" rel="alternate" type="text/html" title="Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では教師なしの連続データに対して解釈可能な表現を学習するFactrized hierarchical variational autoencoderを提案している。具体的には潜在変数のことなるデータに対し、事前確率としてSequence-dependent priorsとSequence-independent priorsをFactorized hierarchical graphical model内で組み合わせることで、連続データが持つマルチスケールな情報を利用するモデルとなっている。&lt;/p&gt;

&lt;p&gt;本モデルは2つの音声コーパスTIMITとAurora-4を用いて評価を行っている。具体的には異なる潜在変数の組を使って、スピーカーや言語コンテンツを変換する能力を定性的に評価している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Unsupervised-Learning-of-Disentangled-and-Interpretable-Representations-from-Sequential-Data/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6784-unsupervised-learning-of-disentangled-latent-representations-from-sequential-data&quot;&gt;Hsu, Wei-Ning, Yu Zhang, and James Glass. “Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Sparse Embedded k-Means Clustering</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering" rel="alternate" type="text/html" title="Sparse Embedded k-Means Clustering" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Sparse-Embedded-k-Means-Clustering">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;K-meansクラスタリングは広く知られている素晴らしいアルゴリズムであるが、高次元のデータに対しては、計算コストの高さゆえに様々な分野への応用を妨げている現状がある。一般的には次元削減の手法を用いて対処することが多いが、近年Random projection(RP)などの手法を用いて高速なK-meansクラスタリングを実現することができる。しかしながらこの手法は他の次元削減手法よりも多くの改善点が存在している。例として特異値分解(SVD)に基づく特徴抽出手法と比較して、RPは近似を行いつつ、データ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; で特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; のデータ &lt;script type=&quot;math/tex&quot;&gt;X \in \mathbb{R}^{n \times d}&lt;/script&gt; に対して &lt;script type=&quot;math/tex&quot;&gt;\min{(n, d)}\epsilon^{2} \frac{\log{(d)}}{k}&lt;/script&gt; だけ実行時間を削減している。&lt;/p&gt;

&lt;p&gt;これらの改善を経てもなお行列の乗算には &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O} \left(\frac{ndk}{\epsilon^{2} \log{(d)}} \right)&lt;/script&gt; だけ必要であり、特にデータ数 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; や特徴数 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; が大きい場合にはとても大きな計算コストとなってしまう。これらのボトルネックを解消するために、本研究では &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(nnz(X))&lt;/script&gt; ( &lt;script type=&quot;math/tex&quot;&gt;nnz(X)&lt;/script&gt; は &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 内における非ゼロの数を表している) を必要とする高速な行列の乗算を行う枠組みを用いて、スパースな埋め込み表現に対してk-meansクラスタリングを行う手法を提案をしている。また本研究ではRPの近似精度についても改善を行っている。ILSVRC2012等のデータセットに対して従来の次元削減手法を次元を落としてからk-meansクラスタリングをした結果と、提案手法の高速な次元圧縮を利用したクラスタリング結果を比較している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Sparse-Embedded-k-Means-Clustering/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6924-sparse-embedded-k-means-clustering.pdf&quot;&gt;Liu, Weiwei, Xiaobo Shen, and Ivor Tsang. “Sparse Embedded $ k $-Means Clustering.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks</title><link href="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks" rel="alternate" type="text/html" title="SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究では特異値分解を利用した、とても大きいボキャブラリを持つSoftmax関数を高速に近似する手法を提案している。SVD-Softmaxはニューラル言語モデルの推論時に最上位となりうる単語について、高速かつ正確に確率推定を行うことを目的としている。提案手法ではSVDを使って出力ベクトルの計算に用いた重み行列を変換する。各単語の近似確率については、いくつかの大きな特異値を使用することで単語の大部分の性質を持たせることができ、これを利用して重み行列を推定できると主張している。&lt;/p&gt;

&lt;p&gt;本研究の手法を言語モデリングとニューラル機械翻訳に適用することで、提案手法で導入されている近似手法が効果を発揮していることを検証している。本アルゴリズムでは800,000個の語彙の場合においても、約20%程度の算術演算しか必要とせず、GPUを利用することで3倍以上スピードアップしていることが分かっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/SVD-Softmax-Fast-Sotfmax-Approximation-on-Large-Vocabulary-Neural-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks&quot;&gt;Shim, Kyuhong, et al. “SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization" rel="alternate" type="text/html" title="Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;過学習はDeep neural networkの学習における課題の1つであり、汎化性能を向上させるためにさまざまな正則化手法が提案されてきた。中でも学習中にHidden unitに対してノイズを乗せるDropoutは有名な正則化手法として知られているが、こうした正則化手法がなぜ効果があるのかは不明である。&lt;/p&gt;

&lt;p&gt;本研究では従来のノイズ付加による正則化手法が真の目的関数の下限に対して最適化すること、Stochastic gradient descentにおいて、より制約の強い下限にフィットするよう複数のノイズを導入する手法を提案している。CIFAR10等のデータセットを用いて提案手法の効果を確かめている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Regularizing-Deep-Neural-Networks-by-Noise-Its-Interpretation-and-Optimization/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7096-regularizing-deep-neural-networks-by-noise-its-interpretation-and-optimization&quot;&gt;Noh, Hyeonwoo, et al. “Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Few-Shot Adversarial Domain Adaption</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation" rel="alternate" type="text/html" title="Few-Shot Adversarial Domain Adaption" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Few-Shot-Adversarial-Domain-Adaptation">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;本研究ではDeepモデルを用いた教師ありのドメイン適応の問題に対して対処するフレームワークを提案している。主なアイディアは埋め込み表現を学習する際にAdversarialな学習を導入し、2つの異なるドメインの特徴を保持するように埋め込みつつ、同時に埋め込み表現が意味をなすように配置させるところである。教師ありタスクの場合、一般的には大量のラベル付与済みデータが必要であるが、ラベルを付与すべきデータが少なく済むことでより扱いやすい問題となる。こういったFew-shotな学習の場合にデータ欠損があると、埋め込み空間に対して埋め込み表現の配置と分離というのは困難を極める。&lt;/p&gt;

&lt;p&gt;提案モデルでは典型的な2値のAdversarial discriminatorが4つの異なるクラスを分離するためにData augmentationに工夫することで、教師ありのドメイン適応問題対して有効であることを見つけている。加えて本手法ではラベル付与済みデータがとても少ない場合、特にカテゴリあたり1サンプルであったとしても高速に適応できていることが示されている。MNISTやSVHNデータセット等を用いて本手法のドメイン適応の効果を確認している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Few-Shot-Adversarial-Domain-Adaptation/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7244-few-shot-adversarial-domain-adaptation&quot;&gt;Motiian, Saeid, et al. “Few-Shot Adversarial Domain Adaptation.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">DropoutNet: Addressing Cold Start in Recommender Systems</title><link href="https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System" rel="alternate" type="text/html" title="DropoutNet: Addressing Cold Start in Recommender Systems" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;潜在的な意味を捉えるモデルは、精度の良さやスケーラビリティを有するため、レコメンドシステムを導入する際にデフォルトの選択肢の1つとなっている。しかしながらレコメンドといった分野の先行研究では、主にユーザーとアイテムの関係をモデル化したものが多く、データが少ない場合に起こる「コールドスタート問題」に対する解決策を提示しているモデルは少ない。Deep learningは近年様々な入力に対して優れた成功を収めている。そういったモデルを利用し、レコメンドシステムにおけるコールドスタート問題を解決するため、ニューラルネットワークベースのDropoutNetを提案している。&lt;/p&gt;

&lt;p&gt;コンテンツベースの目的関数を追加した既存の手法と異なり、本研究では最適化手法に焦点を当てたのと、Dropoutを応用して明示的にコールドスタートに対してモデルのトレーニングを行う方法を示している。提案モデルは既存のモデルに対して適用することができ、コールドスタートに対して効果を発揮している。コールドスタート問題を解決しているかを評価できるCiteULikeデータセットやACM RecSys 2017 challengeデータセットを用いて、提案モデルが優れた結果を出していることを確認している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/DropoutNet-Addressing-Cold-Start-in-Recommender-System/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7081-dropoutnet-addressing-cold-start-in-recommender-systems.pdf&quot;&gt;Volkovs, Maksims, Guangwei Yu, and Tomi Poutanen. “DropoutNet: Addressing Cold Start in Recommender Systems.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Dilated Recurrent Neural Networks</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks" rel="alternate" type="text/html" title="Dilated Recurrent Neural Networks" /><published>2018-02-19T00:00:00+00:00</published><updated>2018-02-19T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Dilated-Recurrent-Neural-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;長い文章等に対してRNNを学習させることはとても難しい問題となっている。特に複雑な依存関係、勾配消失/爆発問題、並列化が難しい点が挙げられている。本研究ではシンプルで効率的なRNNのアーキテクチャであるDilatedRNNを提案している。本アーキテクチャは複数のDilated recurrent skip connectionを組み合わせており、さまざまなRNNセルとフレキシブルに組み合わせることができる。またDilatedRNNは必要なパラメータ数を減らしつつ学習効率を大幅に向上させることができ、とても長いスパンで依存性のあるデータやタスクで優れたパフォーマンスを発揮している。&lt;/p&gt;

&lt;p&gt;提案モデルの利点を理論的に定量化するため、Memory capacity measureを導入している。これは長いスキップコネクションを持つRNNに対して既存の指標よりも適していることが示されており、LSTMなど他のRNNアーキテクチャのMemory capacity measureを比較することでDilatedRNNが優れていること証明している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Dilated-Recurrent-Neural-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6613-dilated-recurrent-neural-networks&quot;&gt;Chang, Shiyu, et al. “Dilated recurrent neural networks.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>